
@article{cafri_meta-meta-analysis_2010,
	title = {A {Meta}-{Meta}-{Analysis}: {Empirical} {Review} of {Statistical} {Power}, {Type} {I} {Error} {Rates}, {Effect} {Sizes}, and {Model} {Selection} of {Meta}-{Analyses} {Published} in {Psychology}},
	volume = {45},
	issn = {0027-3171},
	shorttitle = {A {Meta}-{Meta}-{Analysis}},
	url = {https://doi.org/10.1080/00273171003680187},
	doi = {10.1080/00273171003680187},
	abstract = {This article uses meta-analyses published in Psychological Bulletin from 1995 to 2005 to describe meta-analyses in psychology, including examination of statistical power, Type I errors resulting from multiple comparisons, and model choice. Retrospective power estimates indicated that univariate categorical and continuous moderators, individual moderators in multivariate analyses, and tests of residual variability within individual levels of categorical moderators had the lowest and most concerning levels of power. Using methods of calculating power prospectively for significance tests in meta-analysis, we illustrate how power varies as a function of the number of effect sizes, the average sample size per effect size, effect size magnitude, and level of heterogeneity of effect sizes. In most meta-analyses many significance tests were conducted, resulting in a sizable estimated probability of a Type I error, particularly for tests of means within levels of a moderator, univariate categorical moderators, and residual variability within individual levels of a moderator. Across all surveyed studies, the median effect size and the median difference between two levels of study level moderators were smaller than Cohen's (1988) conventions for a medium effect size for a correlation or difference between two correlations. The median Birge's (1932) ratio was larger than the convention of medium heterogeneity proposed by Hedges and Pigott (2001) and indicates that the typical meta-analysis shows variability in underlying effects well beyond that expected by sampling error alone. Fixed-effects models were used with greater frequency than random-effects models; however, random-effects models were used with increased frequency over time. Results related to model selection of this study are carefully compared with those from Schmidt, Oh, and Hayes (2009), who independently designed and produced a study similar to the one reported here. Recommendations for conducting future meta-analyses in light of the findings are provided.},
	number = {2},
	urldate = {2024-01-23},
	journal = {Multivariate Behavioral Research},
	author = {Cafri, Guy and Kromrey, Jeffrey D. and Brannick, Michael T.},
	month = mar,
	year = {2010},
	pmid = {26760285},
	note = {Number: 2
Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171003680187},
	pages = {239--270},
}

@article{marszalek_sample_2011,
	title = {Sample size in psychological research over the past 30 years},
	volume = {112},
	issn = {0031-5125},
	doi = {10.2466/03.11.PMS.112.2.331-348},
	abstract = {The American Psychological Association (APA) Task Force on Statistical Inference was formed in 1996 in response to a growing body of research demonstrating methodological issues that threatened the credibility of psychological research, and made recommendations to address them. One issue was the small, even dramatically inadequate, size of samples used in studies published by leading journals. The present study assessed the progress made since the Task Force's final report in 1999. Sample sizes reported in four leading APA journals in 1955, 1977, 1995, and 2006 were compared using nonparametric statistics, while data from the last two waves were fit to a hierarchical generalized linear growth model for more in-depth analysis. Overall, results indicate that the recommendations for increasing sample sizes have not been integrated in core psychological research, although results slightly vary by field. This and other implications are discussed in the context of current methodological critique and practice.},
	language = {eng},
	number = {2},
	journal = {Perceptual and Motor Skills},
	author = {Marszalek, Jacob M. and Barber, Carolyn and Kohlhart, Julie and Holmes, Cooper B.},
	month = apr,
	year = {2011},
	pmid = {21667745},
	note = {Number: 2},
	keywords = {Humans, Reproducibility of Results, Psychology, Publishing, Research, Forecasting, Periodicals as Topic, Data Interpretation, Statistical, Sample Size, Bias, Editorial Policies, Linear Models, Statistics, Nonparametric},
	pages = {331--348},
}

@article{shen_samples_2011,
	title = {Samples in applied psychology: {Over} a decade of research in review},
	volume = {96},
	issn = {1939-1854},
	shorttitle = {Samples in applied psychology},
	doi = {10.1037/a0023322},
	abstract = {This study examines sample characteristics of articles published in Journal of Applied Psychology (JAP) from 1995 to 2008. At the individual level, the overall median sample size over the period examined was approximately 173, which is generally adequate for detecting the average magnitude of effects of primary interest to researchers who publish in JAP. Samples using higher units of analyses (e.g., teams, departments/work units, and organizations) had lower median sample sizes (Mdn ≈ 65), yet were arguably robust given typical multilevel design choices of JAP authors despite the practical constraints of collecting data at higher units of analysis. A substantial proportion of studies used student samples ({\textasciitilde}40\%); surprisingly, median sample sizes for student samples were smaller than working adult samples. Samples were more commonly occupationally homogeneous ({\textasciitilde}70\%) than occupationally heterogeneous. U.S. and English-speaking participants made up the vast majority of samples, whereas Middle Eastern, African, and Latin American samples were largely unrepresented. On the basis of study results, recommendations are provided for authors, editors, and readers, which converge on 3 themes: (a) appropriateness and match between sample characteristics and research questions, (b) careful consideration of statistical power, and (c) the increased popularity of quantitative synthesis. Implications are discussed in terms of theory building, generalizability of research findings, and statistical power to detect effects. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {5},
	journal = {Journal of Applied Psychology},
	author = {Shen, Winny and Kiger, Thomas B. and Davies, Stacy E. and Rasch, Rena L. and Simon, Kara M. and Ones, Deniz S.},
	year = {2011},
	note = {Number: 5
Place: US
Publisher: American Psychological Association},
	keywords = {Methodology, Statistical Power, Applied Psychology, Experimental Design, Sample Size},
	pages = {1055--1064},
}

@article{lovakov_empirically_2021,
	title = {Empirically derived guidelines for effect size interpretation in social psychology},
	volume = {51},
	issn = {1099-0992},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2752},
	doi = {10.1002/ejsp.2752},
	abstract = {This study estimates empirically derived guidelines for effect size interpretation for research in social psychology overall and sub-disciplines within social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis of empirically derived distributions of 12,170 correlation coefficients and 6,447 Cohen's d statistics extracted from studies included in 134 published meta-analyses revealed that the 25th, 50th, and 75th percentiles corresponded to correlation coefficient values of 0.12, 0.24, and 0.41 and to Cohen's d values of 0.15, 0.36, and 0.65 respectively. The analysis suggests that the widely used Cohen's guidelines tend to overestimate medium and large effect sizes. Empirically derived effect size distributions in social psychology overall and its sub-disciplines can be used both for effect size interpretation and for sample size planning when other information about effect size is not available.},
	language = {en},
	number = {3},
	urldate = {2024-01-10},
	journal = {European Journal of Social Psychology},
	author = {Lovakov, Andrey and Agadullina, Elena R.},
	year = {2021},
	note = {Number: 3
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ejsp.2752},
	keywords = {effect size, sample size, correlation, Cohen's d},
	pages = {485--504},
}

@article{weinerova_published_2022,
	title = {Published correlational effect sizes in social and developmental psychology},
	volume = {9},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.220311},
	doi = {10.1098/rsos.220311},
	abstract = {The distribution of effect sizes may offer insights about the research done and reported in a scientific field. We have evaluated 12 412 manually collected correlation effect sizes (Sample 1) and 31 157 computer-extracted correlation effect sizes (Sample 2) published in journals focused on social or developmental psychology. Sample 1 consisted of 243 studies from six journals published in 2010 and 2019. Sample 2 consisted of 5012 papers published in 10 journals between 2010 and 2019. The 25th, 50th and 75th effect size percentiles were 0.08, 0.17 and 0.33, and 0.17, 0.31 and 0.52 in Samples 1 and 2, respectively. Sample 2 percentiles were probably larger because Sample 2 only included effect sizes from the text but not from tables. In text authors may have emphasized larger correlations. Large sample sizes were associated with smaller reported correlations. In Sample 1 about 70\% of studies specified a directional hypothesis. In 2010 no papers had power calculations, while in 2019 14\% of papers had power calculations. These data offer empirical insights into the distribution of reported correlations and may inform the interpretation of effect sizes. They also demonstrate the importance of computation of statistical power and highlight potential reporting bias.},
	number = {12},
	urldate = {2023-12-01},
	journal = {Royal Society Open Science},
	author = {Weinerová, Josefína and Szűcs, Dénes and Ioannidis, John P. A.},
	month = dec,
	year = {2022},
	note = {Number: 12
Publisher: Royal Society},
	keywords = {notion, effect size, sample size, correlation, statistical power},
	pages = {220311},
}

@article{schafer_meaningfulness_2019,
	title = {The {Meaningfulness} of {Effect} {Sizes} in {Psychological} {Research}: {Differences} {Between} {Sub}-{Disciplines} and the {Impact} of {Potential} {Biases}},
	volume = {10},
	shorttitle = {The {Meaningfulness} of {Effect} {Sizes} in {Psychological} {Research}},
	doi = {10.3389/fpsyg.2019.00813},
	abstract = {Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes-when is an effect small, medium, or large?-has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.},
	journal = {Frontiers in Psychology},
	author = {Schäfer, Thomas and Schwarz, Marcus},
	month = apr,
	year = {2019},
	keywords = {notion},
	pages = {1--13},
}

@article{kuhberger_publication_2014,
	title = {Publication {Bias} in {Psychology}: {A} {Diagnosis} {Based} on the {Correlation} between {Effect} {Size} and {Sample} {Size}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {Publication {Bias} in {Psychology}},
	url = {https://dx.plos.org/10.1371/journal.pone.0105825},
	doi = {10.1371/journal.pone.0105825},
	language = {en},
	number = {9},
	urldate = {2023-05-10},
	journal = {PLoS ONE},
	author = {Kühberger, Anton and Fritz, Astrid and Scherndl, Thomas},
	editor = {Fanelli, Daniele},
	month = sep,
	year = {2014},
	note = {Number: 9},
	keywords = {notion},
	pages = {e105825},
}

@article{cafri_meta-meta-analysis_2010-1,
	title = {A {Meta}-{Meta}-{Analysis}: {Empirical} {Review} of {Statistical} {Power}, {Type} {I} {Error} {Rates}, {Effect} {Sizes}, and {Model} {Selection} of {Meta}-{Analyses} {Published} in {Psychology}},
	volume = {45},
	issn = {0027-3171},
	shorttitle = {A {Meta}-{Meta}-{Analysis}},
	url = {https://doi.org/10.1080/00273171003680187},
	doi = {10.1080/00273171003680187},
	abstract = {This article uses meta-analyses published in Psychological Bulletin from 1995 to 2005 to describe meta-analyses in psychology, including examination of statistical power, Type I errors resulting from multiple comparisons, and model choice. Retrospective power estimates indicated that univariate categorical and continuous moderators, individual moderators in multivariate analyses, and tests of residual variability within individual levels of categorical moderators had the lowest and most concerning levels of power. Using methods of calculating power prospectively for significance tests in meta-analysis, we illustrate how power varies as a function of the number of effect sizes, the average sample size per effect size, effect size magnitude, and level of heterogeneity of effect sizes. In most meta-analyses many significance tests were conducted, resulting in a sizable estimated probability of a Type I error, particularly for tests of means within levels of a moderator, univariate categorical moderators, and residual variability within individual levels of a moderator. Across all surveyed studies, the median effect size and the median difference between two levels of study level moderators were smaller than Cohen's (1988) conventions for a medium effect size for a correlation or difference between two correlations. The median Birge's (1932) ratio was larger than the convention of medium heterogeneity proposed by Hedges and Pigott (2001) and indicates that the typical meta-analysis shows variability in underlying effects well beyond that expected by sampling error alone. Fixed-effects models were used with greater frequency than random-effects models; however, random-effects models were used with increased frequency over time. Results related to model selection of this study are carefully compared with those from Schmidt, Oh, and Hayes (2009), who independently designed and produced a study similar to the one reported here. Recommendations for conducting future meta-analyses in light of the findings are provided.},
	number = {2},
	urldate = {2024-01-23},
	journal = {Multivariate Behavioral Research},
	author = {Cafri, Guy and Kromrey, Jeffrey D. and Brannick, Michael T.},
	month = mar,
	year = {2010},
	pmid = {26760285},
	note = {Number: 2
Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171003680187},
	pages = {239--270},
}

@article{marszalek_sample_2011-1,
	title = {Sample size in psychological research over the past 30 years},
	volume = {112},
	issn = {0031-5125},
	doi = {10.2466/03.11.PMS.112.2.331-348},
	abstract = {The American Psychological Association (APA) Task Force on Statistical Inference was formed in 1996 in response to a growing body of research demonstrating methodological issues that threatened the credibility of psychological research, and made recommendations to address them. One issue was the small, even dramatically inadequate, size of samples used in studies published by leading journals. The present study assessed the progress made since the Task Force's final report in 1999. Sample sizes reported in four leading APA journals in 1955, 1977, 1995, and 2006 were compared using nonparametric statistics, while data from the last two waves were fit to a hierarchical generalized linear growth model for more in-depth analysis. Overall, results indicate that the recommendations for increasing sample sizes have not been integrated in core psychological research, although results slightly vary by field. This and other implications are discussed in the context of current methodological critique and practice.},
	language = {eng},
	number = {2},
	journal = {Perceptual and Motor Skills},
	author = {Marszalek, Jacob M. and Barber, Carolyn and Kohlhart, Julie and Holmes, Cooper B.},
	month = apr,
	year = {2011},
	pmid = {21667745},
	note = {Number: 2},
	keywords = {Humans, Reproducibility of Results, Psychology, Publishing, Research, Forecasting, Periodicals as Topic, Data Interpretation, Statistical, Sample Size, Bias, Editorial Policies, Linear Models, Statistics, Nonparametric},
	pages = {331--348},
}

@article{shen_samples_2011-1,
	title = {Samples in applied psychology: {Over} a decade of research in review},
	volume = {96},
	issn = {1939-1854},
	shorttitle = {Samples in applied psychology},
	doi = {10.1037/a0023322},
	abstract = {This study examines sample characteristics of articles published in Journal of Applied Psychology (JAP) from 1995 to 2008. At the individual level, the overall median sample size over the period examined was approximately 173, which is generally adequate for detecting the average magnitude of effects of primary interest to researchers who publish in JAP. Samples using higher units of analyses (e.g., teams, departments/work units, and organizations) had lower median sample sizes (Mdn ≈ 65), yet were arguably robust given typical multilevel design choices of JAP authors despite the practical constraints of collecting data at higher units of analysis. A substantial proportion of studies used student samples ({\textasciitilde}40\%); surprisingly, median sample sizes for student samples were smaller than working adult samples. Samples were more commonly occupationally homogeneous ({\textasciitilde}70\%) than occupationally heterogeneous. U.S. and English-speaking participants made up the vast majority of samples, whereas Middle Eastern, African, and Latin American samples were largely unrepresented. On the basis of study results, recommendations are provided for authors, editors, and readers, which converge on 3 themes: (a) appropriateness and match between sample characteristics and research questions, (b) careful consideration of statistical power, and (c) the increased popularity of quantitative synthesis. Implications are discussed in terms of theory building, generalizability of research findings, and statistical power to detect effects. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {5},
	journal = {Journal of Applied Psychology},
	author = {Shen, Winny and Kiger, Thomas B. and Davies, Stacy E. and Rasch, Rena L. and Simon, Kara M. and Ones, Deniz S.},
	year = {2011},
	note = {Number: 5
Place: US
Publisher: American Psychological Association},
	keywords = {Methodology, Statistical Power, Applied Psychology, Experimental Design, Sample Size},
	pages = {1055--1064},
}

@article{lovakov_empirically_2021-1,
	title = {Empirically derived guidelines for effect size interpretation in social psychology},
	volume = {51},
	issn = {1099-0992},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2752},
	doi = {10.1002/ejsp.2752},
	abstract = {This study estimates empirically derived guidelines for effect size interpretation for research in social psychology overall and sub-disciplines within social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis of empirically derived distributions of 12,170 correlation coefficients and 6,447 Cohen's d statistics extracted from studies included in 134 published meta-analyses revealed that the 25th, 50th, and 75th percentiles corresponded to correlation coefficient values of 0.12, 0.24, and 0.41 and to Cohen's d values of 0.15, 0.36, and 0.65 respectively. The analysis suggests that the widely used Cohen's guidelines tend to overestimate medium and large effect sizes. Empirically derived effect size distributions in social psychology overall and its sub-disciplines can be used both for effect size interpretation and for sample size planning when other information about effect size is not available.},
	language = {en},
	number = {3},
	urldate = {2024-01-10},
	journal = {European Journal of Social Psychology},
	author = {Lovakov, Andrey and Agadullina, Elena R.},
	year = {2021},
	note = {Number: 3
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ejsp.2752},
	keywords = {effect size, sample size, correlation, Cohen's d},
	pages = {485--504},
}

@article{weinerova_published_2022-1,
	title = {Published correlational effect sizes in social and developmental psychology},
	volume = {9},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.220311},
	doi = {10.1098/rsos.220311},
	abstract = {The distribution of effect sizes may offer insights about the research done and reported in a scientific field. We have evaluated 12 412 manually collected correlation effect sizes (Sample 1) and 31 157 computer-extracted correlation effect sizes (Sample 2) published in journals focused on social or developmental psychology. Sample 1 consisted of 243 studies from six journals published in 2010 and 2019. Sample 2 consisted of 5012 papers published in 10 journals between 2010 and 2019. The 25th, 50th and 75th effect size percentiles were 0.08, 0.17 and 0.33, and 0.17, 0.31 and 0.52 in Samples 1 and 2, respectively. Sample 2 percentiles were probably larger because Sample 2 only included effect sizes from the text but not from tables. In text authors may have emphasized larger correlations. Large sample sizes were associated with smaller reported correlations. In Sample 1 about 70\% of studies specified a directional hypothesis. In 2010 no papers had power calculations, while in 2019 14\% of papers had power calculations. These data offer empirical insights into the distribution of reported correlations and may inform the interpretation of effect sizes. They also demonstrate the importance of computation of statistical power and highlight potential reporting bias.},
	number = {12},
	urldate = {2023-12-01},
	journal = {Royal Society Open Science},
	author = {Weinerová, Josefína and Szűcs, Dénes and Ioannidis, John P. A.},
	month = dec,
	year = {2022},
	note = {Number: 12
Publisher: Royal Society},
	keywords = {notion, effect size, sample size, correlation, statistical power},
	pages = {220311},
}

@article{schafer_meaningfulness_2019-1,
	title = {The {Meaningfulness} of {Effect} {Sizes} in {Psychological} {Research}: {Differences} {Between} {Sub}-{Disciplines} and the {Impact} of {Potential} {Biases}},
	volume = {10},
	shorttitle = {The {Meaningfulness} of {Effect} {Sizes} in {Psychological} {Research}},
	doi = {10.3389/fpsyg.2019.00813},
	abstract = {Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes-when is an effect small, medium, or large?-has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.},
	journal = {Frontiers in Psychology},
	author = {Schäfer, Thomas and Schwarz, Marcus},
	month = apr,
	year = {2019},
	keywords = {notion},
	pages = {1--13},
}

@article{kuhberger_publication_2014-1,
	title = {Publication {Bias} in {Psychology}: {A} {Diagnosis} {Based} on the {Correlation} between {Effect} {Size} and {Sample} {Size}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {Publication {Bias} in {Psychology}},
	url = {https://dx.plos.org/10.1371/journal.pone.0105825},
	doi = {10.1371/journal.pone.0105825},
	language = {en},
	number = {9},
	urldate = {2023-05-10},
	journal = {PLoS ONE},
	author = {Kühberger, Anton and Fritz, Astrid and Scherndl, Thomas},
	editor = {Fanelli, Daniele},
	month = sep,
	year = {2014},
	note = {Number: 9},
	keywords = {notion},
	pages = {e105825},
}

@article{rosenthal_file_1979,
	title = {The file drawer problem and tolerance for null results},
	volume = {86},
	issn = {1939-1455},
	doi = {10.1037/0033-2909.86.3.638},
	abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Bulletin},
	author = {Rosenthal, Robert},
	year = {1979},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Experimentation, Scientific Communication, Statistical Probability, Statistical Tests, Type I Errors},
	pages = {638--641},
	file = {Rosenthal - 1979 - The file drawer problem and tolerance for null res.pdf:/Users/luca/Zotero/storage/ETVSCNZ8/Rosenthal - 1979 - The file drawer problem and tolerance for null res.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/U7C5RPAQ/1979-27602-001.html:text/html},
}

@article{dickersin_existence_1990,
	title = {The existence of publication bias and risk factors for its occurrence},
	volume = {263},
	issn = {0098-7484},
	abstract = {Publication bias is the tendency on the parts of investigators, reviewers, and editors to submit or accept manuscripts for publication based on the direction or strength of the study findings. Much of what has been learned about publication bias comes from the social sciences, less from the field of medicine. In medicine, three studies have provided direct evidence for this bias. Prevention of publication bias is important both from the scientific perspective (complete dissemination of knowledge) and from the perspective of those who combine results from a number of similar studies (meta-analysis). If treatment decisions are based on the published literature, then the literature must include all available data that is of acceptable quality. Currently, obtaining information regarding all studies undertaken in a given field is difficult, even impossible. Registration of clinical trials, and perhaps other types of studies, is the direction in which the scientific community should move.},
	language = {eng},
	number = {10},
	journal = {JAMA},
	author = {Dickersin, K.},
	month = mar,
	year = {1990},
	pmid = {2406472},
	keywords = {Publishing, American Medical Association, History, 19th Century, Periodicals as Topic, Research Support as Topic, Risk Factors, United Kingdom, United States},
	pages = {1385--1389},
	file = {Dickersin - 1990 - The existence of publication bias and risk factors.pdf:/Users/luca/Zotero/storage/92D8H2I7/Dickersin - 1990 - The existence of publication bias and risk factors.pdf:application/pdf},
}

@article{song_publication_2013,
	title = {Publication bias: what is it? {How} do we measure it? {How} do we avoid it?},
	volume = {5},
	shorttitle = {Publication bias},
	url = {https://www.dovepress.com/publication-bias-what-is-it-how-do-we-measure-it-how-do-we-avoid-it-peer-reviewed-fulltext-article-OAJCT},
	doi = {10.2147/OAJCT.S34419},
	abstract = {Publication bias: what is it? How do we measure it? How do we avoid it? Fujian Song, Lee Hooper, Yoon K LokeNorwich Medical School, University of East Anglia, Norwich, UKAbstract: Publication bias occurs when results of published studies are systematically different from results of unpublished studies. The term \&quot;dissemination bias\&quot; has also been recommended to describe all forms of biases in the research-dissemination process, including outcome-reporting bias, time-lag bias, gray-literature bias, full-publication bias, language bias, citation bias, and media-attention bias. We can measure publication bias by comparing the results of published and unpublished studies addressing the same question. Following up cohorts of studies from inception and comparing publication levels in studies with statistically significant or \&quot;positive\&quot; results suggested greater odds of formal publication in those with such results, compared to those without. Within reviews, funnel plots and related statistical methods can be used to indicate presence or absence of publication bias, although these can be unreliable in many circumstances. Methods of avoiding publication bias, by identifying and including unpublished outcomes and unpublished studies, are discussed and evaluated. These include searching without limiting by outcome, searching prospective trials registers, searching informal sources, including meeting abstracts and PhD theses, searching regulatory body websites, contacting authors of included studies, and contacting pharmaceutical or medical device companies for further studies. Adding unpublished studies often alters effect sizes, but may not always eliminate publication bias. The compulsory registration of all clinical trials at inception is an important move forward, but it can be difficult for reviewers to access data from unpublished studies located this way. Publication bias may be reduced by journals by publishing high-quality studies regardless of novelty or unexciting results, and by publishing protocols or full-study data sets. No single step can be relied upon to fully overcome the complex actions involved in publication bias, and a multipronged approach is required by researchers, patients, journal editors, peer reviewers, research sponsors, research ethics committees, and regulatory and legislation authorities.Keywords: publication bias, reporting bias, research-dissemination bias, evidence synthesis, systematic review, meta-analysis},
	language = {English},
	urldate = {2024-01-02},
	journal = {Open Access Journal of Clinical Trials},
	author = {Song, Fujian and Hooper, Lee and Loke, Yoon K.},
	month = jul,
	year = {2013},
	note = {Publisher: Dove Press},
	pages = {71--81},
	file = {Full Text PDF:/Users/luca/Zotero/storage/UPVM46CN/Song et al. - 2013 - Publication bias what is it How do we measure it.pdf:application/pdf},
}

@book{rothstein_publication_2005,
	title = {Publication {Bias} in {Meta}‐{Analysis}: {Prevention}, {Assessment} and {Adjustments}},
	url = {https://doi.org/10.1002/0470870168},
	publisher = {John Wiley \& Sons},
	author = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michael},
	year = {2005},
	file = {Rothstein et al. - 2005 - Publication Bias in Meta‐Analysis Prevention, Ass.pdf:/Users/luca/Zotero/storage/3QBACNXS/Rothstein et al. - 2005 - Publication Bias in Meta‐Analysis Prevention, Ass.pdf:application/pdf},
}

@article{song_dissemination_2010,
	title = {Dissemination and publication of research findings : an updated review of related biases},
	volume = {14},
	issn = {ISSN: 2046-4924, ISSN: 1366-5278},
	shorttitle = {Dissemination and publication of research findings},
	url = {https://www.journalslibrary.nihr.ac.uk/hta/hta14080/},
	doi = {10.3310/hta14080},
	abstract = {Objectives To identify and appraise empirical studies on publication and related biases published since 1998; to assess methods to deal with publication and related biases; and to examine, in a random sample of published systematic reviews, measures taken to prevent, reduce and detect dissemination bias. Data sources The main literature search, in August 2008, covered the Cochrane Methodology Register Database, MEDLINE, EMBASE, AMED and CINAHL. In May 2009, PubMed, PsycINFO and OpenSIGLE were also searched. Reference lists of retrieved studies were also examined. Review methods In Part I, studies were classified as evidence or method studies and data were extracted according to types of dissemination bias or methods for dealing with it. Evidence from empirical studies was summarised narratively. In Part II, 300 systematic reviews were randomly selected from MEDLINE and the methods used to deal with publication and related biases were assessed. Results Studies with significant or positive results were more likely to be published than those with non-significant or negative results, thereby confirming findings from a previous HTA report. There was convincing evidence that outcome reporting bias exists and has an impact on the pooled summary in systematic reviews. Studies with significant results tended to be published earlier than studies with non-significant results, and empirical evidence suggests that published studies tended to report a greater treatment effect than those from the grey literature. Exclusion of non-English-language studies appeared to result in a high risk of bias in some areas of research such as complementary and alternative medicine. In a few cases, publication and related biases had a potentially detrimental impact on patients or resource use. Publication bias can be prevented before a literature review (e.g. by prospective registration of trials), or detected during a literature review (e.g. by locating unpublished studies, funnel plot and related tests, sensitivity analysis modelling), or its impact can be minimised after a literature review (e.g. by confirmatory large-scale trials, updating the systematic review). The interpretation of funnel plot and related statistical tests, often used to assess publication bias, was often too simplistic and likely misleading. More sophisticated modelling methods have not been widely used. Compared with systematic reviews published in 1996, recent reviews of health-care interventions were more likely to locate and include non-English-language studies and grey literature or unpublished studies, and to test for publication bias. Conclusions Dissemination of research findings is likely to be a biased process, although the actual impact of such bias depends on specific circumstances. The prospective registration of clinical trials and the endorsement of reporting guidelines may reduce research dissemination bias in clinical research. In systematic reviews, measures can be taken to minimise the impact of dissemination bias by systematically searching for and including relevant studies that are difficult to access. Statistical methods can be useful for sensitivity analyses. Further research is needed to develop methods for qualitatively assessing the risk of publication bias in systematic reviews, and to evaluate the effect of prospective registration of studies, open access policy and improved publication guidelines.},
	language = {EN},
	number = {8},
	urldate = {2024-01-02},
	journal = {Health Technology Assessment},
	author = {Song, F. and Parekh, S. and Hooper, L. and Loke, Y. K. and Ryder, J. and Sutton, A. J. and Hing, C. and Kwok, C. S. and Pang, C. and Harvey, I.},
	month = feb,
	year = {2010},
	pages = {1--220},
	file = {Volltext:/Users/luca/Zotero/storage/J69UJG4P/Song et al. - 2010 - Dissemination and publication of research findings.pdf:application/pdf},
}

@article{schafer_meaningfulness_2019-2,
	title = {The {Meaningfulness} of {Effect} {Sizes} in {Psychological} {Research}: {Differences} {Between} {Sub}-{Disciplines} and the {Impact} of {Potential} {Biases}},
	volume = {10},
	shorttitle = {The {Meaningfulness} of {Effect} {Sizes} in {Psychological} {Research}},
	doi = {10.3389/fpsyg.2019.00813},
	abstract = {Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes-when is an effect small, medium, or large?-has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.},
	journal = {Frontiers in Psychology},
	author = {Schäfer, Thomas and Schwarz, Marcus},
	month = apr,
	year = {2019},
	keywords = {notion},
	pages = {1--13},
	file = {Full Text PDF:/Users/luca/Zotero/storage/C2Z4KVX3/Schäfer und Schwarz - 2019 - The Meaningfulness of Effect Sizes in Psychologica.pdf:application/pdf},
}

@article{gurevitch_statistical_1999,
	title = {Statistical {Issues} in {Ecological} {Meta}-{Analyses}},
	volume = {80},
	issn = {0012-9658},
	url = {https://www.jstor.org/stable/177061},
	doi = {10.2307/177061},
	abstract = {Meta-analysis is the use of statistical methods to summarize research findings across studies. Special statistical methods are usually needed for meta-analysis, both because effect-size indexes are typically highly heteroscedastic and because it is desirable to be able to distinguish between-study variance from within-study sampling-error variance. We outline a number of considerations related to choosing methods for the meta-analysis of ecological data, including the choice of parametric vs. resampling methods, reasons for conducting weighted analyses where possible, and comparisons fixed vs. mixed models in categorical and regression-type analyses.},
	number = {4},
	urldate = {2024-02-04},
	journal = {Ecology},
	author = {Gurevitch, Jessica and Hedges, Larry V.},
	year = {1999},
	note = {Publisher: Ecological Society of America},
	pages = {1142--1149},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/HQXUAEVK/Gurevitch und Hedges - 1999 - Statistical Issues in Ecological Meta-Analyses.pdf:application/pdf},
}

@article{weinerova_published_2022-2,
	title = {Published correlational effect sizes in social and developmental psychology},
	volume = {9},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.220311},
	doi = {10.1098/rsos.220311},
	abstract = {The distribution of effect sizes may offer insights about the research done and reported in a scientific field. We have evaluated 12 412 manually collected correlation effect sizes (Sample 1) and 31 157 computer-extracted correlation effect sizes (Sample 2) published in journals focused on social or developmental psychology. Sample 1 consisted of 243 studies from six journals published in 2010 and 2019. Sample 2 consisted of 5012 papers published in 10 journals between 2010 and 2019. The 25th, 50th and 75th effect size percentiles were 0.08, 0.17 and 0.33, and 0.17, 0.31 and 0.52 in Samples 1 and 2, respectively. Sample 2 percentiles were probably larger because Sample 2 only included effect sizes from the text but not from tables. In text authors may have emphasized larger correlations. Large sample sizes were associated with smaller reported correlations. In Sample 1 about 70\% of studies specified a directional hypothesis. In 2010 no papers had power calculations, while in 2019 14\% of papers had power calculations. These data offer empirical insights into the distribution of reported correlations and may inform the interpretation of effect sizes. They also demonstrate the importance of computation of statistical power and highlight potential reporting bias.},
	number = {12},
	urldate = {2023-12-01},
	journal = {Royal Society Open Science},
	author = {Weinerová, Josefína and Szűcs, Dénes and Ioannidis, John P. A.},
	month = dec,
	year = {2022},
	note = {Publisher: Royal Society},
	keywords = {notion, effect size, sample size, correlation, statistical power},
	pages = {220311},
	file = {Weinerová et al. - 2022 - Published correlational effect sizes in social and.pdf:/Users/luca/Zotero/storage/ANWEZZ59/Weinerová et al. - 2022 - Published correlational effect sizes in social and.pdf:application/pdf},
}

@article{kuhberger_publication_2014-2,
	title = {Publication {Bias} in {Psychology}: {A} {Diagnosis} {Based} on the {Correlation} between {Effect} {Size} and {Sample} {Size}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {Publication {Bias} in {Psychology}},
	url = {https://dx.plos.org/10.1371/journal.pone.0105825},
	doi = {10.1371/journal.pone.0105825},
	language = {en},
	number = {9},
	urldate = {2023-05-10},
	journal = {PLoS ONE},
	author = {Kühberger, Anton and Fritz, Astrid and Scherndl, Thomas},
	editor = {Fanelli, Daniele},
	month = sep,
	year = {2014},
	keywords = {notion},
	pages = {e105825},
	file = {Full Text PDF:/Users/luca/Zotero/storage/T84PVVFX/Kühberger et al. - 2014 - Publication Bias in Psychology A Diagnosis Based .pdf:application/pdf},
}

@article{slavin_relationship_2009,
	title = {The {Relationship} {Between} {Sample} {Sizes} and {Effect} {Sizes} in {Systematic} {Reviews} in {Education}},
	volume = {31},
	issn = {0162-3737},
	url = {https://doi.org/10.3102/0162373709352369},
	doi = {10.3102/0162373709352369},
	abstract = {Research in fields other than education has found that studies with small sample sizes tend to have larger effect sizes than those with large samples. This article examines the relationship between sample size and effect size in education. It analyzes data from 185 studies of elementary and secondary mathematics programs that met the standards of the Best Evidence Encyclopedia. As predicted, there was a significant negative correlation between sample size and effect size. The differences in effect sizes between small and large experiments were much greater than those between randomized and matched experiments. Explanations for the effects of sample size on effect size are discussed.},
	language = {en},
	number = {4},
	urldate = {2023-12-01},
	journal = {Educational Evaluation and Policy Analysis},
	author = {Slavin, Robert and Smith, Dewi},
	month = dec,
	year = {2009},
	note = {Publisher: American Educational Research Association},
	keywords = {notion},
	pages = {500--506},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/DPXQW39N/Slavin und Smith - 2009 - The Relationship Between Sample Sizes and Effect S.pdf:application/pdf},
}

@article{gerber_testing_2001,
	title = {Testing for {Publication} {Bias} in {Political} {Science}},
	volume = {9},
	issn = {1047-1987},
	url = {https://www.jstor.org/stable/25791658},
	abstract = {If the publication decisions of journals are a function of the statistical significance of research findings, the published literature may suffer from "publication bias." This paper describes a method for detecting publication bias. We point out that to achieve statistical significance, the effect size must be larger in small samples. If publications tend to be biased against statistically insignificant results, we should observe that the effect size diminishes as sample sizes increase. This proposition is tested and confirmed using the experimental literature on voter mobilization.},
	number = {4},
	urldate = {2023-12-01},
	journal = {Political Analysis},
	author = {Gerber, Alan S. and Green, Donald P. and Nickerson, David},
	year = {2001},
	note = {Publisher: [Oxford University Press, Society for Political Methodology]},
	keywords = {notion},
	pages = {385--392},
	file = {Gerber et al. - 2001 - Testing for Publication Bias in Political Science.pdf:/Users/luca/Zotero/storage/UMTTCZGD/Gerber et al. - 2001 - Testing for Publication Bias in Political Science.pdf:application/pdf},
}

@article{levine_sample_2009,
	title = {Sample {Sizes} and {Effect} {Sizes} are {Negatively} {Correlated} in {Meta}-{Analyses}: {Evidence} and {Implications} of a {Publication} {Bias} {Against} {NonSignificant} {Findings}},
	volume = {76},
	issn = {0363-7751},
	shorttitle = {Sample {Sizes} and {Effect} {Sizes} are {Negatively} {Correlated} in {Meta}-{Analyses}},
	url = {https://doi.org/10.1080/03637750903074685},
	doi = {10.1080/03637750903074685},
	abstract = {Meta-analysis involves cumulating effects across studies in order to qualitatively summarize existing literatures. A recent finding suggests that the effect sizes reported in meta-analyses may be negatively correlated with study sample sizes. This prediction was tested with a sample of 51 published meta-analyses summarizing the results of 3,602 individual studies. The correlation between effect size and sample size was negative in almost 80 percent of the meta-analyses examined, and the negative correlation was not limited to a particular type of research or substantive area. This result most likely stems from a bias against publishing findings that are not statistically significant. The primary implication is that meta-analyses may systematically overestimate population effect sizes. It is recommended that researchers routinely examine the n–r scatter plot and correlation, or some other indication of publication bias and report this information in meta-analyses.},
	number = {3},
	urldate = {2023-06-04},
	journal = {Communication Monographs},
	author = {Levine, Timothy R. and Asada, Kelli J. and Carpenter, Chris},
	month = sep,
	year = {2009},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/03637750903074685},
	keywords = {notion, Effect Size, Meta-Analysis, Publication Bias},
	pages = {286--302},
	file = {Levine et al. - 2009 - Sample Sizes and Effect Sizes are Negatively Corre.pdf:/Users/luca/Zotero/storage/28V95VIF/Levine et al. - 2009 - Sample Sizes and Effect Sizes are Negatively Corre.pdf:application/pdf},
}

@article{jennions_relationships_2002,
	title = {Relationships fade with time: a meta-analysis of temporal trends in publication in ecology and evolution.},
	volume = {269},
	issn = {0962-8452},
	shorttitle = {Relationships fade with time},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1690867/},
	doi = {10.1098/rspb.2001.1832},
	abstract = {Both significant positive and negative relationships between the magnitude of research findings (their 'effect size') and their year of publication have been reported in a few areas of biology. These trends have been attributed to Kuhnian paradigm shifts, scientific fads and bias in the choice of study systems. Here we test whether or not these isolated cases reflect a more general trend. We examined the relationship using effect sizes extracted from 44 peer-reviewed meta-analyses covering a wide range of topics in ecological and evolutionary biology. On average, there was a small but significant decline in effect size with year of publication. For the original empirical studies there was also a significant decrease in effect size as sample size increased. However, the effect of year of publication remained even after we controlled for sampling effort. Although these results have several possible explanations, it is suggested that a publication bias against non-significant or weaker findings offers the most parsimonious explanation. As in the medical sciences, non-significant results may take longer to publish and studies with both small sample sizes and non-significant results may be less likely to be published.},
	number = {1486},
	urldate = {2024-02-04},
	journal = {Proceedings of the Royal Society B: Biological Sciences},
	author = {Jennions, Michael D and Møller, Anders P},
	month = jan,
	year = {2002},
	pmid = {11788035},
	pmcid = {PMC1690867},
	pages = {43--48},
	file = {PubMed Central Full Text PDF:/Users/luca/Zotero/storage/ACFV77VB/Jennions und Møller - 2002 - Relationships fade with time a meta-analysis of t.pdf:application/pdf},
}

@article{linden_publication_2024,
	title = {Publication {Bias} in {Psychology}: {A} {Closer} {Look} at the {Correlation} {Between} {Sample} {Size} and {Effect} {Size}},
	shorttitle = {Publication {Bias} in {Psychology}},
	url = {https://osf.io/s4znd},
	doi = {10.31234/osf.io/s4znd},
	abstract = {Previously observed negative correlations between sample size and effect size (n-ES correlation) in psychological research have been interpreted as evidence for publication bias and related undesirable biases. Here, we present two studies aimed at better understanding to what extent negative n-ES correlations reflect such biases or might be explained by unproblematic adjustments of sample size to expected effect sizes. In Study 1, we analysed n-ES correlations in 150 meta-analyses from cognitive, organizational, and social psychology and in 57 multiple replications, which are free from relevant biases. In Study 2, we used a random sample of 160 psychology papers to compare the n-ES correlation for effects that are central to these papers and effects selected at random from these papers. n-ES correlations proved inconspicuous in meta-analyses. In line with previous research, they do not suggest that publication bias and related biases have a strong impact on meta-analyses in psychology. A much higher n-ES correlation emerged for publications’ focal effects. To what extent this should be attributed to publication bias and related biases remains unclear.},
	language = {en-us},
	urldate = {2024-01-21},
	author = {Linden, Audrey and Pollet, Thomas V. and Hönekopp, Johannes},
	month = jan,
	year = {2024},
	note = {Publisher: OSF},
	keywords = {notion},
	file = {ATJ publication bias R1 final preprint.docx:/Users/luca/Zotero/storage/2VME7AP9/ATJ publication bias R1 final preprint.docx:application/vnd.openxmlformats-officedocument.wordprocessingml.document;ATJ publication bias R1 final preprint.pdf:/Users/luca/Zotero/storage/AI5QHZDE/linden_etal_2023.pdf:application/pdf;linden_etal_2023.pdf:/Users/luca/Zotero/storage/WDWZX785/linden_etal_2023.pdf:application/pdf;linden_etal_2023.pdf:/Users/luca/Zotero/storage/UXQN38L3/linden_etal_2023.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/HAGRECZX/s4znd.html:text/html},
}

@article{jennions_publication_2002,
	title = {Publication bias in ecology and evolution: an empirical assessment using the ‘trim and fill’ method},
	volume = {77},
	issn = {1469-185X},
	shorttitle = {Publication bias in ecology and evolution},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1017/S1464793101005875},
	doi = {10.1017/S1464793101005875},
	abstract = {Recent reviews of specific topics, such as the relationship between male attractiveness to females and fluctuating asymmetry or attractiveness and the expression of secondary sexual characters, suggest that publication bias might be a problem in ecology and evolution. In these cases, there is a significant negative correlation between the sample size of published studies and the magnitude or strength of the research findings (formally the ‘effect size’). If all studies that are conducted are equally likely to be published, irrespective of their findings, there should not be a directional relationship between effect size and sample size; only a decrease in the variance in effect size as sample size increases due to a reduction in sampling error. One interpretation of these reports of negative correlations is that studies with small sample sizes and weaker findings (smaller effect sizes) are less likely to be published. If the biological literature is systematically biased this could undermine the attempts of reviewers to summarise actual biology relationships by inflating estimates of average effect sizes. But how common is this problem? And does it really effect the general conclusions of literature reviews? Here, we examine data sets of effect sizes extracted from 40 peer-reviewed, published meta-analyses. We estimate how many studies are missing using the newly developed ‘trim and fill’ method. This method uses asymmetry in plots of effect size against sample size (‘funnel plots’) to detect ‘missing’ studies. For random-effect models of meta-analysis 38\% (15/40) of data sets had a significant number of ‘missing’ studies. After correcting for potential publication bias, 21\% (8/38) of weighted mean effects were no longer significantly greater than zero, and 15\% (5/34) were no longer statistically robust when we used random-effects models in a weighted meta-analysis. The mean correlation between sample size and the magnitude of standardised effect size was also significantly negative (rs=-0.20, P {\textless} 0-0001). Individual correlations were significantly negative (P {\textless} 0.10) in 35\% (14/40) of cases. Publication bias may therefore effect the main conclusions of at least 15–21\% of meta-analyses. We suggest that future literature reviews assess the robustness of their main conclusions by correcting for potential publication bias using the ‘trim and fill’ method.},
	language = {en},
	number = {2},
	urldate = {2023-12-01},
	journal = {Biological Reviews},
	author = {Jennions, Michael D. and Møller, Anders P.},
	year = {2002},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1017/S1464793101005875},
	keywords = {meta-analysis, notion, publication bias, effect size, fail-safe number, fluctuating asymmetry, funnel plots, trim and fill},
	pages = {211--222},
	file = {Jennions und Møller - 2002 - Publication bias in ecology and evolution an empi.pdf:/Users/luca/Zotero/storage/2B8UV4AU/Jennions und Møller - 2002 - Publication bias in ecology and evolution an empi.pdf:application/pdf},
}

@article{begg_operating_1994,
	title = {Operating {Characteristics} of a {Rank} {Correlation} {Test} for {Publication} {Bias}},
	volume = {50},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2533446},
	doi = {10.2307/2533446},
	abstract = {An adjusted rank correlation test is proposed as a technique for identifying publication bias in a meta-analysis, and its operating characteristics are evaluated via simulations. The test statistic is a direct statistical analogue of the popular "funnel-graph." The number of component studies in the meta-analysis, the nature of the selection mechanism, the range of variances of the effect size estimates, and the true underlying effect size are all observed to be influential in determining the power of the test. The test is fairly powerful for large meta-analyses with 75 component studies, but has only moderate power for meta-analyses with 25 component studies. However, in many of the configurations in which there is low power, there is also relatively little bias in the summary effect size estimate. Nonetheless, the test must be interpreted with caution in small meta-analyses. In particular, bias cannot be ruled out if the test is not significant. The proposed technique has potential utility as an exploratory tool for meta-analysts, as a formal procedure to complement the funnel-graph.},
	number = {4},
	urldate = {2024-02-04},
	journal = {Biometrics},
	author = {Begg, Colin B. and Mazumdar, Madhuchhanda},
	year = {1994},
	note = {Publisher: [Wiley, International Biometric Society]},
	keywords = {notion},
	pages = {1088--1101},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/QPAWGDD9/Begg und Mazumdar - 1994 - Operating Characteristics of a Rank Correlation Te.pdf:application/pdf},
}

@article{moller_how_2001,
	title = {How important are direct fitness benefits of sexual selection?},
	volume = {88},
	issn = {1432-1904},
	url = {https://doi.org/10.1007/s001140100255},
	doi = {10.1007/s001140100255},
	abstract = {Females may choose mates based on the expression of secondary sexual characters that signal direct, material fitness benefits or indirect, genetic fitness benefits. Genetic benefits are acquired in the generation subsequent to that in which mate choice is performed, and the maintenance of genetic variation in viability has been considered a theoretical problem. Consequently, the magnitude of indirect benefits has traditionally been considered to be small. Direct fitness benefits can be maintained without consideration of mechanisms sustaining genetic variability, and they have thus been equated with the default benefits acquired by choosy females. There is, however, still debate as to whether or not males should honestly advertise direct benefits such as their willingness to invest in parental care. We use meta-analysis to estimate the magnitude of direct fitness benefits in terms of fertility, fecundity and two measures of paternal care (feeding rate in birds, hatching rate in male guarding ectotherms) based on an extensive literature survey. The mean coefficients of determination weighted by sample size were 6.3\%, 2.3\%, 1.3\% and 23.6\%, respectively. This compares to a mean weighted coefficient of determination of 1.5\% for genetic viability benefits in studies of sexual selection. Thus, for several fitness components, direct benefits are only slightly more important than indirect ones arising from female choice. Hatching rate in male guarding ectotherms was by far the most important direct fitness component, explaining almost a quarter of the variance. Our analysis also shows that male sexual advertisements do not always reliably signal direct fitness benefits.},
	language = {en},
	number = {10},
	urldate = {2024-02-04},
	journal = {Naturwissenschaften},
	author = {Møller, A. and Jennions, M.},
	month = oct,
	year = {2001},
	pages = {401--415},
	file = {Full Text PDF:/Users/luca/Zotero/storage/J6RQJJKW/Møller und Jennions - 2001 - How important are direct fitness benefits of sexua.pdf:application/pdf},
}

@article{lovakov_empirically_2021-2,
	title = {Empirically derived guidelines for effect size interpretation in social psychology},
	volume = {51},
	issn = {1099-0992},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2752},
	doi = {10.1002/ejsp.2752},
	abstract = {This study estimates empirically derived guidelines for effect size interpretation for research in social psychology overall and sub-disciplines within social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis of empirically derived distributions of 12,170 correlation coefficients and 6,447 Cohen's d statistics extracted from studies included in 134 published meta-analyses revealed that the 25th, 50th, and 75th percentiles corresponded to correlation coefficient values of 0.12, 0.24, and 0.41 and to Cohen's d values of 0.15, 0.36, and 0.65 respectively. The analysis suggests that the widely used Cohen's guidelines tend to overestimate medium and large effect sizes. Empirically derived effect size distributions in social psychology overall and its sub-disciplines can be used both for effect size interpretation and for sample size planning when other information about effect size is not available.},
	language = {en},
	number = {3},
	urldate = {2023-12-01},
	journal = {European Journal of Social Psychology},
	author = {Lovakov, Andrey and Agadullina, Elena R.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ejsp.2752},
	keywords = {effect size, sample size, correlation, Cohen's d},
	pages = {485--504},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/MUUFWJMD/Lovakov und Agadullina - 2021 - Empirically derived guidelines for effect size int.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/CIS4UIMU/ejsp.html:text/html},
}

@article{macaskill_comparison_2001,
	title = {A comparison of methods to detect publication bias in meta-analysis},
	volume = {20},
	issn = {0277-6715},
	doi = {10.1002/sim.698},
	abstract = {Meta-analyses are subject to bias for many of reasons, including publication bias. Asymmetry in a funnel plot of study size against treatment effect is often used to identify such bias. We compare the performance of three simple methods of testing for bias: the rank correlation method; a simple linear regression of the standardized estimate of treatment effect on the precision of the estimate; and a regression of the treatment effect on sample size. The tests are applied to simulated meta-analyses in the presence and absence of publication bias. Both one-sided and two-sided censoring of studies based on statistical significance was used. The results indicate that none of the tests performs consistently well. Test performance varied with the magnitude of the true treatment effect, distribution of study size and whether a one- or two-tailed significance test was employed. Overall, the power of the tests was low when the number of studies per meta-analysis was close to that often observed in practice. Tests that showed the highest power also had type I error rates higher than the nominal level. Based on the empirical type I error rates, a regression of treatment effect on sample size, weighted by the inverse of the variance of the logit of the pooled proportion (using the marginal total) is the preferred method.},
	language = {eng},
	number = {4},
	journal = {Statistics in Medicine},
	author = {Macaskill, P. and Walter, S. D. and Irwig, L.},
	month = feb,
	year = {2001},
	pmid = {11223905},
	keywords = {Humans, Computer Simulation, Statistics as Topic, Publication Bias, Meta-Analysis as Topic, Sample Size, Linear Models},
	pages = {641--654},
}

@article{la_france_is_2004,
	title = {Is there empirical evidence for a nonverbal profile of extraversion?: a meta‐analysis and critique of the literature},
	volume = {71},
	issn = {0363-7751, 1479-5787},
	shorttitle = {Is there empirical evidence for a nonverbal profile of extraversion?},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03634520410001693148},
	doi = {10.1080/03634520410001693148},
	language = {en},
	number = {1},
	urldate = {2023-12-01},
	journal = {Communication Monographs},
	author = {La France, Betty H. and Heisel, Alan D. and Beatty, Michael J.},
	month = mar,
	year = {2004},
	keywords = {notion},
	pages = {28--48},
	file = {La France et al. - 2004 - Is there empirical evidence for a nonverbal profil.pdf:/Users/luca/Zotero/storage/NS6JSKNT/La France et al. - 2004 - Is there empirical evidence for a nonverbal profil.pdf:application/pdf},
}

@article{palmer_detecting_1999,
	title = {Detecting {Publication} {Bias} in {Meta}‐analyses: {A} {Case} {Study} of {Fluctuating} {Asymmetry} and {Sexual} {Selection}.},
	volume = {154},
	issn = {0003-0147},
	shorttitle = {Detecting {Publication} {Bias} in {Meta}‐analyses},
	url = {https://www.journals.uchicago.edu/doi/10.1086/303223},
	doi = {10.1086/303223},
	number = {2},
	urldate = {2024-02-04},
	journal = {The American Naturalist},
	author = {Palmer, A. Richard},
	month = aug,
	year = {1999},
	note = {Publisher: The University of Chicago Press},
	keywords = {selective reporting, developmental stability, funnel graph, investigator effects, mating behavior, sexual signaling, statistical methods},
	pages = {220--233},
	file = {Palmer - 1999 - Detecting Publication Bias in Meta‐analyses A Cas.pdf:/Users/luca/Zotero/storage/VUNN4FV7/Palmer - 1999 - Detecting Publication Bias in Meta‐analyses A Cas.pdf:application/pdf},
}

@article{sasaki_sparking_2023,
	title = {{SPARKing}: {Sample}-size planning after the results are known},
	volume = {17},
	issn = {1662-5161},
	shorttitle = {{SPARKing}},
	url = {https://www.frontiersin.org/articles/10.3389/fnhum.2023.912338},
	urldate = {2024-01-14},
	journal = {Frontiers in Human Neuroscience},
	author = {Sasaki, Kyoshiro and Yamada, Yuki},
	year = {2023},
	file = {Full Text PDF:/Users/luca/Zotero/storage/5ZXRQ2K9/Sasaki und Yamada - 2023 - SPARKing Sample-size planning after the results a.pdf:application/pdf},
}

@article{lenth_practical_2001,
	title = {Some {Practical} {Guidelines} for {Effective} {Sample} {Size} {Determination}},
	volume = {55},
	issn = {0003-1305},
	url = {https://doi.org/10.1198/000313001317098149},
	doi = {10.1198/000313001317098149},
	abstract = {Sample size determination is often an important step in planning a statistical study—and it is usually a difficult one. Among the important hurdles to be surpassed, one must obtain an estimate of one or more error variances and specify an effect size of importance. There is the temptation to take some shortcuts. This article offers some suggestions for successful and meaningful sample size determination. Also discussed is the possibility that sample size may not be the main issue, that the real goal is to design a high-quality study. Finally, criticism is made of some ill-advised shortcuts relating to power and sample size.},
	number = {3},
	urldate = {2024-01-14},
	journal = {The American Statistician},
	author = {Lenth, Russell V},
	month = aug,
	year = {2001},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/000313001317098149},
	keywords = {Power, Cohen's effect measures, Equivalence testing, Observed power, Retrospective power, Study design},
	pages = {187--193},
}

@article{anderson_sample-size_2017,
	title = {Sample-{Size} {Planning} for {More} {Accurate} {Statistical} {Power}: {A} {Method} {Adjusting} {Sample} {Effect} {Sizes} for {Publication} {Bias} and {Uncertainty}},
	volume = {28},
	issn = {0956-7976},
	shorttitle = {Sample-{Size} {Planning} for {More} {Accurate} {Statistical} {Power}},
	url = {https://doi.org/10.1177/0956797617723724},
	doi = {10.1177/0956797617723724},
	abstract = {The sample size necessary to obtain a desired level of statistical power depends in part on the population value of the effect size, which is, by definition, unknown. A common approach to sample-size planning uses the sample effect size from a prior study as an estimate of the population value of the effect to be detected in the future study. Although this strategy is intuitively appealing, effect-size estimates, taken at face value, are typically not accurate estimates of the population effect size because of publication bias and uncertainty. We show that the use of this approach often results in underpowered studies, sometimes to an alarming degree. We present an alternative approach that adjusts sample effect sizes for bias and uncertainty, and we demonstrate its effectiveness for several experimental designs. Furthermore, we discuss an open-source R package, BUCSS, and user-friendly Web applications that we have made available to researchers so that they can easily implement our suggested methods.},
	language = {en},
	number = {11},
	urldate = {2024-01-14},
	journal = {Psychological Science},
	author = {Anderson, Samantha F. and Kelley, Ken and Maxwell, Scott E.},
	month = nov,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	pages = {1547--1562},
}

@article{bakker_researchers_2016,
	title = {Researchers’ {Intuitions} {About} {Power} in {Psychological} {Research}},
	volume = {27},
	issn = {0956-7976},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4976648/},
	doi = {10.1177/0956797616647519},
	abstract = {Many psychology studies are statistically underpowered. In part, this may be
because many researchers rely on intuition, rules of thumb, and prior practice
(along with practical considerations) to determine the number of subjects to
test. In Study 1, we surveyed 291 published research psychologists and found
large discrepancies between their reports of their preferred amount of power and
the actual power of their studies (calculated from their reported typical cell
size, typical effect size, and acceptable alpha). Furthermore, in Study 2, 89\%
of the 214 respondents overestimated the power of specific research designs with
a small expected effect size, and 95\% underestimated the sample size needed to
obtain .80 power for detecting a small effect. Neither researchers’ experience
nor their knowledge predicted the bias in their self-reported power intuitions.
Because many respondents reported that they based their sample sizes on rules of
thumb or common practice in the field, we recommend that researchers conduct and
report formal power analyses for their studies.},
	number = {8},
	urldate = {2023-12-01},
	journal = {Psychological Science},
	author = {Bakker, Marjan and Hartgerink, Chris H. J. and Wicherts, Jelte M. and van der Maas, Han L. J.},
	month = aug,
	year = {2016},
	pmid = {27354203},
	pmcid = {PMC4976648},
	keywords = {notion},
	pages = {1069--1077},
	file = {PubMed Central Full Text PDF:/Users/luca/Zotero/storage/ZPRMRKU6/Bakker et al. - 2016 - Researchers’ Intuitions About Power in Psychologic.pdf:application/pdf},
}

@article{maxwell_sample_2008,
	title = {Sample {Size} {Planning} for {Statistical} {Power} and {Accuracy} in {Parameter} {Estimation}},
	volume = {59},
	url = {https://doi.org/10.1146/annurev.psych.59.103006.093735},
	doi = {10.1146/annurev.psych.59.103006.093735},
	abstract = {This review examines recent advances in sample size planning, not only from the perspective of an individual researcher, but also with regard to the goal of developing cumulative knowledge. Psychologists have traditionally thought of sample size planning in terms of power analysis. Although we review recent advances in power analysis, our main focus is the desirability of achieving accurate parameter estimates, either instead of or in addition to obtaining sufficient power. Accuracy in parameter estimation (AIPE) has taken on increasing importance in light of recent emphasis on effect size estimation and formation of confidence intervals. The review provides an overview of the logic behind sample size planning for AIPE and summarizes recent advances in implementing this approach in designs commonly used in psychological research.},
	number = {1},
	urldate = {2024-01-14},
	journal = {Annual Review of Psychology},
	author = {Maxwell, Scott E. and Kelley, Ken and Rausch, Joseph R.},
	year = {2008},
	pmid = {17937603},
	note = {\_eprint: https://doi.org/10.1146/annurev.psych.59.103006.093735},
	keywords = {confidence intervals, effect size, cumulative science},
	pages = {537--563},
	file = {Maxwell et al. - 2008 - Sample Size Planning for Statistical Power and Acc.pdf:/Users/luca/Zotero/storage/4FA5C5WG/Maxwell et al. - 2008 - Sample Size Planning for Statistical Power and Acc.pdf:application/pdf},
}

@article{lakens_sample_2022,
	title = {Sample {Size} {Justification}},
	volume = {8},
	issn = {2474-7394},
	url = {https://doi.org/10.1525/collabra.33267},
	doi = {10.1525/collabra.33267},
	abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
	number = {1},
	urldate = {2024-01-14},
	journal = {Collabra: Psychology},
	author = {Lakens, Daniël},
	month = mar,
	year = {2022},
	pages = {33267},
	file = {Full Text PDF:/Users/luca/Zotero/storage/625QAR6H/Lakens - 2022 - Sample Size Justification.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/TZWW33AR/Sample-Size-Justification.html:text/html},
}

@article{giner-sorolla_power_2024,
	title = {Power to {Detect} {What}? {Considerations} for {Planning} and {Evaluating} {Sample} {Size}},
	shorttitle = {Power to {Detect} {What}?},
	url = {https://osf.io/rv3kw},
	doi = {10.31234/osf.io/rv3kw},
	abstract = {In the wake of the replication crisis, social and personality psychologists have increased attention to power analysis and the adequacy of sample sizes. In this paper, we analyze current controversies in this area, including choosing effect sizes, why and whether power analyses should be conducted on already-collected data, how to mitigate negative effects of sample size criteria on specific kinds of research, and which power criterion to use. For novel research questions, we advocate that researchers base sample sizes on effects that are likely to be cost-effective for other people to implement (in applied settings) or to study (in basic research settings), given the limitations of interest-based minimums or field-wide effect sizes. We discuss two alternatives to power analysis, precision analysis and sequential analysis, and end with recommendations for improving the practices of researchers, reviewers, and journal editors in social-personality psychology.},
	language = {en-us},
	urldate = {2024-01-14},
	author = {Giner-Sorolla, Roger and Montoya, Amanda and Aberson, Chris and Carpenter, Tom and Neil Lewis, Jr and Bostyn, Dries H. and Conrique, Beverly and Ng, Brandon W. and Reifman, Alan and Schoemann, Alexander M. and Soderberg, Courtney K.},
	month = jan,
	year = {2024},
	note = {Publisher: OSF},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/U9PC5EEN/Giner-Sorolla et al. - 2024 - Power to Detect What Considerations for Planning .pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/62UW8W7F/rv3kw.html:text/html},
}

@article{button_power_2013,
	title = {Power failure: why small sample size undermines the reliability of neuroscience},
	volume = {14},
	copyright = {2013 Springer Nature Limited},
	issn = {1471-0048},
	shorttitle = {Power failure},
	url = {https://www.nature.com/articles/nrn3475},
	doi = {10.1038/nrn3475},
	abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between ∼8\% and ∼31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
	language = {en},
	number = {5},
	urldate = {2024-02-03},
	journal = {Nature Reviews Neuroscience},
	author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
	month = may,
	year = {2013},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {Molecular neuroscience},
	pages = {365--376},
	file = {Full Text PDF:/Users/luca/Zotero/storage/JW44ZWJJ/Button et al. - 2013 - Power failure why small sample size undermines th.pdf:application/pdf},
}

@article{schafer_meaningfulness_2019-3,
	title = {The {Meaningfulness} of {Effect} {Sizes} in {Psychological} {Research}: {Differences} {Between} {Sub}-{Disciplines} and the {Impact} of {Potential} {Biases}},
	volume = {10},
	shorttitle = {The {Meaningfulness} of {Effect} {Sizes} in {Psychological} {Research}},
	doi = {10.3389/fpsyg.2019.00813},
	abstract = {Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes-when is an effect small, medium, or large?-has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median r = 0.36) were much larger than effects from the latter (median r = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.},
	journal = {Frontiers in Psychology},
	author = {Schäfer, Thomas and Schwarz, Marcus},
	month = apr,
	year = {2019},
	keywords = {notion},
	pages = {1--13},
	file = {Full Text PDF:/Users/luca/Zotero/storage/D2ZA76PZ/Schäfer und Schwarz - 2019 - The Meaningfulness of Effect Sizes in Psychologica.pdf:application/pdf},
}

@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2023-04-24},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	note = {Publisher: Public Library of Science},
	keywords = {Finance, notion, Metaanalysis, Cancer risk factors, Genetic epidemiology, Genetics of disease, Randomized controlled trials, Research design, Schizophrenia},
	pages = {e124},
	file = {Full Text PDF:/Users/luca/Zotero/storage/DVBFMXRN/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf},
}

@article{collins_using_2021,
	title = {Using and {Understanding} {Power} in {Psychological} {Research}: {A} {Survey} {Study}},
	volume = {7},
	issn = {2474-7394},
	shorttitle = {Using and {Understanding} {Power} in {Psychological} {Research}},
	url = {https://doi.org/10.1525/collabra.28250},
	doi = {10.1525/collabra.28250},
	abstract = {Statistical power is key to planning studies if understood and used correctly. Power is the probability of obtaining a statistically significant p-value, given a set alpha, sample size, and population effect size. The literature suggests that psychology studies are underpowered due to small sample sizes, and that researchers do not hold accurate intuitions about sensible sample sizes and associated levels of power. In this study, we surveyed 214 psychological researchers, and asked them about their experiences of using a priori power analysis, effect size estimation methods, post hoc power, and their understanding of what the term “power” actually means. Power analysis use was high, although participants reported difficulties with complex research designs, and effect size estimation. Participants also typically could not accurately define power. If psychological researchers are expected to compute a priori power analyses to plan their research, clearer educational material and guidelines should be made available.},
	number = {1},
	urldate = {2023-12-01},
	journal = {Collabra: Psychology},
	author = {Collins, Elizabeth and Watt, Roger},
	month = oct,
	year = {2021},
	keywords = {notion},
	pages = {28250},
	file = {Full Text PDF:/Users/luca/Zotero/storage/QRSPE7Z3/Collins und Watt - 2021 - Using and Understanding Power in Psychological Res.pdf:application/pdf},
}

@article{siegel_times_2022,
	title = {Times are changing, bias isn’t: {A} meta-meta-analysis on publication bias detection practices, prevalence rates, and predictors in industrial/organizational psychology},
	volume = {107},
	issn = {1939-1854},
	shorttitle = {Times are changing, bias isn’t},
	doi = {10.1037/apl0000991},
	abstract = {Effect misestimations plague Psychological Science, but advances in the identification of dissemination biases in general and publication bias in particular have helped in dealing with biased effects in the literature. However, the application of publication bias detection methods appears to be not equally prevalent across subdisciplines. It has been suggested that particularly in I/O Psychology, appropriate publication bias detection methods are underused. In this meta-meta-analysis, we present prevalence estimates, predictors, and time trends of publication bias in 128 meta-analyses that were published in the Journal of Applied Psychology (7,263 effect sizes, 3,000,000 + participants). Moreover, we reanalyzed data of 87 meta-analyses and applied nine standard and more modern publication bias detection methods. We show that (a) the bias detection method applications are underused (only 41\% of meta-analyses use at least one method) but have increased in recent years, (b) those meta-analyses that apply such methods now use more, but mostly inappropriate methods, and (c) the prevalence of potential publication bias is concerning but mostly remains undetected. Although our results indicate somewhat of a trend toward higher bias awareness, they substantiate concerns about potential publication bias in I/O Psychology, warranting increased researcher awareness about appropriate and state-of-the-art bias detection and triangulation. Embracing open science practices such as data sharing or study preregistration is needed to raise reproducibility and ultimately strengthen Psychological Science in general and I/O Psychology in particular. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	journal = {Journal of Applied Psychology},
	author = {Siegel, Magdalena and Eder, Junia Sophia Nur and Wicherts, Jelte M. and Pietschnig, Jakob},
	year = {2022},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Behavioral Sciences, notion, Awareness, Data Sharing, Epidemics, Industrial and Organizational Psychology, Industrialization, Time Estimation, Trends},
	pages = {2013--2039},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/V8IIM6F6/Siegel et al. - 2020 - Times are Changing, Bias isn’t A Meta-Meta-Analys.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/RJD4PMEG/2022-17310-001.html:text/html},
}

@article{siegel_times_2022-1,
	title = {Times are changing, bias isn’t: {A} meta-meta-analysis on publication bias detection practices, prevalence rates, and predictors in industrial/organizational psychology},
	volume = {107},
	issn = {1939-1854},
	shorttitle = {Times are changing, bias isn’t},
	doi = {10.1037/apl0000991},
	abstract = {Effect misestimations plague Psychological Science, but advances in the identification of dissemination biases in general and publication bias in particular have helped in dealing with biased effects in the literature. However, the application of publication bias detection methods appears to be not equally prevalent across subdisciplines. It has been suggested that particularly in I/O Psychology, appropriate publication bias detection methods are underused. In this meta-meta-analysis, we present prevalence estimates, predictors, and time trends of publication bias in 128 meta-analyses that were published in the Journal of Applied Psychology (7,263 effect sizes, 3,000,000 + participants). Moreover, we reanalyzed data of 87 meta-analyses and applied nine standard and more modern publication bias detection methods. We show that (a) the bias detection method applications are underused (only 41\% of meta-analyses use at least one method) but have increased in recent years, (b) those meta-analyses that apply such methods now use more, but mostly inappropriate methods, and (c) the prevalence of potential publication bias is concerning but mostly remains undetected. Although our results indicate somewhat of a trend toward higher bias awareness, they substantiate concerns about potential publication bias in I/O Psychology, warranting increased researcher awareness about appropriate and state-of-the-art bias detection and triangulation. Embracing open science practices such as data sharing or study preregistration is needed to raise reproducibility and ultimately strengthen Psychological Science in general and I/O Psychology in particular. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	journal = {Journal of Applied Psychology},
	author = {Siegel, Magdalena and Eder, Junia Sophia Nur and Wicherts, Jelte M. and Pietschnig, Jakob},
	year = {2022},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Behavioral Sciences, notion, Awareness, Data Sharing, Epidemics, Industrial and Organizational Psychology, Industrialization, Time Estimation, Trends},
	pages = {2013--2039},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/WM3RP7G9/siegel_etal_2021.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/G4HCZTXE/2022-17310-001.html:text/html},
}

@article{mone_perceptions_1996,
	title = {The {Perceptions} and {Usage} of {Statistical} {Power} in {Applied} {Psychology} and {Management} {Research}},
	volume = {49},
	issn = {1744-6570},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1744-6570.1996.tb01793.x},
	doi = {10.1111/j.1744-6570.1996.tb01793.x},
	abstract = {We first assess the current level of statistical power across articles in seven leading journals that represent a broad sample of applied psychology and management research. We next survey the authors of these articles to examine their perceptions and usage of statistical power analysis. Finally, we examine the perceptions and usage of power analysis in a survey of authors of regression-based research appearing in leading journals. Findings from the assessment of power and surveys of researchers indicate that power analyses are not typically conducted, researchers perceive little need for statistical power, and power in published research is low. We conclude by discussing implications of low power for the field and recommending avenues for improving researchers' awareness and usage of statistical power.},
	language = {en},
	number = {1},
	urldate = {2023-12-01},
	journal = {Personnel Psychology},
	author = {Mone, Mark A. and Mueller, George C. and Mauland, Wade},
	year = {1996},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1744-6570.1996.tb01793.x},
	keywords = {notion},
	pages = {103--120},
	file = {Full Text PDF:/Users/luca/Zotero/storage/JC66EXVA/Mone et al. - 1996 - The Perceptions and Usage of Statistical Power in .pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/V638ZHZS/j.1744-6570.1996.tb01793.html:text/html},
}

@article{rossi_statistical_1990,
	title = {Statistical power of psychological research: what have we gained in 20 years?},
	volume = {58},
	issn = {0022-006X},
	shorttitle = {Statistical power of psychological research},
	doi = {10.1037//0022-006x.58.5.646},
	abstract = {Power was calculated for 6,155 statistical tests in 221 journal articles published in the 1982 volumes of the Journal of Abnormal Psychology, Journal of Consulting and Clinical Psychology, and Journal of Personality and Social Psychology. Power to detect small, medium, and large effects was .17, .57, and .83, respectively. 20 years after Cohen (1962) conducted the first power survey, the power of psychological research is still low. The implications of these results concerning the proliferation of Type I errors in the published literature, the failure of replication studies, and the interpretation of null (negative) results are emphasized. An example is given of the use of power analysis to help interpret null results by setting probable upper bounds on the magnitudes of effects. Limitations of statistical power analyses, suggestions for future research, sources of computational information, and recommendations for improving power are discussed.},
	language = {eng},
	number = {5},
	journal = {Journal of Consulting and Clinical Psychology},
	author = {Rossi, J. S.},
	month = oct,
	year = {1990},
	pmid = {2254513},
	keywords = {Humans, Psychometrics, Software, notion, Research, Models, Statistical},
	pages = {646--656},
}

@article{nosek_replicability_2022,
	title = {Replicability, {Robustness}, and {Reproducibility} in {Psychological} {Science}},
	volume = {73},
	url = {https://doi.org/10.1146/annurev-psych-020821-114157},
	doi = {10.1146/annurev-psych-020821-114157},
	abstract = {Replication—an important, uncommon, and misunderstood practice—is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.},
	number = {1},
	urldate = {2023-05-19},
	journal = {Annual Review of Psychology},
	author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aurélien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Michèle B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Schönbrodt, Felix D. and Vazire, Simine},
	year = {2022},
	pmid = {34665669},
	note = {\_eprint: https://doi.org/10.1146/annurev-psych-020821-114157},
	keywords = {notion},
	pages = {719--748},
	file = {Full Text PDF:/Users/luca/Zotero/storage/GAW8WGKR/Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf:application/pdf},
}

@article{song_publication_2013-1,
	title = {Publication bias: {What} is it? {How} do we measure it? {How} do we avoid it?},
	volume = {5},
	shorttitle = {Publication bias},
	doi = {10.2147/OAJCT.S34419},
	abstract = {Abstract: Publication bias occurs when results of published studies are systematically different from results of unpublished studies. The term "dissemination bias" has also been recommended to describe all forms of biases in the research-dissemination process, including outcome-reporting bias, time-lag bias, gray-literature bias, full-publication bias, language bias, citation bias, and media-attention bias. We can measure publication bias by comparing the results of published and unpublished studies addressing the same question. Following up cohorts of studies from inception and comparing publication levels in studies with statistically significant or "positive" results suggested greater odds of formal publication in those with such results, compared to those without. Within reviews, funnel plots and related statistical methods can be used to indicate presence or absence of publication bias, although these can be unreliable in many circumstances. Methods of avoiding publication bias, by identifying and including unpublished outcomes and unpublished studies, are discussed and evaluated. These include searching without limiting by outcome, searching prospective trials registers, searching informal sources, including meeting abstracts and PhD theses, searching regulatory body websites, contacting authors of included studies, and contacting pharmaceutical or medical device companies for further studies. Adding unpublished studies often alters effect sizes, but may not always eliminate publication bias. The compulsory registration of all clinical trials at inception is an important move forward, but it can be difficult for reviewers to access data from unpublished studies located this way. Publication bias may be reduced by journals by publishing high-quality studies regardless of novelty or unexciting results, and by publishing protocols or full-study data sets. No single step can be relied upon to fully overcome the complex actions involved in publication bias, and a multipronged approach is required by researchers, patients, journal editors, peer reviewers, research sponsors, research ethics committees, and regulatory and legislation authorities.},
	journal = {Open Access Journal of Clinical Trials},
	author = {Song, Fujian and Hooper, Lee and YK, Loke},
	month = jul,
	year = {2013},
	keywords = {notion},
	pages = {51--81},
	file = {Full Text PDF:/Users/luca/Zotero/storage/7X2RBT62/Song et al. - 2013 - Publication bias what is it How do we measure it.pdf:application/pdf},
}

@article{song_publication_2013-2,
	title = {Publication bias: {What} is it? {How} do we measure it? {How} do we avoid it?},
	volume = {5},
	shorttitle = {Publication bias},
	doi = {10.2147/OAJCT.S34419},
	abstract = {Abstract: Publication bias occurs when results of published studies are systematically different from results of unpublished studies. The term "dissemination bias" has also been recommended to describe all forms of biases in the research-dissemination process, including outcome-reporting bias, time-lag bias, gray-literature bias, full-publication bias, language bias, citation bias, and media-attention bias. We can measure publication bias by comparing the results of published and unpublished studies addressing the same question. Following up cohorts of studies from inception and comparing publication levels in studies with statistically significant or "positive" results suggested greater odds of formal publication in those with such results, compared to those without. Within reviews, funnel plots and related statistical methods can be used to indicate presence or absence of publication bias, although these can be unreliable in many circumstances. Methods of avoiding publication bias, by identifying and including unpublished outcomes and unpublished studies, are discussed and evaluated. These include searching without limiting by outcome, searching prospective trials registers, searching informal sources, including meeting abstracts and PhD theses, searching regulatory body websites, contacting authors of included studies, and contacting pharmaceutical or medical device companies for further studies. Adding unpublished studies often alters effect sizes, but may not always eliminate publication bias. The compulsory registration of all clinical trials at inception is an important move forward, but it can be difficult for reviewers to access data from unpublished studies located this way. Publication bias may be reduced by journals by publishing high-quality studies regardless of novelty or unexciting results, and by publishing protocols or full-study data sets. No single step can be relied upon to fully overcome the complex actions involved in publication bias, and a multipronged approach is required by researchers, patients, journal editors, peer reviewers, research sponsors, research ethics committees, and regulatory and legislation authorities.},
	journal = {Open Access Journal of Clinical Trials},
	author = {Song, Fujian and Hooper, Lee and YK, Loke},
	month = jul,
	year = {2013},
	keywords = {notion},
	pages = {51--81},
	file = {Full Text PDF:/Users/luca/Zotero/storage/KHDRQ6US/Song et al. - 2013 - Publication bias what is it How do we measure it.pdf:application/pdf},
}

@article{olsson_publication_2023,
	title = {Publication bias, time-lag bias, and place-of-publication bias in social intervention research: {An} exploratory study of 527 {Swedish} articles published between 1990–2019},
	volume = {18},
	issn = {1932-6203},
	shorttitle = {Publication bias, time-lag bias, and place-of-publication bias in social intervention research},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281110},
	doi = {10.1371/journal.pone.0281110},
	abstract = {Publication and related biases constitute serious threats to the validity of research synthesis. If research syntheses are based on a biased selection of the available research, there is an increased risk of producing misleading results. The purpose fo this study is to explore the extent of positive outcome bias, time-lag bias, and place-of-publication bias in published research on the effects of psychological, social, and behavioral interventions. The results are based on 527 Swedish outcome trials published in peer-reviewed journals between 1990 and 2019. We found no difference in the number of studies reporting significant compared to non-significant findings or in the number of studies reporting strong effect sizes in the published literature. We found no evidence of time-lag bias or place-of-publication bias in our results. The average reported effect size remained constant over time as did the proportion of studies reporting significant effects.},
	language = {en},
	number = {2},
	urldate = {2023-06-13},
	journal = {PLOS ONE},
	author = {Olsson, Tina M. and Sundell, Knut},
	month = feb,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {notion, Publication ethics, Bibliometrics, Scientific publishing, Mental health and psychiatry, Social psychology, Social research, Social sciences, Sweden},
	pages = {e0281110},
	file = {Full Text PDF:/Users/luca/Zotero/storage/3C9W2AWE/Olsson und Sundell - 2023 - Publication bias, time-lag bias, and place-of-publ.pdf:application/pdf},
}

@article{ferguson_publication_2012,
	title = {Publication bias in psychological science: {Prevalence}, methods for identifying and controlling, and implications for the use of meta-analyses},
	volume = {17},
	issn = {1939-1463},
	shorttitle = {Publication bias in psychological science},
	doi = {10.1037/a0024445},
	abstract = {The issue of publication bias in psychological science is one that has remained difficult to address despite decades of discussion and debate. The current article examines a sample of 91 recent meta-analyses published in American Psychological Association and Association for Psychological Science journals and the methods used in these analyses to identify and control for publication bias. Of the 91 studies analyzed, 64 (70\%) made some effort to analyze publication bias, and 26 (41\%) reported finding evidence of bias. Approaches to controlling publication bias were heterogeneous among studies. Of these studies, 57 (63\%) attempted to find unpublished studies to control for publication bias. Nonetheless, those studies that included unpublished studies were just as likely to find evidence for publication bias as those that did not. Furthermore, authors of meta-analyses themselves were overrepresented in unpublished studies acquired, as compared with published studies, suggesting that searches for unpublished studies may increase rather than decrease some sources of bias. A subset of 48 meta-analyses for which study sample sizes and effect sizes were available was further analyzed with a conservative and newly developed tandem procedure of assessing publication bias. Results indicated that publication bias was worrisome in about 25\% of meta-analyses. Meta-analyses that included unpublished studies were more likely to show bias than those that did not, likely due to selection bias in unpublished literature searches. Sources of publication bias and implications for the use of meta-analysis are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Psychological Methods},
	author = {Ferguson, Christopher J. and Brannick, Michael T.},
	year = {2012},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Psychology, notion, Statistics, Meta Analysis, Scientific Communication},
	pages = {120--128},
	file = {Volltext:/Users/luca/Zotero/storage/GP88KGCR/Ferguson und Brannick - 2012 - Publication bias in psychological science Prevale.pdf:application/pdf},
}

@article{van_aert_publication_2019,
	title = {Publication bias examined in meta-analyses from psychology and medicine: {A} meta-meta-analysis},
	volume = {14},
	issn = {1932-6203},
	shorttitle = {Publication bias examined in meta-analyses from psychology and medicine},
	url = {https://dx.plos.org/10.1371/journal.pone.0215052},
	doi = {10.1371/journal.pone.0215052},
	language = {en},
	number = {4},
	urldate = {2023-11-30},
	journal = {PLOS ONE},
	author = {Van Aert, Robbie C. M. and Wicherts, Jelte M. and Van Assen, Marcel A. L. M.},
	editor = {Macleod, Malcolm R.},
	month = apr,
	year = {2019},
	keywords = {notion},
	pages = {e0215052},
	file = {Full Text PDF:/Users/luca/Zotero/storage/BUMRDEZX/Van Aert et al. - 2019 - Publication bias examined in meta-analyses from ps.pdf:application/pdf},
}

@article{ioannidis_publication_2014,
	title = {Publication and other reporting biases in cognitive sciences: detection, prevalence, and prevention},
	volume = {18},
	issn = {1364-6613},
	shorttitle = {Publication and other reporting biases in cognitive sciences},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661314000540},
	doi = {10.1016/j.tics.2014.02.010},
	abstract = {Recent systematic reviews and empirical evaluations of the cognitive sciences literature suggest that publication and other reporting biases are prevalent across diverse domains of cognitive science. In this review, we summarize the various forms of publication and reporting biases and other questionable research practices, and overview the available methods for probing into their existence. We discuss the available empirical evidence for the presence of such biases across the neuroimaging, animal, other preclinical, psychological, clinical trials, and genetics literature in the cognitive sciences. We also highlight emerging solutions (from study design to data analyses and reporting) to prevent bias and improve the fidelity in the field of cognitive science research.},
	number = {5},
	urldate = {2023-08-28},
	journal = {Trends in Cognitive Sciences},
	author = {Ioannidis, John P. A. and Munafò, Marcus R. and Fusar-Poli, Paolo and Nosek, Brian A. and David, Sean P.},
	month = may,
	year = {2014},
	keywords = {notion, bias, publication bias, cognitive sciences, neuroscience, reporting bias},
	pages = {235--241},
	file = {ScienceDirect Full Text PDF:/Users/luca/Zotero/storage/J28T5KZS/Ioannidis et al. - 2014 - Publication and other reporting biases in cognitiv.pdf:application/pdf},
}

@article{nelson_psychologys_2018,
	title = {Psychology’s renaissance},
	volume = {69},
	issn = {1545-2085},
	doi = {10.1146/annurev-psych-122216-011836},
	abstract = {In 2010–2012, a few largely coincidental events led experimental psychologists to realize that their approach to collecting, analyzing, and reporting data made it too easy to publish false-positive findings. This sparked a period of methodological reflection that we review here and call Psychology’s Renaissance. We begin by describing how psychologists’ concerns with publication bias shifted from worrying about file-drawered studies to worrying about p-hacked analyses. We then review the methodological changes that psychologists have proposed and, in some cases, embraced. In describing how the renaissance has unfolded, we attempt to describe different points of view fairly but not neutrally, so as to identify the most promising paths forward. In so doing, we champion disclosure and preregistration, express skepticism about most statistical solutions to publication bias, take positions on the analysis and interpretation of replication failures, and contend that meta-analytical thinking increases the prevalence of false positives. Our general thesis is that the scientific practices of experimental psychologists have improved dramatically. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	journal = {Annual Review of Psychology},
	author = {Nelson, Leif D. and Simmons, Joseph and Simonsohn, Uri},
	year = {2018},
	note = {Place: US
Publisher: Annual Reviews},
	keywords = {Experimentation, Psychology, notion, Publication Bias, Scientific Communication, Methodology},
	pages = {511--534},
	file = {Snapshot:/Users/luca/Zotero/storage/5GH3XGPS/2018-04521-021.html:text/html;Volltext:/Users/luca/Zotero/storage/SDPDNCCU/Nelson et al. - 2018 - Psychology’s renaissance.pdf:application/pdf},
}

@article{van_den_akker_preregistration_2021,
	title = {Preregistration of secondary data analysis: {A} template and tutorial},
	volume = {5},
	issn = {2003-2714},
	shorttitle = {Preregistration of secondary data analysis},
	doi = {10.15626/MP.2020.2625},
	abstract = {Preregistration has been lauded as one of the solutions to the so-called ‘crisis of confidence’ in the social sciences and has therefore gained popularity in recent years. However, the current guidelines for preregistration have been developed primarily for studies where new data will be collected. Yet, preregistering secondary data analyses--- where new analyses are proposed for existing data---is just as important, given that researchers’ hypotheses and analyses may be biased by their prior knowledge of the data. The need for proper guidance in this area is especially desirable now that data is increasingly shared publicly. In this tutorial, we present a template specifically designed for the preregistration of secondary data analyses and provide comments and a worked example that may help with using the template effectively. Through this illustration, we show that completing such a template is feasible, helps limit researcher degrees of freedom, and may make researchers more deliberate in their data selection and analysis efforts.},
	number = {2625},
	journal = {Meta-Psychology},
	author = {van den Akker, Olmo and Weston, Sara and Campbell, Lorne and Chopik, Bill and Damian, Rodica and Davis-Kean, Pamela and Hall, Andrew and Kosie, Jessica and Kruse, Elliott and Olsen, Jerome and Ritchie, Stuart and Valentine, KD and Veer, Anna Van 't and Bakker, Marjan},
	year = {2021},
	file = {Volltext:/Users/luca/Zotero/storage/7WJ4AAU7/van den Akker et al. - 2021 - Preregistration of secondary data analysis A temp.pdf:application/pdf},
}

@article{moss_modelling_2023,
	title = {Modelling publication bias and p-hacking},
	volume = {79},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13560},
	doi = {10.1111/biom.13560},
	abstract = {Publication bias and p-hacking are two well-known phenomena that strongly affect the scientific literature and cause severe problems in meta-analyses. Due to these phenomena, the assumptions of meta-analyses are seriously violated and the results of the studies cannot be trusted. While publication bias is very often captured well by the weighting function selection model, p-hacking is much harder to model and no definitive solution has been found yet. In this paper, we advocate the selection model approach to model publication bias and propose a mixture model for p-hacking. We derive some properties for these models, and we compare them formally and through simulations. Finally, two real data examples are used to show how the models work in practice.},
	language = {en},
	number = {1},
	urldate = {2023-12-27},
	journal = {Biometrics},
	author = {Moss, Jonas and De Bin, Riccardo},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13560},
	keywords = {meta-analysis, notion, questionable research practices, file drawer problem, fishing for significance, selection bias},
	pages = {319--331},
	file = {Full Text PDF:/Users/luca/Zotero/storage/9R2RHHW8/Moss und De Bin - 2023 - Modelling publication bias and p-hacking.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/4HUSHC9B/biom.html:text/html},
}

@article{hedges_modeling_1992,
	title = {Modeling {Publication} {Selection} {Effects} in {Meta}-{Analysis}},
	volume = {7},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-7/issue-2/Modeling-Publication-Selection-Effects-in-Meta-Analysis/10.1214/ss/1177011364.full},
	doi = {10.1214/ss/1177011364},
	abstract = {Publication selection effects arise in meta-analysis when the effect magnitude estimates are observed in (available from) only a subset of the studies that were actually conducted and the probability that an estimate is observed is related to the size of that estimate. Such selection effects can lead to substantial bias in estimates of effect magnitude. Research on the selection process suggests that much of the selection occurs because researchers, reviewers and editors view the results of studies as more conclusive when they are more highly statistically significant. This suggests a model of the selection process that depends on effect magnitude via the p-value or significance level. A model of the selection process involving a step function relating the p-value to the probability of selection is introduced in the context of a random effects model for meta-analysis. The model permits estimation of a weight function representing selection along the mean and variance of effects. Some ideas for graphical procedures and a test for publication selection are also introduced. The method is then applied to a meta-analysis of test validity studies.},
	number = {2},
	urldate = {2023-12-27},
	journal = {Statistical Science},
	author = {Hedges, Larry V.},
	month = may,
	year = {1992},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {publication bias, Meta-analysis, file-drawer problem, random effects models, selection models, weight function models},
	pages = {246--255},
	file = {Full Text PDF:/Users/luca/Zotero/storage/7FS3IRZT/Hedges - 1992 - Modeling Publication Selection Effects in Meta-Ana.pdf:application/pdf},
}

@article{nakagawa_methods_2022,
	title = {Methods for testing publication bias in ecological and evolutionary meta-analyses},
	volume = {13},
	copyright = {© 2021 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13724},
	doi = {10.1111/2041-210X.13724},
	abstract = {Publication bias threatens the validity of quantitative evidence from meta-analyses as it results in some findings being overrepresented in meta-analytic datasets because they are published more frequently or sooner (e.g. ‘positive’ results). Unfortunately, methods to test for the presence of publication bias, or assess its impact on meta-analytic results, are unsuitable for datasets with high heterogeneity and non-independence, as is common in ecology and evolutionary biology. We first review both classic and emerging publication bias tests (e.g. funnel plots, Egger's regression, cumulative meta-analysis, fail-safe N, trim-and-fill tests, p-curve and selection models), showing that some tests cannot handle heterogeneity, and, more importantly, none of the methods can deal with non-independence. For each method, we estimate current usage in ecology and evolutionary biology, based on a representative sample of 102 meta-analyses published in the last 10 years. Then, we propose a new method using multilevel meta-regression, which can model both heterogeneity and non-independence, by extending existing regression-based methods (i.e. Egger's regression). We describe how our multilevel meta-regression can test not only publication bias, but also time-lag bias, and how it can be supplemented by residual funnel plots. Overall, we provide ecologists and evolutionary biologists with practical recommendations on which methods are appropriate to employ given independent and non-independent effect sizes. No method is ideal, and more simulation studies are required to understand how Type 1 and Type 2 error rates are impacted by complex data structures. Still, the limitations of these methods do not justify ignoring publication bias in ecological and evolutionary meta-analyses.},
	language = {en},
	number = {1},
	urldate = {2023-07-07},
	journal = {Methods in Ecology and Evolution},
	author = {Nakagawa, Shinichi and Lagisz, Malgorzata and Jennions, Michael D. and Koricheva, Julia and Noble, Daniel W. A. and Parker, Timothy H. and Sánchez-Tójar, Alfredo and Yang, Yefeng and O'Dea, Rose E.},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13724},
	keywords = {notion, p-hacking, selection bias, decline effect, effective sample size, multilevel meta-analysis, outcome reporting bias, radial plot, time-lag bias},
	pages = {4--21},
	file = {Full Text PDF:/Users/luca/Zotero/storage/7CVJQ4MV/Nakagawa et al. - 2022 - Methods for testing publication bias in ecological.pdf:application/pdf},
}

@article{john_measuring_2012,
	title = {Measuring the {Prevalence} of {Questionable} {Research} {Practices} {With} {Incentives} for {Truth} {Telling}},
	volume = {23},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/0956797611430953},
	doi = {10.1177/0956797611430953},
	abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
	number = {5},
	urldate = {2023-03-20},
	journal = {Psychological Science},
	author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
	month = may,
	year = {2012},
	note = {Publisher: SAGE Publications Inc},
	keywords = {notion},
	pages = {524--532},
}

@article{maxwell_is_2015,
	title = {Is psychology suffering from a replication crisis? {What} does “failure to replicate” really mean?},
	volume = {70},
	issn = {1935-990X},
	shorttitle = {Is psychology suffering from a replication crisis?},
	doi = {10.1037/a0039400},
	abstract = {Psychology has recently been viewed as facing a replication crisis because efforts to replicate past study findings frequently do not show the same result. Often, the first study showed a statistically significant result but the replication does not. Questions then arise about whether the first study results were false positives, and whether the replication study correctly indicates that there is truly no effect after all. This article suggests these so-called failures to replicate may not be failures at all, but rather are the result of low statistical power in single replication studies, and the result of failure to appreciate the need for multiple replications in order to have enough power to identify true effects. We provide examples of these power problems and suggest some solutions using Bayesian statistics and meta-analysis. Although the need for multiple replication studies may frustrate those who would prefer quick answers to psychology’s alleged crisis, the large sample sizes typically needed to provide firm evidence will almost always require concerted efforts from multiple investigators. As a result, it remains to be seen how many of the recently claimed failures to replicate will be supported or instead may turn out to be artifacts of inadequate sample sizes and single study replications. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {6},
	journal = {American Psychologist},
	author = {Maxwell, Scott E. and Lau, Michael Y. and Howard, George S.},
	year = {2015},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {notion, Experimental Replication, Failure, Statistical Power, Statistical Probability},
	pages = {487--498},
	file = {Snapshot:/Users/luca/Zotero/storage/PAH8IICW/2015-39598-001.html:text/html},
}

@article{andrews_identification_2019,
	title = {Identification of and {Correction} for {Publication} {Bias}},
	volume = {109},
	issn = {0002-8282},
	url = {https://pubs.aeaweb.org/doi/10.1257/aer.20180310},
	doi = {10.1257/aer.20180310},
	abstract = {Some empirical results are more likely to be published than others. Selective publication leads to biased estimates and distorted inference. We propose two approaches for identifying the conditional probability of publication as a function of a study’s results, the first based on systematic replication studies and the second on meta-studies. For known conditional publication probabilities, we propose bias-corrected estimators and confidence sets. We apply our methods to recent replication studies in experimental economics and psychology, and to a meta-study on the effect of the minimum wage. When replication and meta-study data are available, we find similar results from both.(JEL C13, C90, I23, J23, J38, L82)},
	language = {en},
	number = {8},
	urldate = {2023-09-25},
	journal = {American Economic Review},
	author = {Andrews, Isaiah and Kasy, Maximilian},
	month = aug,
	year = {2019},
	keywords = {notion},
	pages = {2766--2794},
	file = {Full Text PDF:/Users/luca/Zotero/storage/D4E6S5AA/Andrews und Kasy - 2019 - Identification of and Correction for Publication B.pdf:application/pdf},
}

@article{renkewitz_how_2019,
	title = {How to {Detect} {Publication} {Bias} in {Psychological} {Research}},
	volume = {227},
	issn = {2190-8370},
	url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000386},
	doi = {10.1027/2151-2604/a000386},
	abstract = {. Publication biases and questionable research practices are assumed to be two of the main causes of low replication rates. Both of these problems lead to severely inflated effect size estimates in meta-analyses. Methodologists have proposed a number of statistical tools to detect such bias in meta-analytic results. We present an evaluation of the performance of six of these tools. To assess the Type I error rate and the statistical power of these methods, we simulated a large variety of literatures that differed with regard to true effect size, heterogeneity, number of available primary studies, and sample sizes of these primary studies; furthermore, simulated studies were subjected to different degrees of publication bias. Our results show that across all simulated conditions, no method consistently outperformed the others. Additionally, all methods performed poorly when true effect sizes were heterogeneous or primary studies had a small chance of being published, irrespective of their results. This suggests that in many actual meta-analyses in psychology, bias will remain undiscovered no matter which detection method is used.},
	number = {4},
	urldate = {2023-11-30},
	journal = {Zeitschrift für Psychologie},
	author = {Renkewitz, Frank and Keiner, Melanie},
	month = oct,
	year = {2019},
	note = {Publisher: Hogrefe Publishing},
	keywords = {meta-analysis, notion, publication bias, bias detection, heterogeneity, optional stopping},
	pages = {261--279},
	file = {Full Text PDF:/Users/luca/Zotero/storage/TSW6R9JI/Renkewitz und Keiner - 2019 - How to Detect Publication Bias in Psychological Re.pdf:application/pdf},
}

@article{linden_heterogeneity_2021,
	title = {Heterogeneity of {Research} {Results}: {A} {New} {Perspective} {From} {Which} to {Assess} and {Promote} {Progress} in {Psychological} {Science}},
	volume = {16},
	issn = {1745-6916},
	shorttitle = {Heterogeneity of {Research} {Results}},
	url = {https://doi.org/10.1177/1745691620964193},
	doi = {10.1177/1745691620964193},
	abstract = {Heterogeneity emerges when multiple close or conceptual replications on the same subject produce results that vary more than expected from the sampling error. Here we argue that unexplained heterogeneity reflects a lack of coherence between the concepts applied and data observed and therefore a lack of understanding of the subject matter. Typical levels of heterogeneity thus offer a useful but neglected perspective on the levels of understanding achieved in psychological science. Focusing on continuous outcome variables, we surveyed heterogeneity in 150 meta-analyses from cognitive, organizational, and social psychology and 57 multiple close replications. Heterogeneity proved to be very high in meta-analyses, with powerful moderators being conspicuously absent. Population effects in the average meta-analysis vary from small to very large for reasons that are typically not understood. In contrast, heterogeneity was moderate in close replications. A newly identified relationship between heterogeneity and effect size allowed us to make predictions about expected heterogeneity levels. We discuss important implications for the formulation and evaluation of theories in psychology. On the basis of insights from the history and philosophy of science, we argue that the reduction of heterogeneity is important for progress in psychology and its practical applications, and we suggest changes to our collective research practice toward this end.},
	number = {2},
	urldate = {2023-11-29},
	journal = {Perspectives on Psychological Science},
	author = {Linden, Audrey Helen and Hönekopp, Johannes},
	month = mar,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	keywords = {notion},
	pages = {358--376},
	file = {Full Text PDF:/Users/luca/Zotero/storage/GU9IGIU7/Linden und Hönekopp - 2021 - Heterogeneity of Research Results A New Perspecti.pdf:application/pdf},
}

@article{schneck_examining_2017,
	title = {Examining publication bias—a simulation-based evaluation of statistical tests on publication bias},
	volume = {5},
	issn = {2167-8359},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5712469/},
	doi = {10.7717/peerj.4115},
	abstract = {Background
Publication bias is a form of scientific misconduct. It threatens the validity of research results and the credibility of science. Although several tests on publication bias exist, no in-depth evaluations are available that examine which test performs best for different research settings.

Methods
Four tests on publication bias, Egger’s test (FAT), p-uniform, the test of excess significance (TES), as well as the caliper test, were evaluated in a Monte Carlo simulation. Two different types of publication bias and its degree (0\%, 50\%, 100\%) were simulated. The type of publication bias was defined either as file-drawer, meaning the repeated analysis of new datasets, or p-hacking, meaning the inclusion of covariates in order to obtain a significant result. In addition, the underlying effect (β = 0, 0.5, 1, 1.5), effect heterogeneity, the number of observations in the simulated primary studies (N = 100, 500), and the number of observations for the publication bias tests (K = 100, 1,000) were varied.

Results
All tests evaluated were able to identify publication bias both in the file-drawer and p-hacking condition. The false positive rates were, with the exception of the 15\%- and 20\%-caliper test, unbiased. The FAT had the largest statistical power in the file-drawer conditions, whereas under p-hacking the TES was, except under effect heterogeneity, slightly better. The CTs were, however, inferior to the other tests under effect homogeneity and had a decent statistical power only in conditions with 1,000 primary studies.

Discussion
The FAT is recommended as a test for publication bias in standard meta-analyses with no or only small effect heterogeneity. If two-sided publication bias is suspected as well as under p-hacking the TES is the first alternative to the FAT. The 5\%-caliper test is recommended under conditions of effect heterogeneity and a large number of primary studies, which may be found if publication bias is examined in a discipline-wide setting when primary studies cover different research problems.},
	urldate = {2023-06-06},
	journal = {PeerJ},
	author = {Schneck, Andreas},
	month = nov,
	year = {2017},
	pmid = {29204324},
	pmcid = {PMC5712469},
	keywords = {notion},
	pages = {e4115},
	file = {PubMed Central Full Text PDF:/Users/luca/Zotero/storage/KVKU62F4/Schneck - 2017 - Examining publication bias—a simulation-based eval.pdf:application/pdf},
}

@article{hedges_estimation_1984,
	title = {Estimation of {Effect} {Size} under {Nonrandom} {Sampling}: {The} {Effects} of {Censoring} {Studies} {Yielding} {Statistically} {Insignificant} {Mean} {Differences}},
	volume = {9},
	issn = {0362-9791},
	shorttitle = {Estimation of {Effect} {Size} under {Nonrandom} {Sampling}},
	url = {https://doi.org/10.3102/10769986009001061},
	doi = {10.3102/10769986009001061},
	abstract = {Quantitative research synthesis usually involves the combination of estimates of the standardized mean difference (effect size) derived from independent research studies. In some cases, effect size estimates are available only if the difference between experimental and control group means is statistically significant. If the quantitative result of a study is observed only when the mean difference is statistically significant, the observed mean difference, variance, and effect size are biased estimators of the corresponding population parameters. The exact distribution of the sample effect size is derived for the case in which only studies yielding statistically significant results may be observed. The maximum likelihood estimator of effect size also is derived under the model in which only significant results are observed. The exact distribution of the maximum likelihood estimator is obtained numerically and is used to study the bias of the maximum likelihood estimator. An empirical sampling study is used to supplement the analytic results.},
	language = {en},
	number = {1},
	urldate = {2023-12-28},
	journal = {Journal of Educational Statistics},
	author = {Hedges, Larry V.},
	month = mar,
	year = {1984},
	note = {Publisher: American Educational Research Association},
	pages = {61--85},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/KU2IEH9B/Hedges - 1984 - Estimation of Effect Size under Nonrandom Sampling.pdf:application/pdf},
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	url = {https://www.science.org/doi/full/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	number = {6251},
	urldate = {2023-12-01},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {notion},
	pages = {aac4716},
	file = {Full Text PDF:/Users/luca/Zotero/storage/NEC76IE4/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf:application/pdf},
}

@article{ingre_estimating_2018,
	title = {Estimating statistical power, posterior probability and publication bias of psychological research using the observed replication rate},
	volume = {5},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.181190},
	doi = {10.1098/rsos.181190},
	abstract = {In this paper, we show how Bayes' theorem can be used to better understand the implications of the 36\% reproducibility rate of published psychological findings reported by the Open Science Collaboration. We demonstrate a method to assess publication bias and show that the observed reproducibility rate was not consistent with an unbiased literature. We estimate a plausible range for the prior probability of this body of research, suggesting expected statistical power in the original studies of 48–75\%, producing (positive) findings that were expected to be true 41–62\% of the time. Publication bias was large, assuming a literature with 90\% positive findings, indicating that negative evidence was expected to have been observed 55–98 times before one negative result was published. These findings imply that even when studied associations are truly NULL, we expect the literature to be dominated by statistically significant findings.},
	number = {9},
	urldate = {2023-12-18},
	journal = {Royal Society Open Science},
	author = {Ingre, Michael and Nilsonne, Gustav},
	month = sep,
	year = {2018},
	note = {Publisher: Royal Society},
	keywords = {notion, selection bias, falsification, prior probability},
	pages = {181190},
	file = {Full Text PDF:/Users/luca/Zotero/storage/WQM7JN5P/Ingre und Nilsonne - 2018 - Estimating statistical power, posterior probabilit.pdf:application/pdf},
}

@article{lane_estimating_1978,
	title = {Estimating effect size: {Bias} resulting from the significance criterion in editorial decisions},
	volume = {31},
	issn = {2044-8317},
	shorttitle = {Estimating effect size},
	doi = {10.1111/j.2044-8317.1978.tb00578.x},
	abstract = {Experiments that find larger differences between groups than actually exist in the population are more likely to pass stringent tests of significance and be published than experiments that find smaller differences. Published measures of the magnitude of experimental effects will therefore tend to overestimate these effects. This bias was investigated as a function of sample size, actual population difference, and alpha level. The overestimation of experimental effects was found to be quite large with the commonly employed significance levels of 5\% and 1\%. Further, the recently recommended measure, ω–2, was found to depend more heavily on the alpha level employed than the true population ω–2 value. Hence, it is concluded that effect size estimation is impractical unless scientific journals drop the consideration of statistical significance as one of the criteria of publication. (10 ref) (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {2},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Lane, David M. and Dunlap, William P.},
	year = {1978},
	note = {Place: United Kingdom
Publisher: British Psychological Society},
	keywords = {Scientific Communication, Statistical Significance},
	pages = {107--112},
	file = {Snapshot:/Users/luca/Zotero/storage/85GK4MRW/1980-21926-001.html:text/html},
}

@article{hedges_estimating_1996,
	title = {Estimating {Effect} {Size} {Under} {Publication} {Bias}: {Small} {Sample} {Properties} and {Robustness} of a {Random} {Effects} {Selection} {Model}},
	volume = {21},
	issn = {1076-9986},
	shorttitle = {Estimating {Effect} {Size} {Under} {Publication} {Bias}},
	url = {https://doi.org/10.3102/10769986021004299},
	doi = {10.3102/10769986021004299},
	abstract = {When there is publication bias, studies yielding large p values, and hence small effect estimates, are less likely to be published, which leads to biased estimates of effects in meta-analysis. We investigate a selection model based on one-tailed p values in the context of a random effects model. The procedure both models the selection process and corrects for the consequences of selection on estimates of the mean and variance of effect parameters. A test of the statistical significance of selection is also provided. The small sample properties of the method are evaluated by means of simulations, and the asymptotic theory is found to be reasonably accurate under correct model specification and plausible conditions. The method substantially reduces bias due to selection when model specification is correct, but the variance of estimates is increased; thus mean squared error is reduced only when selection produces substantial bias. The robustness of the method to violations of assumptions about the form of the distribution of the random effects is also investigated via simulation, and the model-corrected estimates of the mean effect are generally found to be much less biased than the uncorrected estimates. The significance test for selection bias, however, is found to be highly nonrobust, rejecting at up to 10 times the nominal rate when there is no selection but the distribution of the effects is incorrectly specified.},
	language = {en},
	number = {4},
	urldate = {2023-12-28},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Hedges, Larry V. and Vevea, Jack L.},
	month = dec,
	year = {1996},
	note = {Publisher: American Educational Research Association},
	keywords = {notion},
	pages = {299--332},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/5WNK8SSP/Hedges und Vevea - 1996 - Estimating Effect Size Under Publication Bias Sma.pdf:application/pdf},
}

@article{lakens_equivalence_2017,
	title = {Equivalence {Tests}: {A} {Practical} {Primer} for t {Tests}, {Correlations}, and {Meta}-{Analyses}},
	volume = {8},
	issn = {1948-5506},
	shorttitle = {Equivalence {Tests}},
	url = {https://doi.org/10.1177/1948550617697177},
	doi = {10.1177/1948550617697177},
	abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
	language = {en},
	number = {4},
	urldate = {2024-01-28},
	journal = {Social Psychological and Personality Science},
	author = {Lakens, Daniël},
	month = may,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	keywords = {notion},
	pages = {355--362},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/FFT5TWIM/Lakens - 2017 - Equivalence Tests A Practical Primer for t Tests,.pdf:application/pdf},
}

@article{lakens_equivalence_2018,
	title = {Equivalence {Testing} for {Psychological} {Research}: {A} {Tutorial}},
	volume = {1},
	issn = {2515-2459},
	shorttitle = {Equivalence {Testing} for {Psychological} {Research}},
	url = {https://doi.org/10.1177/2515245918770963},
	doi = {10.1177/2515245918770963},
	abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
	language = {en},
	number = {2},
	urldate = {2024-01-22},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Lakens, Daniël and Scheel, Anne M. and Isager, Peder M.},
	month = jun,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	pages = {259--269},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/UPDF82T8/Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf:application/pdf},
}

@article{elwert_endogenous_2014,
	title = {Endogenous {Selection} {Bias}: {The} {Problem} of {Conditioning} on a {Collider} {Variable}},
	volume = {40},
	shorttitle = {Endogenous {Selection} {Bias}},
	url = {https://doi.org/10.1146/annurev-soc-071913-043455},
	doi = {10.1146/annurev-soc-071913-043455},
	abstract = {Endogenous selection bias is a central problem for causal inference. Recognizing the problem, however, can be difficult in practice. This article introduces a purely graphical way of characterizing endogenous selection bias and of understanding its consequences (Hernán et al. 2004). We use causal graphs (direct acyclic graphs, or DAGs) to highlight that endogenous selection bias stems from conditioning (e.g., controlling, stratifying, or selecting) on a so-called collider variable, i.e., a variable that is itself caused by two other variables, one that is (or is associated with) the treatment and another that is (or is associated with) the outcome. Endogenous selection bias can result from direct conditioning on the outcome variable, a post-outcome variable, a post-treatment variable, and even a pre-treatment variable. We highlight the difference between endogenous selection bias, common-cause confounding, and overcontrol bias and discuss numerous examples from social stratification, cultural sociology, social network analysis, political sociology, social demography, and the sociology of education.},
	number = {1},
	urldate = {2024-01-24},
	journal = {Annual Review of Sociology},
	author = {Elwert, Felix and Winship, Christopher},
	year = {2014},
	pmid = {30111904},
	note = {\_eprint: https://doi.org/10.1146/annurev-soc-071913-043455},
	pages = {31--53},
	file = {Full Text PDF:/Users/luca/Zotero/storage/BQBCDI6T/Elwert und Winship - 2014 - Endogenous Selection Bias The Problem of Conditio.pdf:application/pdf},
}

@article{szucs_empirical_2017,
	title = {Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature},
	volume = {15},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.2000797},
	doi = {10.1371/journal.pbio.2000797},
	language = {en},
	number = {3},
	urldate = {2023-03-20},
	journal = {PLOS Biology},
	author = {Szucs, Denes and Ioannidis, John P. A.},
	editor = {Wagenmakers, Eric-Jan},
	month = mar,
	year = {2017},
	keywords = {notion},
	pages = {e2000797},
	file = {Volltext:/Users/luca/Zotero/storage/2IHA2K2K/Szucs und Ioannidis - 2017 - Empirical assessment of published effect sizes and.pdf:application/pdf},
}

@article{szucs_empirical_2017-1,
	title = {Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature},
	volume = {15},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.2000797},
	doi = {10.1371/journal.pbio.2000797},
	language = {en},
	number = {3},
	urldate = {2023-03-20},
	journal = {PLOS Biology},
	author = {Szucs, Denes and Ioannidis, John P. A.},
	editor = {Wagenmakers, Eric-Jan},
	month = mar,
	year = {2017},
	keywords = {notion},
	pages = {e2000797},
	file = {Volltext:/Users/luca/Zotero/storage/QQVSTCD7/Szucs und Ioannidis - 2017 - Empirical assessment of published effect sizes and.pdf:application/pdf},
}

@book{harrer_doing_2021,
	address = {Boca Raton, FL and London},
	edition = {1st},
	title = {Doing {Meta}-{Analysis} {With} {R}: {A} {Hands}-{On} {Guide}},
	isbn = {978-0-367-61007-4},
	publisher = {Chapman \& Hall/CRC Press},
	author = {Harrer, Mathias and Cuijpers, Pim and A, Furukawa Toshi and Ebert, David D},
	year = {2021},
}

@article{song_dissemination_2010-1,
	title = {Dissemination and publication of research findings : an updated review of related biases},
	volume = {14},
	issn = {ISSN: 2046-4924, ISSN: 1366-5278},
	shorttitle = {Dissemination and publication of research findings},
	url = {https://www.journalslibrary.nihr.ac.uk/hta/hta14080/},
	doi = {10.3310/hta14080},
	abstract = {Objectives To identify and appraise empirical studies on publication and related biases published since 1998; to assess methods to deal with publication and related biases; and to examine, in a random sample of published systematic reviews, measures taken to prevent, reduce and detect dissemination bias. Data sources The main literature search, in August 2008, covered the Cochrane Methodology Register Database, MEDLINE, EMBASE, AMED and CINAHL. In May 2009, PubMed, PsycINFO and OpenSIGLE were also searched. Reference lists of retrieved studies were also examined. Review methods In Part I, studies were classified as evidence or method studies and data were extracted according to types of dissemination bias or methods for dealing with it. Evidence from empirical studies was summarised narratively. In Part II, 300 systematic reviews were randomly selected from MEDLINE and the methods used to deal with publication and related biases were assessed. Results Studies with significant or positive results were more likely to be published than those with non-significant or negative results, thereby confirming findings from a previous HTA report. There was convincing evidence that outcome reporting bias exists and has an impact on the pooled summary in systematic reviews. Studies with significant results tended to be published earlier than studies with non-significant results, and empirical evidence suggests that published studies tended to report a greater treatment effect than those from the grey literature. Exclusion of non-English-language studies appeared to result in a high risk of bias in some areas of research such as complementary and alternative medicine. In a few cases, publication and related biases had a potentially detrimental impact on patients or resource use. Publication bias can be prevented before a literature review (e.g. by prospective registration of trials), or detected during a literature review (e.g. by locating unpublished studies, funnel plot and related tests, sensitivity analysis modelling), or its impact can be minimised after a literature review (e.g. by confirmatory large-scale trials, updating the systematic review). The interpretation of funnel plot and related statistical tests, often used to assess publication bias, was often too simplistic and likely misleading. More sophisticated modelling methods have not been widely used. Compared with systematic reviews published in 1996, recent reviews of health-care interventions were more likely to locate and include non-English-language studies and grey literature or unpublished studies, and to test for publication bias. Conclusions Dissemination of research findings is likely to be a biased process, although the actual impact of such bias depends on specific circumstances. The prospective registration of clinical trials and the endorsement of reporting guidelines may reduce research dissemination bias in clinical research. In systematic reviews, measures can be taken to minimise the impact of dissemination bias by systematically searching for and including relevant studies that are difficult to access. Statistical methods can be useful for sensitivity analyses. Further research is needed to develop methods for qualitatively assessing the risk of publication bias in systematic reviews, and to evaluate the effect of prospective registration of studies, open access policy and improved publication guidelines.},
	language = {EN},
	number = {8},
	urldate = {2023-12-18},
	journal = {Health Technology Assessment},
	author = {Song, F. and Parekh, S. and Hooper, L. and Loke, Y. K. and Ryder, J. and Sutton, A. J. and Hing, C. and Kwok, C. S. and Pang, C. and Harvey, I.},
	month = feb,
	year = {2010},
	keywords = {notion},
	pages = {1--220},
	file = {Full Text PDF:/Users/luca/Zotero/storage/7ZJ3NJ94/Song et al. - 2010 - Dissemination and publication of research findings.pdf:application/pdf},
}

@article{stanley_detecting_2021,
	title = {Detecting publication selection bias through excess statistical significance},
	volume = {12},
	copyright = {© 2021 John Wiley \& Sons Ltd.},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1512},
	doi = {10.1002/jrsm.1512},
	abstract = {We introduce and evaluate three tests for publication selection bias based on excess statistical significance (ESS). The proposed tests incorporate heterogeneity explicitly in the formulas for expected and ESS. We calculate the expected proportion of statistically significant findings in the absence of selective reporting or publication bias based on each study's SE and meta-analysis estimates of the mean and variance of the true-effect distribution. A simple proportion of statistical significance test (PSST) compares the expected to the observed proportion of statistically significant findings. Alternatively, we propose a direct test of excess statistical significance (TESS). We also combine these two tests of excess statistical significance (TESSPSST). Simulations show that these ESS tests often outperform the conventional Egger test for publication selection bias and the three-parameter selection model (3PSM).},
	language = {en},
	number = {6},
	urldate = {2023-12-21},
	journal = {Research Synthesis Methods},
	author = {Stanley, T. D. and Doucouliagos, Hristos and Ioannidis, John P. A. and Carter, Evan C.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1512},
	keywords = {meta-analysis, notion, statistical power, excess statistical significance, publication selection bias},
	pages = {776--795},
	file = {Snapshot:/Users/luca/Zotero/storage/CT55Z28V/jrsm.html:text/html;Stanley et al. - 2021 - Detecting publication selection bias through exces.pdf:/Users/luca/Zotero/storage/3E887JHZ/Stanley et al. - 2021 - Detecting publication selection bias through exces.pdf:application/pdf},
}

@article{carter_correcting_2019,
	title = {Correcting for {Bias} in {Psychology}: {A} {Comparison} of {Meta}-{Analytic} {Methods}},
	volume = {2},
	issn = {2515-2459},
	shorttitle = {Correcting for {Bias} in {Psychology}},
	url = {https://doi.org/10.1177/2515245919847196},
	doi = {10.1177/2515245919847196},
	abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/metaExplorer/.},
	language = {en},
	number = {2},
	urldate = {2023-11-29},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Carter, Evan C. and Schönbrodt, Felix D. and Gervais, Will M. and Hilgard, Joseph},
	month = jun,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	keywords = {notion},
	pages = {115--144},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/PHNKNNRT/Carter et al. - 2019 - Correcting for Bias in Psychology A Comparison of.pdf:application/pdf},
}

@article{mcshane_adjusting_2016,
	title = {Adjusting for {Publication} {Bias} in {Meta}-{Analysis}: {An} {Evaluation} of {Selection} {Methods} and {Some} {Cautionary} {Notes}},
	volume = {11},
	issn = {1745-6916},
	shorttitle = {Adjusting for {Publication} {Bias} in {Meta}-{Analysis}},
	url = {https://doi.org/10.1177/1745691616662243},
	doi = {10.1177/1745691616662243},
	abstract = {We review and evaluate selection methods, a prominent class of techniques first proposed by Hedges (1984) that assess and adjust for publication bias in meta-analysis, via an extensive simulation study. Our simulation covers both restrictive settings as well as more realistic settings and proceeds across multiple metrics that assess different aspects of model performance. This evaluation is timely in light of two recently proposed approaches, the so-called p-curve and p-uniform approaches, that can be viewed as alternative implementations of the original Hedges selection method approach. We find that the p-curve and p-uniform approaches perform reasonably well but not as well as the original Hedges approach in the restrictive setting for which all three were designed. We also find they perform poorly in more realistic settings, whereas variants of the Hedges approach perform well. We conclude by urging caution in the application of selection methods: Given the idealistic model assumptions underlying selection methods and the sensitivity of population average effect size estimates to them, we advocate that selection methods should be used less for obtaining a single estimate that purports to adjust for publication bias ex post and more for sensitivity analysis—that is, exploring the range of estimates that result from assuming different forms of and severity of publication bias.},
	language = {en},
	number = {5},
	urldate = {2023-12-28},
	journal = {Perspectives on Psychological Science},
	author = {McShane, Blakeley B. and Böckenholt, Ulf and Hansen, Karsten T.},
	month = sep,
	year = {2016},
	note = {Publisher: SAGE Publications Inc},
	pages = {730--749},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/67EMXIWK/McShane et al. - 2016 - Adjusting for Publication Bias in Meta-Analysis A.pdf:application/pdf},
}

@book{rothstein_publication_2005-1,
	title = {Publication {Bias} in {Meta}‐{Analysis}: {Prevention}, {Assessment} and {Adjustments}},
	url = {https://doi.org/10.1002/0470870168},
	publisher = {John Wiley \& Sons},
	author = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michael},
	year = {2005},
	file = {Rothstein et al. - 2005 - Publication Bias in Meta‐Analysis Prevention, Ass.pdf:/Users/luca/Zotero/storage/TBZQK5CW/Rothstein et al. - 2005 - Publication Bias in Meta‐Analysis Prevention, Ass.pdf:application/pdf},
}

@article{franco_publication_2014,
	title = {Publication bias in the social sciences: {Unlocking} the file drawer},
	volume = {345},
	shorttitle = {Publication bias in the social sciences},
	url = {https://www.science.org/doi/full/10.1126/science.1255484},
	doi = {10.1126/science.1255484},
	abstract = {We studied publication bias in the social sciences by analyzing a known population of conducted studies—221 in total—in which there is a full accounting of what is published and unpublished. We leveraged Time-sharing Experiments in the Social Sciences (TESS), a National Science Foundation–sponsored program in which researchers propose survey-based experiments to be run on representative samples of American adults. Because TESS proposals undergo rigorous peer review, the studies in the sample all exceed a substantial quality threshold. Strong results are 40 percentage points more likely to be published than are null results and 60 percentage points more likely to be written up. We provide direct evidence of publication bias and identify the stage of research production at which publication bias occurs: Authors do not write up and submit null findings.},
	number = {6203},
	urldate = {2024-01-02},
	journal = {Science},
	author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
	month = sep,
	year = {2014},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {notion},
	pages = {1502--1505},
	file = {Full Text PDF:/Users/luca/Zotero/storage/QUN44Q6N/Franco et al. - 2014 - Publication bias in the social sciences Unlocking.pdf:application/pdf},
}

@incollection{begg_publication_1994,
	address = {New York, NY, US},
	title = {Publication bias},
	isbn = {978-0-87154-226-7},
	abstract = {publication bias presents possibly the greatest methodologic threat to validity of a meta-analysis / it can be caused by the biased and selective reporting of the results of a given study, or, more seriously, by the selective decision to publish the results of the study in the first place / undetected publication bias is especially serious owing to the fact that the meta-analysis may not only lead to a spurious conclusion, but the aggregation of data may give the impression, with standard statistical methodology, that the conclusions are very precise  methods for identifying publication bias / methods for correcting publication bias (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	booktitle = {The handbook of research synthesis},
	publisher = {Russell Sage Foundation},
	author = {Begg, Colin B.},
	year = {1994},
	keywords = {Statistical Validity, Meta Analysis, Scientific Communication, Data Collection},
	pages = {399--409},
	file = {Begg - 1994 - Publication bias.pdf:/Users/luca/Zotero/storage/ANRARK4F/The Handbook of research synthesis -- Cooper, Harris M\; Hedges, Larry V -- 1994 -- New York_ Russell Sage Foundation -- 9780871542267 -- e5ec7912251b20962cc27abb619a2e25 -- Anna’s Archive.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/T8LJUQX6/1993-99100-024.html:text/html;The Handbook of research synthesis -- Cooper, Harris M\; Hedges, Larry V -- 1994 -- New York_ Russell Sage Foundation -- 9780871542267 -- e5ec7912251b20962cc27abb619a2e25 -- Anna’s Archive.pdf:/Users/luca/Zotero/storage/KVRTTN2R/The Handbook of research synthesis -- Cooper, Harris M\; Hedges, Larry V -- 1994 -- New York_ Russell Sage Foundation -- 9780871542267 -- e5ec7912251b20962cc27abb619a2e25 -- Anna’s Archive.pdf:application/pdf;The Handbook of research synthesis -- Cooper, Harris M\; Hedges, Larry V -- 1994 -- New York_ Russell Sage Foundation -- 9780871542267 -- e5ec7912251b20962cc27abb619a2e25 -- Anna’s Archive.pdf:/Users/luca/Zotero/storage/8HTFWE9Y/The Handbook of research synthesis -- Cooper, Harris M\; Hedges, Larry V -- 1994 -- New York_ Russell Sage Foundation -- 9780871542267 -- e5ec7912251b20962cc27abb619a2e25 -- Anna’s Archive.pdf:application/pdf},
}

@book{light_summing_1984,
	title = {Summing up: {The} science of reviewing research.},
	shorttitle = {Summing up},
	url = {https://scholars.unh.edu/psych_facpub/194},
	publisher = {Harvard University Press},
	author = {Light, Richard and Pillemer, David},
	month = oct,
	year = {1984},
	file = {"Summing up\: The science of reviewing research." by Richard J. Light and David B. Pillemer:/Users/luca/Zotero/storage/MZT78TNA/194.html:text/html;Light und Pillemer - 1984 - Summing up The science of reviewing research..pdf:/Users/luca/Zotero/storage/WMDSNYVP/Light und Pillemer - 1984 - Summing up The science of reviewing research..pdf:application/pdf},
}

@article{palmer_quasi-replication_2000,
	title = {Quasi-{Replication} and the {Contract} of {Error}: {Lessons} from {Sex} {Ratios}, {Heritabilities} and {Fluctuating} {Asymmetry}},
	volume = {31},
	shorttitle = {Quasi-{Replication} and the {Contract} of {Error}},
	url = {https://doi.org/10.1146/annurev.ecolsys.31.1.441},
	doi = {10.1146/annurev.ecolsys.31.1.441},
	abstract = {Selective reporting—e.g., the preferential publication of results that are statistically significant, or consistent with theory or expectation—presents a challenge to meta-analysis and seriously undermines the quest for generalizations. Funnel graphs (scatterplots of effect size vs. sample size) help reveal the extent of selective reporting. They also allow the strength of biological effects to be judged easily, and they reaffirm the value of graphical presentations of data over statistical summaries. Funnel graphs of published results, including: (a) sex-ratio variation in birds, (b) field estimates of heritabilities, and (c) relations between fluctuating asymmetry and individual attractiveness or fitness, suggest selective reporting is widespread and raise doubts about the true magnitude of these phenomena. Quasireplication—the “replication” of previous studies using different species or systems—has almost completely supplanted replicative research in ecology and evolution. Without incentives for formal replicative studies, which could come from changes to editorial policies, graduate training programs, and research funding priorities, the contract of error will continue to thwart attempts at robust generalizations. “For as knowledges are now delivered, there is a kind of contract of error between the deliverer and the receiver: for he that delivereth knowledge desireth to deliver it in such a form as may be best believed, and not as may be best examined; and he that receiveth knowledge desireth rather present satisfaction than expectant inquiry; and so rather not to doubt than not to err: glory making the author not to lay open his weakness, and sloth making the disciple not to know his strength.” The Advancement of Learning, Francis Bacon, 1605 (8:170–171)},
	number = {1},
	urldate = {2024-02-04},
	journal = {Annual Review of Ecology and Systematics},
	author = {Palmer, A. Richard},
	year = {2000},
	note = {\_eprint: https://doi.org/10.1146/annurev.ecolsys.31.1.441},
	keywords = {meta-analysis, publication bias, replication, selective reporting, research synthesis, funnel graph},
	pages = {441--480},
	file = {Palmer - 2000 - Quasi-Replication and the Contract of Error Lesso.pdf:/Users/luca/Zotero/storage/GWSDKNVI/Palmer - 2000 - Quasi-Replication and the Contract of Error Lesso.pdf:application/pdf},
}

@article{sassenberg_research_2019,
	title = {Research in {Social} {Psychology} {Changed} {Between} 2011 and 2016: {Larger} {Sample} {Sizes}, {More} {Self}-{Report} {Measures}, and {More} {Online} {Studies}},
	volume = {2},
	issn = {2515-2459},
	shorttitle = {Research in {Social} {Psychology} {Changed} {Between} 2011 and 2016},
	url = {https://doi.org/10.1177/2515245919838781},
	doi = {10.1177/2515245919838781},
	abstract = {The debate about false positives in psychological research has led to a demand for higher statistical power. To meet this demand, researchers need to collect data from larger samples—which is important to increase replicability, but can be costly in both time and money (i.e., remuneration of participants). Given that researchers might need to compensate for these higher costs, we hypothesized that larger sample sizes might have been accompanied by more frequent use of less costly research methods (i.e., online data collection and self-report measures). To test this idea, we analyzed social psychology studies published in 2009, 2011, 2016, and 2018. Indeed, research reported in 2016 and 2018 (vs. 2009 and 2011) had larger sample sizes and relied more on online data collection and self-report measures. Thus, over these years, research improved in its statistical power, but also changed with regard to the methods applied. Implications for social psychology as a discipline are discussed.},
	language = {en},
	number = {2},
	urldate = {2024-02-04},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Sassenberg, Kai and Ditrich, Lara},
	month = jun,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	pages = {107--114},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/8QMC7KGK/Sassenberg und Ditrich - 2019 - Research in Social Psychology Changed Between 2011.pdf:application/pdf},
}

@article{kullback_information_1951,
	title = {On {Information} and {Sufficiency}},
	volume = {22},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-1/On-Information-and-Sufficiency/10.1214/aoms/1177729694.full},
	doi = {10.1214/aoms/1177729694},
	abstract = {The Annals of Mathematical Statistics},
	number = {1},
	urldate = {2024-02-05},
	journal = {The Annals of Mathematical Statistics},
	author = {Kullback, S. and Leibler, R. A.},
	month = mar,
	year = {1951},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {79--86},
	file = {Kullback und Leibler - 1951 - On Information and Sufficiency.pdf:/Users/luca/Zotero/storage/T8BLE8SW/Kullback und Leibler - 1951 - On Information and Sufficiency.pdf:application/pdf},
}

@article{kullback_information_1951-1,
	title = {On {Information} and {Sufficiency}},
	volume = {22},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-1/On-Information-and-Sufficiency/10.1214/aoms/1177729694.full},
	doi = {10.1214/aoms/1177729694},
	abstract = {The Annals of Mathematical Statistics},
	number = {1},
	urldate = {2024-02-05},
	journal = {The Annals of Mathematical Statistics},
	author = {Kullback, S. and Leibler, R. A.},
	month = mar,
	year = {1951},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {79--86},
}

@article{schneck_examining_2017-1,
	title = {Examining publication bias—a simulation-based evaluation of statistical tests on publication bias},
	volume = {5},
	issn = {2167-8359},
	url = {https://peerj.com/articles/4115},
	doi = {10.7717/peerj.4115},
	abstract = {Background Publication bias is a form of scientific misconduct. It threatens the validity of research results and the credibility of science. Although several tests on publication bias exist, no in-depth evaluations are available that examine which test performs best for different research settings. Methods Four tests on publication bias, Egger’s test (FAT), p-uniform, the test of excess significance (TES), as well as the caliper test, were evaluated in a Monte Carlo simulation. Two different types of publication bias and its degree (0\%, 50\%, 100\%) were simulated. The type of publication bias was defined either as file-drawer, meaning the repeated analysis of new datasets, or p-hacking, meaning the inclusion of covariates in order to obtain a significant result. In addition, the underlying effect (β = 0, 0.5, 1, 1.5), effect heterogeneity, the number of observations in the simulated primary studies (N = 100, 500), and the number of observations for the publication bias tests (K = 100, 1,000) were varied. Results All tests evaluated were able to identify publication bias both in the file-drawer and p-hacking condition. The false positive rates were, with the exception of the 15\%- and 20\%-caliper test, unbiased. The FAT had the largest statistical power in the file-drawer conditions, whereas under p-hacking the TES was, except under effect heterogeneity, slightly better. The CTs were, however, inferior to the other tests under effect homogeneity and had a decent statistical power only in conditions with 1,000 primary studies. Discussion The FAT is recommended as a test for publication bias in standard meta-analyses with no or only small effect heterogeneity. If two-sided publication bias is suspected as well as under p-hacking the TES is the first alternative to the FAT. The 5\%-caliper test is recommended under conditions of effect heterogeneity and a large number of primary studies, which may be found if publication bias is examined in a discipline-wide setting when primary studies cover different research problems.},
	language = {en},
	urldate = {2024-02-06},
	journal = {PeerJ},
	author = {Schneck, Andreas},
	month = nov,
	year = {2017},
	note = {Publisher: PeerJ Inc.},
	pages = {e4115},
	file = {Full Text PDF:/Users/luca/Zotero/storage/CELSQQ3J/Schneck - 2017 - Examining publication bias—a simulation-based eval.pdf:application/pdf},
}

@article{schneck_are_2023,
	title = {Are most published research findings false? {Trends} in statistical power, publication selection bias, and the false discovery rate in psychology (1975–2017)},
	volume = {18},
	issn = {1932-6203},
	shorttitle = {Are most published research findings false?},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0292717},
	doi = {10.1371/journal.pone.0292717},
	abstract = {The validity of scientific findings may be challenged by the replicability crisis (or cases of fraud), which may result not only in a loss of trust within society but may also lead to wrong or even harmful policy or medical decisions. The question is: how reliable are scientific results that are reported as statistically significant, and how does this reliability develop over time? Based on 35,515 papers in psychology published between 1975 and 2017 containing 487,996 test values, this article empirically examines the statistical power, publication bias, and p-hacking, as well as the false discovery rate. Assuming constant true effects, the statistical power was found to be lower than the suggested 80\% except for large underlying true effects (d = 0.8) and increased only slightly over time. Also, publication bias and p-hacking were found to be substantial. The share of false discoveries among all significant results was estimated at 17.7\%, assuming a proportion θ = 50\% of all hypotheses being true and assuming that p-hacking is the only mechanism generating a higher proportion of just significant results compared to just nonsignificant results. As the analyses rely on multiple assumptions that cannot be tested, alternative scenarios were laid out, again resulting in the rather optimistic result that although research results may suffer from low statistical power and publication selection bias, most of the results reported as statistically significant may contain substantial results, rather than statistical artifacts.},
	language = {en},
	number = {10},
	urldate = {2024-02-06},
	journal = {PLOS ONE},
	author = {Schneck, Andreas},
	month = oct,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {Probability distribution, Psychology, Publication ethics, Test statistics, Statistical distributions, Research design, Social psychology, Research reporting guidelines},
	pages = {e0292717},
	file = {Full Text PDF:/Users/luca/Zotero/storage/UJWKQNIR/Schneck - 2023 - Are most published research findings false Trends.pdf:application/pdf},
}

@article{rodriguez_psymetadata_2022,
	title = {psymetadata: {An} {R} {Package} {Containing} {Open} {Datasets} from {Meta}-{Analyses} in {Psychology}},
	volume = {10},
	issn = {2050-9863},
	shorttitle = {psymetadata},
	url = {https://openpsychologydata.metajnl.com/articles/10.5334/jopd.61},
	doi = {10.5334/jopd.61},
	abstract = {We present 22 open-source datasets from meta-analyses in psychology. These data span areas such as social, developmental, and cognitive psychology, among others. These datasets are useful for two main purposes: (1) for demonstrative use in the teaching of meta-analysis techniques and (2) the illustration of novel statistical methods in journal articles. Additionally, they can be used to elicit informative priors for a Bayesian meta-analysis. All datasets are available through the R package psymetadata.},
	language = {en-US},
	number = {1},
	urldate = {2024-02-06},
	author = {Rodriguez, Josue E. and Williams, Donald R.},
	month = may,
	year = {2022},
	note = {Number: 1
Publisher: Ubiquity Press},
	pages = {8},
	file = {Full Text PDF:/Users/luca/Zotero/storage/YD2PNPJC/Rodriguez und Williams - 2022 - psymetadata An R Package Containing Open Datasets.pdf:application/pdf},
}

@article{funder_evaluating_2019,
	title = {Evaluating {Effect} {Size} in {Psychological} {Research}: {Sense} and {Nonsense}},
	volume = {2},
	issn = {2515-2459},
	shorttitle = {Evaluating {Effect} {Size} in {Psychological} {Research}},
	url = {https://doi.org/10.1177/2515245919847202},
	doi = {10.1177/2515245919847202},
	abstract = {Effect sizes are underappreciated and often misinterpreted—the most common mistakes being to describe them in ways that are uninformative (e.g., using arbitrary standards) or misleading (e.g., squaring effect-size rs). We propose that effect sizes can be usefully evaluated by comparing them with well-understood benchmarks or by considering them in terms of concrete consequences. In that light, we conclude that when reliably estimated (a critical consideration), an effect-size r of .05 indicates an effect that is very small for the explanation of single events but potentially consequential in the not-very-long run, an effect-size r of .10 indicates an effect that is still small at the level of single events but potentially more ultimately consequential, an effect-size r of .20 indicates a medium effect that is of some explanatory and practical use even in the short run and therefore even more important, and an effect-size r of .30 indicates a large effect that is potentially powerful in both the short and the long run. A very large effect size (r = .40 or greater) in the context of psychological research is likely to be a gross overestimate that will rarely be found in a large sample or in a replication. Our goal is to help advance the treatment of effect sizes so that rather than being numbers that are ignored, reported without interpretation, or interpreted superficially or incorrectly, they become aspects of research reports that can better inform the application and theoretical development of psychological research.},
	language = {en},
	number = {2},
	urldate = {2024-02-08},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Funder, David C. and Ozer, Daniel J.},
	month = jun,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	pages = {156--168},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/3CPZVDTQ/Funder und Ozer - 2019 - Evaluating Effect Size in Psychological Research .pdf:application/pdf},
}

@article{lakens_equivalence_2018-1,
	title = {Equivalence {Testing} for {Psychological} {Research}: {A} {Tutorial}},
	volume = {1},
	issn = {2515-2459},
	shorttitle = {Equivalence {Testing} for {Psychological} {Research}},
	url = {https://doi.org/10.1177/2515245918770963},
	doi = {10.1177/2515245918770963},
	abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
	language = {en},
	number = {2},
	urldate = {2024-02-08},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Lakens, Daniël and Scheel, Anne M. and Isager, Peder M.},
	month = jun,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	pages = {259--269},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/FV83PBCE/Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf:application/pdf},
}

@article{lakens_performing_2014,
	title = {Performing high-powered studies efficiently with sequential analyses},
	volume = {44},
	copyright = {Copyright © 2014 John Wiley \& Sons, Ltd.},
	issn = {1099-0992},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2023},
	doi = {10.1002/ejsp.2023},
	abstract = {Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre-registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null-hypothesis significance testing (NHST) are discussed. Sequential analyses, which are widely used in large-scale medical trials, provide an efficient way to perform high-powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research. Copyright © 2014 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {7},
	urldate = {2024-02-10},
	journal = {European Journal of Social Psychology},
	author = {Lakens, Daniël},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ejsp.2023},
	pages = {701--710},
	file = {Snapshot:/Users/luca/Zotero/storage/YBUY3TWT/ejsp.html:text/html;Volltext:/Users/luca/Zotero/storage/KVCKPBYJ/Lakens - 2014 - Performing high-powered studies efficiently with s.pdf:application/pdf},
}

@article{friese_p-hacking_2020,
	title = {p-{Hacking} and publication bias interact to distort meta-analytic effect size estimates},
	volume = {25},
	issn = {1939-1463},
	doi = {10.1037/met0000246},
	abstract = {Science depends on trustworthy evidence. Thus, a biased scientific record is of questionable value because it impedes scientific progress, and the public receives advice on the basis of unreliable evidence that has the potential to have far-reaching detrimental consequences. Meta-analysis is a technique that can be used to summarize research evidence. However, meta-analytic effect size estimates may themselves be biased, threatening the validity and usefulness of meta-analyses to promote scientific progress. Here, we offer a large-scale simulation study to elucidate how p-hacking and publication bias distort meta-analytic effect size estimates under a broad array of circumstances that reflect the reality that exists across a variety of research areas. The results revealed that, first, very high levels of publication bias can severely distort the cumulative evidence. Second, p-hacking and publication bias interact: At relatively high and low levels of publication bias, p-hacking does comparatively little harm, but at medium levels of publication bias, p-hacking can considerably contribute to bias, especially when the true effects are very small or are approaching zero. Third, p-hacking can severely increase the rate of false positives. A key implication is that, in addition to preventing p-hacking, policies in research institutions, funding agencies, and scientific journals need to make the prevention of publication bias a top priority to ensure a trustworthy base of evidence. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	number = {4},
	journal = {Psychological Methods},
	author = {Friese, Malte and Frankenbach, Julius},
	year = {2020},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Simulation, Publication Bias, Meta Analysis, Scientific Communication, Estimation, Experimenter Bias, Size},
	pages = {456--471},
	file = {Snapshot:/Users/luca/Zotero/storage/9BYAYHIA/2019-71476-001.html:text/html;Volltext:/Users/luca/Zotero/storage/MHPUN8WD/Friese und Frankenbach - 2020 - p-Hacking and publication bias interact to distort.pdf:application/pdf},
}

@misc{noauthor_false-positive_nodate,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant} - {Joseph} {P}. {Simmons}, {Leif} {D}. {Nelson}, {Uri} {Simonsohn}, 2011},
	url = {https://journals.sagepub.com/doi/10.1177/0956797611417632},
	urldate = {2024-02-10},
	file = {False-Positive Psychology\: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant - Joseph P. Simmons, Leif D. Nelson, Uri Simonsohn, 2011:/Users/luca/Zotero/storage/YDILWJAZ/0956797611417632.html:text/html},
}

@article{simmons_false-positive_2011,
	title = {False-{Positive} {Psychology}: {Undisclosed} {Flexibility} in {Data} {Collection} and {Analysis} {Allows} {Presenting} {Anything} as {Significant}},
	volume = {22},
	issn = {0956-7976},
	shorttitle = {False-{Positive} {Psychology}},
	url = {https://doi.org/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	language = {en},
	number = {11},
	urldate = {2024-02-10},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	note = {Publisher: SAGE Publications Inc},
	pages = {1359--1366},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/LVPQI2KL/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf:application/pdf},
}

@article{johnson_reproducibility_2017,
	title = {On the {Reproducibility} of {Psychological} {Science}},
	volume = {112},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1240079},
	doi = {10.1080/01621459.2016.1240079},
	abstract = {Investigators from a large consortium of scientists recently performed a multi-year study in which they replicated 100 psychology experiments. Although statistically significant results were reported in 97\% of the original studies, statistical significance was achieved in only 36\% of the replicated studies. This article presents a reanalysis of these data based on a formal statistical model that accounts for publication bias by treating outcomes from unpublished studies as missing data, while simultaneously estimating the distribution of effect sizes for those studies that tested nonnull effects. The resulting model suggests that more than 90\% of tests performed in eligible psychology experiments tested negligible effects, and that publication biases based on p-values caused the observed rates of nonreproducibility. The results of this reanalysis provide a compelling argument for both increasing the threshold required for declaring scientific discoveries and for adopting statistical summaries of evidence that account for the high proportion of tested hypotheses that are false. Supplementary materials for this article are available online.},
	number = {517},
	urldate = {2024-02-10},
	journal = {Journal of the American Statistical Association},
	author = {Johnson, Valen E. and Payne, Richard D. and Wang, Tianying and Asher, Alex and Mandal, Soutrik},
	month = jan,
	year = {2017},
	pmid = {29861517},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2016.1240079},
	keywords = {Reproducibility, Publication bias, Bayes factor, Null hypothesis significance test, Posterior model probability, Significance test},
	pages = {1--10},
	file = {Full Text PDF:/Users/luca/Zotero/storage/5S8QELN6/Johnson et al. - 2017 - On the Reproducibility of Psychological Science.pdf:application/pdf},
}

@article{mcnemar_at_1960,
	title = {At random: {Sense} and nonsense},
	volume = {15},
	issn = {1935-990X},
	shorttitle = {At random},
	doi = {10.1037/h0049193},
	abstract = {"It is often said that psychologists are captivated by the magic of words." One example of this is the epidemic use by psychologists of the word 'design.' " "Despite glib talk about using factor analysis to test hypotheses, practically no users of factor analysis ever test hypotheses." A psychologist may "trudge off weighted down with a box full of statistical tools in search of a research problem that permits him to display skill with his tools." Most psychologists "without intentional eavesdropping know that occasionally their theory oriented colleagues simply discard all data of an experiment as bad data if not in agreement with the theory, and start over. The theory is, of course, always good." The theorist who has the one and only approach and solution to all, or nearly all, psychological problems "tends to lure those among us who have a low tolerance for the ambiguity that flourishes in psychology." This "model business is nothing more than a new name for old hat stuff… . The question of the usefulness of mathematical models and, to a certain extent, the value of high powered statistical techniques is debatable." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {5},
	journal = {American Psychologist},
	author = {McNemar, Quinn},
	year = {1960},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Factor Analysis, Statistical Analysis, Psychologists},
	pages = {295--300},
	file = {McNemar - 1960 - At random Sense and nonsense.pdf:/Users/luca/Zotero/storage/UG24GGPG/McNemar - 1960 - At random Sense and nonsense.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/LVJZK2DB/1961-02880-001.html:text/html},
}

@article{sterling_publication_1959,
	title = {Publication {Decisions} and their {Possible} {Effects} on {Inferences} {Drawn} from {Tests} of {Significance}—or {Vice} {Versa}},
	volume = {54},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.1959.10501497},
	doi = {10.1080/01621459.1959.10501497},
	abstract = {There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs—an “error of the first kind”—and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance. * The author wishes to express his thanks to Sir Ronald Fisher whose discussion on related topics stimulated this research in the first place, and to Leo Katz, Oliver Lacey, Enders Robinson, and Paul Siegel for reading and criticizing earlier drafts of this manuscript.},
	number = {285},
	urldate = {2024-02-11},
	journal = {Journal of the American Statistical Association},
	author = {Sterling, Theodore D.},
	month = mar,
	year = {1959},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.1959.10501497},
	pages = {30--34},
	file = {Sterling - 1959 - Publication Decisions and their Possible Effects o.pdf:/Users/luca/Zotero/storage/KAYVC7PF/Sterling - 1959 - Publication Decisions and their Possible Effects o.pdf:application/pdf},
}

@article{smart_importance_1964,
	title = {The importance of negative results in psychological research},
	volume = {5a},
	issn = {0008-4832},
	doi = {10.1037/h0083036},
	abstract = {The purposes of this study were to determine the proportion of papers which contain negative results (results which fail to reject the null hypothesis), and whether there is some selection in the papers published such that negative results are unlikely to be published. An examination of current psychological journals indicated that studies with negative results constitute about 9 per cent of the total volume of published papers. However, data from several unpublished sources indicate that negative results are less likely to be published. The reasons for their neglect - chiefly author selection and the greater editorial scrutiny they get - were presented. The practical, statistical and heuristic value of negative results was also discussed. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {4},
	journal = {Canadian Psychologist / Psychologie canadienne},
	author = {Smart, Reginald G.},
	year = {1964},
	note = {Place: Canada
Publisher: Canadian Psychological Association},
	keywords = {Psychology, Scientific Communication},
	pages = {225--232},
	file = {Smart - 1964 - The importance of negative results in psychologica.pdf:/Users/luca/Zotero/storage/UR9FKN4W/Smart - 1964 - The importance of negative results in psychologica.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/NTEU2N9H/2007-01461-003.html:text/html},
}

@article{scheel_excess_2021,
	title = {An {Excess} of {Positive} {Results}: {Comparing} the {Standard} {Psychology} {Literature} {With} {Registered} {Reports}},
	volume = {4},
	issn = {2515-2459},
	shorttitle = {An {Excess} of {Positive} {Results}},
	url = {https://doi.org/10.1177/25152459211007467},
	doi = {10.1177/25152459211007467},
	abstract = {Selectively publishing results that support the tested hypotheses (“positive” results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
	language = {en},
	number = {2},
	urldate = {2024-02-11},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Daniël},
	month = apr,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	pages = {25152459211007467},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/TXBQNWUL/Scheel et al. - 2021 - An Excess of Positive Results Comparing the Stand.pdf:application/pdf},
}

@book{bakan_method_1967,
	edition = {Marginilia Edition},
	title = {On method: {Toward} a reconstruction of psychological investigation.},
	language = {Englisch},
	publisher = {Jossey Bass},
	author = {Bakan, David},
	month = jan,
	year = {1967},
	file = {Bakan - 1967 - On method Toward a reconstruction of psychologica.pdf:/Users/luca/Zotero/storage/F5VY2SJN/Bakan - 1967 - On method Toward a reconstruction of psychologica.pdf:application/pdf},
}

@article{sterling_publication_1995,
	title = {Publication {Decisions} {Revisited}: {The} {Effect} of the {Outcome} of {Statistical} {Tests} on the {Decision} to {Publish} and {Vice} {Versa}},
	volume = {49},
	issn = {0003-1305},
	shorttitle = {Publication {Decisions} {Revisited}},
	url = {https://www.jstor.org/stable/2684823},
	doi = {10.2307/2684823},
	abstract = {This article presents evidence that published results of scientific investigations are not a representative sample of results of all scientific studies. Research studies from 11 major journals demonstrate the existence of biases that favor studies that observe effects that, on statistical evaluation, have a low probability of erroneously rejecting the so-called null hypothesis (H0). This practice makes the probability of erroneously rejecting H0 different for the reader than for the investigator. It introduces two biases in the interpretation of the scientific literature: one due to multiple repetition of studies with false hypothesis, and one due to failure to publish smaller and less significant outcomes of tests of a true hypotheses. These practices distort the results of literature surveys and of meta-analyses. These results also indicate that practice leading to publication bias have not changed over a period of 30 years},
	number = {1},
	urldate = {2024-02-11},
	journal = {The American Statistician},
	author = {Sterling, T. D. and Rosenbaum, W. L. and Weinkam, J. J.},
	year = {1995},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {108--112},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/TKIWWNL3/Sterling et al. - 1995 - Publication Decisions Revisited The Effect of the.pdf:application/pdf},
}

@article{bozarth_signifying_1972,
	title = {Signifying significant significance},
	volume = {27},
	issn = {1935-990X},
	doi = {10.1037/h0038034},
	abstract = {Examined publication practices in counseling psychology by reviewing 3 counseling psychology journals from 1967 to 1970 to determine the frequencies of articles reporting rejection at the traditional alpha levels. Of 1,046 articles examined, 86\% used tests of statistical significance. Findings suggest that "traditional" significant results were related to publication. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {8},
	journal = {American Psychologist},
	author = {Bozarth, Jerold D. and Roberts, Ralph R.},
	year = {1972},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Scientific Communication, Statistical Significance, Counseling Psychology},
	pages = {774--775},
	file = {Bozarth und Roberts - 1972 - Signifying significant significance.pdf:/Users/luca/Zotero/storage/LVQWFZTL/Bozarth und Roberts - 1972 - Signifying significant significance.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/XVQVX24Z/1990-56998-001.html:text/html},
}

@article{coursol_effect_1986,
	title = {Effect of positive findings on submission and acceptance rates: {A} note on meta-analysis bias},
	volume = {17},
	issn = {1939-1323},
	shorttitle = {Effect of positive findings on submission and acceptance rates},
	doi = {10.1037/0735-7028.17.2.136},
	abstract = {Argues that the effect size reported by M. L. Smith and G. V. Glass (see record 1978-10341-001) in their meta-analysis of therapeutic outcome may have been inflated by the selective bias for positive articles appearing in the research literature. Data from 609 members of the Clinical and Psychotherapy Divisions of the American Psychological Association indicate that there may be a significant relationship between research study outcome and the decision to submit a paper for publication. (13 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Professional Psychology: Research and Practice},
	author = {Coursol, Allan and Wagner, Edwin E.},
	year = {1986},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Meta Analysis, Scientific Communication, Biased Sampling, Psychotherapeutic Outcomes},
	pages = {136--137},
	file = {Coursol und Wagner - 1986 - Effect of positive findings on submission and acce.pdf:/Users/luca/Zotero/storage/B3QBNVUX/Coursol und Wagner - 1986 - Effect of positive findings on submission and acce.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/6RD5378U/1986-20958-001.html:text/html},
}

@article{mahoney_publication_1977,
	title = {Publication prejudices: {An} experimental study of confirmatory bias in the peer review system},
	volume = {1},
	issn = {1573-2819},
	shorttitle = {Publication prejudices},
	url = {https://doi.org/10.1007/BF01173636},
	doi = {10.1007/BF01173636},
	abstract = {Confirmatory bias is the tendency to emphasize and believe experiences which support one's views and to ignore or discredit those which do not. The effects of this tendency have been repeatedly documented in clinical research. However, its ramifications for the behavior of scientists have yet to be adequately explored. For example, although publication is a critical element in determining the contribution and impact of scientific findings, little research attention has been devoted to the variables operative in journal review policies. In the present study, 75 journal reviewers were asked to referee manuscripts which described identical experimental procedures but which reported positive, negative, mixed, or no results. In addition to showing poor interrater agreement, reviewers were strongly biased against manuscripts which reported results contrary to their theoretical perspective. The implications of these findings for epistemology and the peer review system are briefly addressed.},
	language = {en},
	number = {2},
	urldate = {2024-02-11},
	journal = {Cognitive Therapy and Research},
	author = {Mahoney, Michael J.},
	month = jun,
	year = {1977},
	keywords = {Clinical Research, Cognitive Psychology, Experimental Study, Review System, Theoretical Perspective},
	pages = {161--175},
	file = {Full Text PDF:/Users/luca/Zotero/storage/CQR6H8BI/Mahoney - 1977 - Publication prejudices An experimental study of c.pdf:application/pdf},
}

@article{ioannidis_why_2005-1,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2024-02-11},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	note = {Publisher: Public Library of Science},
	keywords = {Finance, Metaanalysis, Cancer risk factors, Genetic epidemiology, Genetics of disease, Randomized controlled trials, Research design, Schizophrenia},
	pages = {e124},
	file = {Full Text PDF:/Users/luca/Zotero/storage/QHHMKHP3/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf},
}

@article{bakan_test_1966,
	title = {The test of significance in psychological research},
	volume = {66},
	issn = {0033-2909},
	doi = {10.1037/h0020412},
	language = {eng},
	number = {6},
	journal = {Psychological Bulletin},
	author = {Bakan, D.},
	month = dec,
	year = {1966},
	pmid = {5974619},
	keywords = {Humans, Psychometrics, Psychological Tests},
	pages = {423--437},
}

@article{begg_publication_1989,
	title = {Publication bias and dissemination of clinical research},
	volume = {81},
	issn = {0027-8874},
	doi = {10.1093/jnci/81.2.107},
	abstract = {Publication bias is a widely recognized phenomenon that occurs because of the influence of study results on the chances of publication. Usually, studies with positive results are more likely to be published than studies with negative results, which leads to a preponderance of false-positive results in the literature. Empiric studies have demonstrated that the induced bias is large and can have a serious impact on meta-analyses, in which data from several studies are aggregated, as well as on informal reviews. The problem is deeply embedded in current research practice, which encourages demonstration of statistical significance to "prove" theories, and one of its causes is the pressure to publish extensively that is an integral part of the competition for academic promotion. Serious efforts to reduce this problem will involve restructuring the process by which study results are disseminated, changing editorial policies, and altering the style and methods of statistical analysis.},
	language = {eng},
	number = {2},
	journal = {Journal of the National Cancer Institute},
	author = {Begg, C. B. and Berlin, J. A.},
	month = jan,
	year = {1989},
	pmid = {2642556},
	keywords = {Humans, Publishing, Meta-Analysis as Topic, Research Design, Attitude of Health Personnel, Clinical Trials as Topic, Neoplasms},
	pages = {107--115},
	file = {Begg und Berlin - 1989 - Publication bias and dissemination of clinical res.pdf:/Users/luca/Zotero/storage/ZJ8SLN7T/Begg und Berlin - 1989 - Publication bias and dissemination of clinical res.pdf:application/pdf},
}

@article{dickersin_publication_1993,
	title = {Publication {Bias}: {The} {Problem} {That} {Won}'t {Go} {Away}},
	volume = {703},
	issn = {1749-6632},
	shorttitle = {Publication {Bias}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.1993.tb26343.x},
	doi = {10.1111/j.1749-6632.1993.tb26343.x},
	language = {en},
	number = {1},
	urldate = {2024-02-11},
	journal = {Annals of the New York Academy of Sciences},
	author = {Dickersin, Kay and Min, Yuan-I},
	year = {1993},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-6632.1993.tb26343.x},
	pages = {135--148},
	file = {Dickersin und Min - 1993 - Publication Bias The Problem That Won't Go Away.pdf:/Users/luca/Zotero/storage/YVLCV6ZZ/Dickersin und Min - 1993 - Publication Bias The Problem That Won't Go Away.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/7JB6X5X8/j.1749-6632.1993.tb26343.html:text/html},
}

@article{nosek_scientific_2012,
	title = {Scientific {Utopia}: {II}. {Restructuring} {Incentives} and {Practices} to {Promote} {Truth} {Over} {Publishability}},
	volume = {7},
	issn = {1745-6916},
	shorttitle = {Scientific {Utopia}},
	url = {https://doi.org/10.1177/1745691612459058},
	doi = {10.1177/1745691612459058},
	abstract = {An academic scientist’s professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but abstract accuracy motive—getting it right—competitive with the more tangible and concrete incentive—getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases.},
	language = {en},
	number = {6},
	urldate = {2024-02-11},
	journal = {Perspectives on Psychological Science},
	author = {Nosek, Brian A. and Spies, Jeffrey R. and Motyl, Matt},
	month = nov,
	year = {2012},
	note = {Publisher: SAGE Publications Inc},
	pages = {615--631},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/CEBH7A6R/Nosek et al. - 2012 - Scientific Utopia II. Restructuring Incentives an.pdf:application/pdf},
}

@article{garcia-perez_thou_2017,
	title = {Thou {Shalt} {Not} {Bear} {False} {Witness} {Against} {Null} {Hypothesis} {Significance} {Testing}},
	volume = {77},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/0013164416668232},
	doi = {10.1177/0013164416668232},
	abstract = {Null hypothesis significance testing (NHST) has been the subject of debate for decades and alternative approaches to data analysis have been proposed. This article addresses this debate from the perspective of scientific inquiry and inference. Inference is an inverse problem and application of statistical methods cannot reveal whether effects exist or whether they are empirically meaningful. Hence, raising conclusions from the outcomes of statistical analyses is subject to limitations. NHST has been criticized for its misuse and the misconstruction of its outcomes, also stressing its inability to meet expectations that it was never designed to fulfil. Ironically, alternatives to NHST are identical in these respects, something that has been overlooked in their presentation. Three of those alternatives are discussed here (estimation via confidence intervals and effect sizes, quantification of evidence via Bayes factors, and mere reporting of descriptive statistics). None of them offers a solution to the problems that NHST is purported to have, all of them are susceptible to misuse and misinterpretation, and some bring around their own problems (e.g., Bayes factors have a one-to-one correspondence with p values, but they are entirely deprived of an inferential framework). Those alternatives also fail to cover a broad area of inference not involving distributional parameters, where NHST procedures remain the only (and suitable) option. Like knives or axes, NHST is not inherently evil; only misuse and misinterpretation of its outcomes needs to be eradicated.},
	language = {en},
	number = {4},
	urldate = {2024-02-12},
	journal = {Educational and Psychological Measurement},
	author = {García-Pérez, Miguel A.},
	month = aug,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	pages = {631--662},
	file = {Volltext:/Users/luca/Zotero/storage/JQZRN5NG/García-Pérez - 2017 - Thou Shalt Not Bear False Witness Against Null Hyp.pdf:application/pdf},
}

@article{cohen_earth_1994,
	title = {The earth is round (p {\textless} .05)},
	volume = {49},
	issn = {1935-990X},
	doi = {10.1037/0003-066X.49.12.997},
	abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {12},
	journal = {American Psychologist},
	author = {Cohen, Jacob},
	year = {1994},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Null Hypothesis Testing},
	pages = {997--1003},
	file = {Cohen - 1994 - The earth is round (p  .05).pdf:/Users/luca/Zotero/storage/X7ITI2PG/Cohen - 1994 - The earth is round (p  .05).pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/AZZ8XQXM/1995-12080-001.html:text/html},
}

@article{perezgonzalez_fisher_2015,
	title = {Fisher, {Neyman}-{Pearson} or {NHST}? {A} tutorial for teaching data testing},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {Fisher, {Neyman}-{Pearson} or {NHST}?},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2015.00223},
	abstract = {Despite frequent calls for the overhaul of null hypothesis significance testing (NHST), this controversial procedure remains ubiquitous in behavioral, social and biomedical teaching and research. Little change seems possible once the procedure becomes well ingrained in the minds and current practice of researchers; thus, the optimal opportunity for such change is at the time the procedure is taught, be this at undergraduate or at postgraduate levels. This paper presents a tutorial for the teaching of data testing procedures, often referred to as hypothesis testing theories. The first procedure introduced is Fisher's approach to data testing—tests of significance; the second is Neyman-Pearson's approach—tests of acceptance; the final procedure is the incongruent combination of the previous two theories into the current approach—NSHT. For those researchers sticking with the latter, two compromise solutions on how to improve NHST conclude the tutorial.},
	urldate = {2024-02-12},
	journal = {Frontiers in Psychology},
	author = {Perezgonzalez, Jose D.},
	year = {2015},
	file = {Full Text PDF:/Users/luca/Zotero/storage/XJBTGZ9S/Perezgonzalez - 2015 - Fisher, Neyman-Pearson or NHST A tutorial for tea.pdf:application/pdf},
}

@article{peikert_reproducible_2021,
	title = {A {Reproducible} {Data} {Analysis} {Workflow} {With} {R} {Markdown}, {Git}, {Make}, and {Docker}},
	volume = {1},
	copyright = {Copyright (c) 2021 Aaron Peikert, Andreas M. Brandmaier},
	issn = {2699-8432},
	url = {https://qcmb.psychopen.eu/index.php/qcmb/article/view/3763},
	doi = {10.5964/qcmb.3763},
	abstract = {In this tutorial, we describe a workflow to ensure long-term reproducibility of R-based data analyses. The workflow leverages established tools and practices from software engineering. It combines the benefits of various open-source software tools including R Markdown, Git, Make, and Docker, whose interplay ensures seamless integration of version management, dynamic report generation conforming to various journal styles, and full cross-platform and long-term computational reproducibility. The workflow ensures meeting the primary goals that 1) the reporting of statistical results is consistent with the actual statistical results (dynamic report generation), 2) the analysis exactly reproduces at a later point in time even if the computing platform or software is changed (computational reproducibility), and 3) changes at any time (during development and post-publication) are tracked, tagged, and documented while earlier versions of both data and code remain accessible. While the research community increasingly recognizes dynamic document generation and version management as tools to ensure reproducibility, we demonstrate with practical examples that these alone are not sufficient to ensure long-term computational reproducibility. Combining containerization, dependence management, version management, and dynamic document generation, the proposed workflow increases scientific productivity by facilitating later reproducibility and reuse of code and data.},
	language = {en},
	number = {1},
	urldate = {2024-02-14},
	journal = {Quantitative and Computational Methods in Behavioral Sciences},
	author = {Peikert, Aaron and Brandmaier, Andreas M.},
	month = may,
	year = {2021},
	keywords = {R, open science, reproducibility, containerization, dependency management, dynamic document generation, version management},
	pages = {1--27},
	file = {Full Text PDF:/Users/luca/Zotero/storage/NVANYADH/Peikert und Brandmaier - 2021 - A Reproducible Data Analysis Workflow With R Markd.pdf:application/pdf},
}

@article{hardwicke_data_2018,
	title = {Data availability, reusability, and analytic reproducibility: evaluating the impact of a mandatory open data policy at the journal {Cognition}},
	volume = {5},
	shorttitle = {Data availability, reusability, and analytic reproducibility},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.180448},
	doi = {10.1098/rsos.180448},
	abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data (‘analytic reproducibility’). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
	number = {8},
	urldate = {2024-02-14},
	journal = {Royal Society Open Science},
	author = {Hardwicke, Tom E. and Mathur, Maya B. and MacDonald, Kyle and Nilsonne, Gustav and Banks, George C. and Kidwell, Mallory C. and Hofelich Mohr, Alicia and Clayton, Elizabeth and Yoon, Erica J. and Henry Tessler, Michael and Lenne, Richie L. and Altman, Sara and Long, Bria and Frank, Michael C.},
	month = aug,
	year = {2018},
	note = {Publisher: Royal Society},
	keywords = {open science, reproducibility, journal policy, interrupted time series, meta-science, open data},
	pages = {180448},
	file = {Full Text PDF:/Users/luca/Zotero/storage/NIEJJEB4/Hardwicke et al. - 2018 - Data availability, reusability, and analytic repro.pdf:application/pdf},
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	copyright = {2017 Macmillan Publishers Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
	language = {en},
	number = {1},
	urldate = {2024-01-01},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	month = jan,
	year = {2017},
	keywords = {notion, Social sciences},
	pages = {1--10},
	file = {Full Text PDF:/Users/luca/Zotero/storage/3H25BTIW/Munafò et al. - 2017 - A manifesto for reproducible science.pdf:application/pdf},
}

@book{cooper_handbook_1994,
	title = {Handbook of {Research} {Synthesis}, {The}},
	isbn = {978-0-87154-226-7},
	url = {https://www.jstor.org/stable/10.7758/9781610441377},
	abstract = {The Handbook of Research Synthesis is the definitive reference and how-to manual for behavioral and medical scientists applying the craft of research synthesis. It draws upon twenty years of ground-breaking advances that have transformed the practice of synthesizing research literature from an art into a scientific process in its own right. Editors Harris Cooper and Larry V. Hedges have brought together leading authorities to guide the reader through every stage of the research synthesis process—problem formulation, literature search and evaluation, statistical integration, and report preparation. The Handbook of Research Synthesis incorporates in a single volume state-of-the-art techniques from all quantitative synthesis traditions, including Bayesian inference and the meta-analytic approaches. Distilling a vast technical literature and many informal sources, the Handbook provides a portfolio of the most effective solutions to problems of quantitative data integration. The Handbook of Research Synthesis also provides a rich treatment of the non-statistical aspects of research synthesis. Topics include searching the literature, managing reference databases and registries, and developing coding schemes. Those engaged in research synthesis will also find useful advice on how tables, graphs, and narration can be deployed to provide the most meaningful communication of the results of research synthesis. The Handbook of Research Synthesis is an illuminating compilation of practical instruction, theory, and problem solving. It provides an accumulation of knowledge about the craft of reviewing a scientific literature that can be found in no other single source. The Handbook offers the reader thorough instruction in the skills necessary to conduct powerful research syntheses meeting the highest standards of objectivity, systematicity, and rigor demanded of scientific enquiry. This definitive work will represent the state of the art in research synthesis for years to come.},
	urldate = {2024-02-16},
	publisher = {Russell Sage Foundation},
	editor = {Cooper, Harris and Hedges, Larry V.},
	year = {1994},
}

@incollection{vevea_publication_2019,
	edition = {3},
	title = {Publication {Bias}},
	booktitle = {The handbook of research synthesis and meta-analysis},
	publisher = {Russell Sage Foundation},
	author = {Vevea, Jack L. and Coburn, Kathleen and Sutton, Alexander J.},
	editor = {Cooper, Harris and Hedges, Larry V. and Valentine, Jeffrey C.},
	year = {2019},
	pages = {383--433},
	file = {Vevea et al. - 2019 - Publication Bias.pdf:/Users/luca/Zotero/storage/V67TUXGI/Vevea et al. - 2019 - Publication Bias.pdf:application/pdf},
}

@article{ferguson_publication_2012-1,
	title = {Publication bias in psychological science: prevalence, methods for identifying and controlling, and implications for the use of meta-analyses},
	volume = {17},
	issn = {1939-1463},
	shorttitle = {Publication bias in psychological science},
	doi = {10.1037/a0024445},
	abstract = {The issue of publication bias in psychological science is one that has remained difficult to address despite decades of discussion and debate. The current article examines a sample of 91 recent meta-analyses published in American Psychological Association and Association for Psychological Science journals and the methods used in these analyses to identify and control for publication bias. Of the 91 studies analyzed, 64 (70\%) made some effort to analyze publication bias, and 26 (41\%) reported finding evidence of bias. Approaches to controlling publication bias were heterogeneous among studies. Of these studies, 57 (63\%) attempted to find unpublished studies to control for publication bias. Nonetheless, those studies that included unpublished studies were just as likely to find evidence for publication bias as those that did not. Furthermore, authors of meta-analyses themselves were overrepresented in unpublished studies acquired, as compared with published studies, suggesting that searches for unpublished studies may increase rather than decrease some sources of bias. A subset of 48 meta-analyses for which study sample sizes and effect sizes were available was further analyzed with a conservative and newly developed tandem procedure of assessing publication bias. Results indicated that publication bias was worrisome in about 25\% of meta-analyses. Meta-analyses that included unpublished studies were more likely to show bias than those that did not, likely due to selection bias in unpublished literature searches. Sources of publication bias and implications for the use of meta-analysis are discussed.},
	language = {eng},
	number = {1},
	journal = {Psychological Methods},
	author = {Ferguson, Christopher J. and Brannick, Michael T.},
	month = mar,
	year = {2012},
	pmid = {21787082},
	keywords = {Humans, Psychology, Publication Bias, Data Interpretation, Statistical, Meta-Analysis as Topic, Prevalence, Quality Control, Selection Bias},
	pages = {120--128},
	file = {Ferguson und Brannick - 2012 - Publication bias in psychological science prevale.pdf:/Users/luca/Zotero/storage/2SCW3PG3/Ferguson und Brannick - 2012 - Publication bias in psychological science prevale.pdf:application/pdf},
}

@book{noauthor_introduction_2009,
	edition = {1},
	title = {Introduction to {Meta}-{Analysis}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9780470743386},
	urldate = {2024-02-16},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2009},
	doi = {10.1002/9780470743386},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470743386},
	file = {Snapshot:/Users/luca/Zotero/storage/LLXJAF5U/9780470743386.html:text/html},
}

@article{cohen_earth_1994-1,
	title = {The earth is round (p {\textless} .05)},
	volume = {49},
	issn = {1935-990X},
	doi = {10.1037/0003-066X.49.12.997},
	abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {12},
	journal = {American Psychologist},
	author = {Cohen, Jacob},
	year = {1994},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Null Hypothesis Testing},
	pages = {997--1003},
	file = {Cohen - 1994 - The earth is round (p  .05).pdf:/Users/luca/Zotero/storage/DHH6XYH4/Cohen - 1994 - The earth is round (p  .05).pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/IXDNGN6H/1995-12080-001.html:text/html},
}

@article{bakan_test_1966-1,
	title = {The test of significance in psychological research},
	volume = {66},
	issn = {1939-1455},
	doi = {10.1037/h0020412},
	abstract = {The test of significance does not provide the information concerning psychological phenomena characteristically attributed to it; and a great deal of mischief has been associated with its use. The basic logic associated with the test of significance is reviewed. The null hypothesis is characteristically false under any circumstances. Publication practices foster the reporting of small effects in populations. Psychologists have "adjusted" by misinterpretation, taking the p value as a "measure," assuming that the test of significance provides automaticity of inference, and confusing the aggregate with the general. The difficulties are illuminated by bringing to bear the contributions from the decision-theory school on the Fisher approach. The Bayesian approach is suggested. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {6},
	journal = {Psychological Bulletin},
	author = {Bakan, David},
	year = {1966},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Statistical Analysis, Experimentation, Statistical Significance, Statistical Tests, Null Hypothesis Testing},
	pages = {423--437},
	file = {Bakan - 1966 - The test of significance in psychological research.pdf:/Users/luca/Zotero/storage/VJM4JMFK/Bakan - 1966 - The test of significance in psychological research.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/PGWVNF5D/2005-10045-001.html:text/html},
}

@book{kitcher_advancement_1993,
	address = {New York},
	title = {The {Advancement} of {Science}: {Science} {Without} {Legend}, {Objectivity} {Without} {Illusions}},
	shorttitle = {The {Advancement} of {Science}},
	publisher = {Oxford University Press},
	author = {Kitcher, Philip},
	year = {1993},
	file = {Snapshot:/Users/luca/Zotero/storage/Q4V4M2KU/KITTAO-2.html:text/html},
}

@book{kitcher_advancement_1995,
	title = {The {Advancement} of {Science}: {Science} {Without} {Legend}, {Objectivity} {Without} {Illusions}},
	isbn = {978-0-19-983335-1},
	shorttitle = {The {Advancement} of {Science}},
	url = {https://academic.oup.com/book/4720},
	abstract = {Abstract. The Advancement of Science attempts to understand the notions of scientific progress, scientific objectivity, and the growth of knowledge by taking up},
	language = {en},
	urldate = {2024-02-16},
	publisher = {Oxford University Press},
	author = {Kitcher, Philip},
	month = aug,
	year = {1995},
	doi = {10.1093/0195096533.001.0001},
	file = {Kitcher - 1995 - The Advancement of Science Science Without Legend.pdf:/Users/luca/Zotero/storage/ZG5M6SII/Kitcher - 1995 - The Advancement of Science Science Without Legend.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/ULNTBTJE/4720.html:text/html},
}

@article{stamenkovic_facts_2023,
	title = {Facts and objectivity in science},
	volume = {48},
	issn = {0308-0188},
	url = {https://doi.org/10.1080/03080188.2022.2150807},
	doi = {10.1080/03080188.2022.2150807},
	abstract = {There are various conceptions of objectivity, a characteristic of the scientific enterprise, the most fundamental being objectivity as faithfulness to facts. A brute fact, which happens independently from us, becomes a scientific fact once we take cognisance of it through the means made available to us by science. Because of the complex, reciprocal relationship between scientific facts and scientific theory, the concept of objectivity as faithfulness to facts does not hold in the strict sense of an aperspectival faithfulness to brute facts. Nevertheless, it holds in the large sense of an underdetermined faithfulness to scientific facts, as long as we keep in mind the complexity of the notion of scientific fact (as theory-laden), and the role of non-factual elements in theory choice (as underdetermined by facts). Science remains our best way to separate our factual beliefs from our other kinds of beliefs.},
	number = {2},
	urldate = {2024-02-16},
	journal = {Interdisciplinary Science Reviews},
	author = {Stamenkovic, Philippe},
	month = apr,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/03080188.2022.2150807},
	keywords = {aperspectival view, facts, faithfulness to facts, Objectivity, realism, theory-ladenness, underdetermination, values},
	pages = {277--298},
	file = {Full Text PDF:/Users/luca/Zotero/storage/YCAWUHG9/Stamenkovic - 2023 - Facts and objectivity in science.pdf:application/pdf},
}

@article{hudson_should_2021,
	title = {Should {We} {Strive} to {Make} {Science} {Bias}-{Free}? {A} {Philosophical} {Assessment} of the {Reproducibility} {Crisis}},
	volume = {52},
	issn = {1572-8587},
	shorttitle = {Should {We} {Strive} to {Make} {Science} {Bias}-{Free}?},
	url = {https://doi.org/10.1007/s10838-020-09548-w},
	doi = {10.1007/s10838-020-09548-w},
	abstract = {Recently, many scientists have become concerned about an excessive number of failures to reproduce statistically significant effects. The situation has become dire enough that the situation has been named the ‘reproducibility crisis’. After reviewing the relevant literature to confirm the observation that scientists do indeed view replication as currently problematic, I explain in philosophical terms why the replication of empirical phenomena, such as statistically significant effects, is important for scientific progress. Following that explanation, I examine various diagnoses of the reproducibility crisis, and argue that for the majority of scientists the crisis is due, at least in part, to a form of publication bias. This conclusion sets the stage for an assessment of the view that evidential relations in science are inherently value-laden, a view championed by Heather Douglas and Kevin Elliott. I argue, in response to Douglas and Elliott, and as motivated by the meta-scientific resistance scientists harbour to a publication bias, that if we advocate the value-ladenness of science the result would be a deepening of the reproducibility crisis.},
	language = {en},
	number = {3},
	urldate = {2024-02-04},
	journal = {Journal for General Philosophy of Science},
	author = {Hudson, Robert},
	month = sep,
	year = {2021},
	keywords = {Publication bias, Heather Douglas, Kevin Elliott, Reproducibility crisis, Statistical significance, Value-ladenness},
	pages = {389--405},
	file = {Full Text PDF:/Users/luca/Zotero/storage/GKMM398X/Hudson - 2021 - Should We Strive to Make Science Bias-Free A Phil.pdf:application/pdf},
}

@article{francis_publication_2012,
	title = {Publication bias and the failure of replication in experimental psychology},
	volume = {19},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-012-0322-y},
	doi = {10.3758/s13423-012-0322-y},
	abstract = {Replication of empirical findings plays a fundamental role in science. Among experimental psychologists, successful replication enhances belief in a finding, while a failure to replicate is often interpreted to mean that one of the experiments is flawed. This view is wrong. Because experimental psychology uses statistics, empirical findings should appear with predictable probabilities. In a misguided effort to demonstrate successful replication of empirical findings and avoid failures to replicate, experimental psychologists sometimes report too many positive results. Rather than strengthen confidence in an effect, too much successful replication actually indicates publication bias, which invalidates entire sets of experimental findings. Researchers cannot judge the validity of a set of biased experiments because the experiment set may consist entirely of type I errors. This article shows how an investigation of the effect sizes from reported experiments can test for publication bias by looking for too much successful replication. Simulated experiments demonstrate that the publication bias test is able to discriminate biased experiment sets from unbiased experiment sets, but it is conservative about reporting bias. The test is then applied to several studies of prominent phenomena that highlight how publication bias contaminates some findings in experimental psychology. Additional simulated experiments demonstrate that using Bayesian methods of data analysis can reduce (and in some cases, eliminate) the occurrence of publication bias. Such methods should be part of a systematic process to remove publication bias from experimental psychology and reinstate the important role of replication as a final arbiter of scientific findings.},
	language = {en},
	number = {6},
	urldate = {2024-02-04},
	journal = {Psychonomic Bulletin \& Review},
	author = {Francis, Gregory},
	month = dec,
	year = {2012},
	keywords = {Publication bias, Hypothesis testing, Meta-analysis, Bayesian methods, Replication},
	pages = {975--991},
	file = {Full Text PDF:/Users/luca/Zotero/storage/N4JWTCU5/Francis - 2012 - Publication bias and the failure of replication in.pdf:application/pdf},
}

@article{ledgerwood_introduction_2014,
	title = {Introduction to the {Special} {Section} on {Advancing} {Our} {Methods} and {Practices}},
	volume = {9},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691614529448},
	doi = {10.1177/1745691614529448},
	language = {en},
	number = {3},
	urldate = {2024-02-16},
	journal = {Perspectives on Psychological Science},
	author = {Ledgerwood, Alison},
	month = may,
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	pages = {275--277},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/A3BQ5TZX/Ledgerwood - 2014 - Introduction to the Special Section on Advancing O.pdf:application/pdf},
}

@article{bird_what_2007,
	title = {What is {Scientific} {Progress}?},
	volume = {41},
	doi = {10.1111/j.1468-0068.2007.00638.x},
	number = {1},
	journal = {Noûs},
	author = {Bird, Alexander},
	year = {2007},
	note = {Publisher: Blackwell},
	pages = {64--89},
	file = {Bird - 2007 - What is Scientific Progress.pdf:/Users/luca/Zotero/storage/IGWCLNNW/Bird - 2007 - What is Scientific Progress.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/X8DBPCYE/BIRWIS.html:text/html},
}

@book{borenstein_introduction_2021,
	title = {Introduction to {Meta}-{Analysis}},
	isbn = {978-1-119-55838-5},
	abstract = {A clear and thorough introduction to meta-analysis, the process of synthesizing data from a series of separate studies The first edition of this text was widely acclaimed for the clarity of the presentation, and quickly established itself as the definitive text in this field. The fully updated second edition includes new and expanded content on avoiding common mistakes in meta-analysis, understanding heterogeneity in effects, publication bias, and more. Several brand-new chapters provide a systematic "how to" approach to performing and reporting a meta-analysis from start to finish. Written by four of the world's foremost authorities on all aspects of meta-analysis, the new edition:  Outlines the role of meta-analysis in the research process Shows how to compute effects sizes and treatment effects Explains the fixed-effect and random-effects models for synthesizing data Demonstrates how to assess and interpret variation in effect size across studies Explains how to avoid common mistakes in meta-analysis Discusses controversies in meta-analysis Includes access to a companion website containing videos, spreadsheets, data files, free software for prediction intervals, and step-by-step instructions for performing analyses using Comprehensive Meta-Analysis (CMA)  Download videos, class materials, and worked examples at www.Introduction-to-Meta-Analysis.com "This book offers the reader a unified framework for thinking about meta-analysis, and then discusses all elements of the analysis within that framework. The authors address a series of common mistakes and explain how to avoid them. As the editor-in-chief of the American Psychologist and former editor of Psychological Bulletin, I can say without hesitation that the quality of manuscript submissions reporting meta-analyses would be vastly better if researchers read this book."—Harris Cooper, Hugo L. Blomquist Distinguished Professor Emeritus of Psychology and Neuroscience, Editor-in-chief of the American Psychologist, former editor of Psychological Bulletin "A superb combination of lucid prose and informative graphics, the authors provide a refreshing departure from cookbook approaches with their clear explanations of the what and why of meta-analysis. The book is ideal as a course textbook or for self-study. My students raved about the clarity of the explanations and examples." —David Rindskopf, Distinguished Professor of Educational Psychology, City University of New York, Graduate School and University Center, \& Editor of the Journal of Educational and Behavioral Statistics "The approach taken by Introduction to Meta-analysis is intended to be primarily conceptual, and it is amazingly successful at achieving that goal. The reader can comfortably skip the formulas and still understand their application and underlying motivation. For the more statistically sophisticated reader, the relevant formulas and worked examples provide a superb practical guide to performing a meta-analysis. The book provides an eclectic mix of examples from education, social science, biomedical studies, and even ecology. For anyone considering leading a course in meta-analysis, or pursuing self-directed study, Introduction to Meta-analysis would be a clear first choice." —Jesse A. Berlin, ScD},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Borenstein, Michael and Hedges, Larry V. and Higgins, Julian P. T. and Rothstein, Hannah R.},
	month = apr,
	year = {2021},
	note = {Google-Books-ID: pdQnEAAAQBAJ},
	keywords = {Mathematics / General, Mathematics / Probability \& Statistics / Stochastic Processes, Medical / Biostatistics},
}

@incollection{cooper_research_2019,
	edition = {3},
	title = {Research synthesis as a scientific process},
	isbn = {978-0-87154-163-5},
	url = {http://www.scopus.com/inward/record.url?scp=84902712953&partnerID=8YFLogxK},
	urldate = {2024-02-17},
	booktitle = {The handbook of research synthesis and meta-analysis},
	publisher = {Russell Sage Foundation},
	author = {Cooper, Harris and Hedges, Larry Vernon and Valentine, Jeffrey C.},
	editor = {Cooper, Harris and Hedges, Larry Vernon and Valentine, Jeffrey C.},
	year = {2019},
	pages = {3--16},
	file = {Cooper et al. - 2019 - Research synthesis as a scientific process.pdf:/Users/luca/Zotero/storage/XKCIHWGG/Cooper et al. - 2019 - Research synthesis as a scientific process.pdf:application/pdf},
}

@article{kicinski_how_2014,
	title = {How does under-reporting of negative and inconclusive results affect the false-positive rate in meta-analysis? {A} simulation study},
	volume = {4},
	copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions.  This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/3.0/},
	issn = {2044-6055, 2044-6055},
	shorttitle = {How does under-reporting of negative and inconclusive results affect the false-positive rate in meta-analysis?},
	url = {https://bmjopen.bmj.com/content/4/8/e004831},
	doi = {10.1136/bmjopen-2014-004831},
	abstract = {Objective To investigate the impact of a higher publishing probability for statistically significant positive outcomes on the false-positive rate in meta-analysis.
Design Meta-analyses of different sizes (N=10, N=20, N=50 and N=100), levels of heterogeneity and levels of publication bias were simulated.
Primary and secondary outcome measures The type I error rate for the test of the mean effect size (ie, the rate at which the meta-analyses showed that the mean effect differed from 0 when it in fact equalled 0) was estimated. Additionally, the power and type I error rate of publication bias detection methods based on the funnel plot were estimated.
Results In the presence of a publication bias characterised by a higher probability of including statistically significant positive results, the meta-analyses frequently concluded that the mean effect size differed from zero when it actually equalled zero. The magnitude of the effect of publication bias increased with an increasing number of studies and between-study variability. A higher probability of including statistically significant positive outcomes introduced little asymmetry to the funnel plot. A publication bias of a sufficient magnitude to frequently overturn the meta-analytic conclusions was difficult to detect by publication bias tests based on the funnel plot. When statistically significant positive results were four times more likely to be included than other outcomes and a large between-study variability was present, more than 90\% of the meta-analyses of 50 and 100 studies wrongly showed that the mean effect size differed from zero. In the same scenario, publication bias tests based on the funnel plot detected the bias at rates not exceeding 15\%.
Conclusions This study adds to the evidence that publication bias is a major threat to the validity of medical research and supports the usefulness of efforts to limit publication bias.},
	language = {en},
	number = {8},
	urldate = {2024-02-17},
	journal = {BMJ Open},
	author = {Kicinski, Michal},
	month = aug,
	year = {2014},
	pmid = {25168036},
	note = {Publisher: British Medical Journal Publishing Group
Section: Medical publishing and peer review},
	keywords = {EGGER'S TEST, FUNNEL PLOT, META-ANALYSIS, PUBLICATION BIAS, TYPE I ERROR},
	pages = {e004831},
	file = {Full Text PDF:/Users/luca/Zotero/storage/LWNJZMM4/Kicinski - 2014 - How does under-reporting of negative and inconclus.pdf:application/pdf},
}

@article{munafo_how_2010,
	title = {How reliable are scientific studies?},
	volume = {197},
	issn = {0007-1250, 1472-1465},
	url = {https://www.cambridge.org/core/journals/the-british-journal-of-psychiatry/article/how-reliable-are-scientific-studies/96B11308710296966BF4B90EBA4F0DF2},
	doi = {10.1192/bjp.bp.109.069849},
	abstract = {SummaryThere is growing concern that a substantial proportion of scientific research may in fact be false. A number of factors have been proposed as contributing to the presence of a large number of false-positive results in the literature, one of which is publication bias. We discuss empirical evidence for these factors.},
	language = {en},
	number = {4},
	urldate = {2024-02-17},
	journal = {The British Journal of Psychiatry},
	author = {Munafò, Marcus R. and Flint, Jonathan},
	month = oct,
	year = {2010},
	note = {Publisher: Cambridge University Press},
	pages = {257--258},
	file = {Full Text PDF:/Users/luca/Zotero/storage/3IS2KBMC/Munafò und Flint - 2010 - How reliable are scientific studies.pdf:application/pdf},
}

@article{klein_many_2018,
	title = {Many {Labs} 2: {Investigating} {Variation} in {Replicability} {Across} {Samples} and {Settings}},
	volume = {1},
	issn = {2515-2459},
	shorttitle = {Many {Labs} 2},
	url = {https://doi.org/10.1177/2515245918810225},
	doi = {10.1177/2515245918810225},
	abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {\textless} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {\textless} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen’s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({\textless} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
	language = {en},
	number = {4},
	urldate = {2024-02-17},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and de Bruijn, Maaike and De Schutter, Leander and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Međedović, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, Félix and Lee Nichols, Austin and Ocampo, Aaron and O’Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, Gábor and Osowiecka, Malgorzata and Packard, Grant and Pérez-Sánchez, Rolando and Petrović, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Schönbrodt, Felix D. and Sekerdej, Maciej B. and Sirlopú, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van ’t Veer, Anna Elisabeth and Vásquez- Echeverría, Alejandro and Ann Vaughn, Leigh and Vázquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
	month = dec,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	pages = {443--490},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/7WUFDPBV/Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf:application/pdf},
}

@article{camerer_evaluating_2018,
	title = {Evaluating the replicability of social science experiments in {Nature} and {Science} between 2010 and 2015},
	volume = {2},
	copyright = {2018 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0399-z},
	doi = {10.1038/s41562-018-0399-z},
	abstract = {Being able to replicate scientific findings is crucial for scientific progress1–15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516–36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
	language = {en},
	number = {9},
	urldate = {2024-02-17},
	journal = {Nature Human Behaviour},
	author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
	month = sep,
	year = {2018},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Psychology, Economics},
	pages = {637--644},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/FWDQE589/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf:application/pdf},
}

@article{errington_investigating_2021,
	title = {Investigating the replicability of preclinical cancer biology},
	volume = {10},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.71601},
	doi = {10.7554/eLife.71601},
	abstract = {Replicability is an important feature of scientific research, but aspects of contemporary research culture, such as an emphasis on novelty, can make replicability seem less important than it should be. The Reproducibility Project: Cancer Biology was set up to provide evidence about the replicability of preclinical research in cancer biology by repeating selected experiments from high-impact papers. A total of 50 experiments from 23 papers were repeated, generating data about the replicability of a total of 158 effects. Most of the original effects were positive effects (136), with the rest being null effects (22). A majority of the original effect sizes were reported as numerical values (117), with the rest being reported as representative images (41). We employed seven methods to assess replicability, and some of these methods were not suitable for all the effects in our sample. One method compared effect sizes: for positive effects, the median effect size in the replications was 85\% smaller than the median effect size in the original experiments, and 92\% of replication effect sizes were smaller than the original. The other methods were binary – the replication was either a success or a failure – and five of these methods could be used to assess both positive and null effects when effect sizes were reported as numerical values. For positive effects, 40\% of replications (39/97) succeeded according to three or more of these five methods, and for null effects 80\% of replications (12/15) were successful on this basis; combining positive and null effects, the success rate was 46\% (51/112). A successful replication does not definitively confirm an original finding or its theoretical interpretation. Equally, a failure to replicate does not disconfirm a finding, but it does suggest that additional investigation is needed to establish its reliability.},
	urldate = {2024-02-17},
	journal = {eLife},
	author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
	editor = {Pasqualini, Renata and Franco, Eduardo},
	month = dec,
	year = {2021},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {meta-analysis, reproducibility, transparency, replication, credibility, reproducibility in cancer biology, Reproducibility Project: Cancer Biology},
	pages = {e71601},
	file = {Volltext:/Users/luca/Zotero/storage/K7S26NBL/Errington et al. - 2021 - Investigating the replicability of preclinical can.pdf:application/pdf},
}

@article{ioannidis_contradicted_2005,
	title = {Contradicted and {Initially} {Stronger} {Effects} in {Highly} {Cited} {Clinical} {Research}},
	volume = {294},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.294.2.218},
	doi = {10.1001/jama.294.2.218},
	abstract = {ContextControversy and uncertainty ensue when the results of clinical research on the effectiveness of interventions are subsequently contradicted. Controversies
are most prominent when high-impact research is involved.ObjectivesTo understand how frequently highly cited studies are contradicted or
find effects that are stronger than in other similar studies and to discern
whether specific characteristics are associated with such refutation over
time.DesignAll original clinical research studies published in 3 major general
clinical journals or high-impact-factor specialty journals in 1990-2003 and
cited more than 1000 times in the literature were examined.Main Outcome MeasureThe results of highly cited articles were compared against subsequent
studies of comparable or larger sample size and similar or better controlled
designs. The same analysis was also performed comparatively for matched studies
that were not so highly cited.ResultsOf 49 highly cited original clinical research studies, 45 claimed that
the intervention was effective. Of these, 7 (16\%) were contradicted by subsequent
studies, 7 others (16\%) had found effects that were stronger than those of
subsequent studies, 20 (44\%) were replicated, and 11 (24\%) remained largely
unchallenged. Five of 6 highly-cited nonrandomized studies had been contradicted
or had found stronger effects vs 9 of 39 randomized controlled trials (P = .008). Among randomized trials, studies with
contradicted or stronger effects were smaller (P = .009)
than replicated or unchallenged studies although there was no statistically
significant difference in their early or overall citation impact. Matched
control studies did not have a significantly different share of refuted results
than highly cited studies, but they included more studies with “negative”
results.ConclusionsContradiction and initially stronger effects are not unusual in highly
cited research of clinical interventions and their outcomes. The extent to
which high citations may provoke contradictions and vice versa needs more
study. Controversies are most common with highly cited nonrandomized studies,
but even the most highly cited randomized trials may be challenged and refuted
over time, especially small ones.},
	number = {2},
	urldate = {2024-02-17},
	journal = {JAMA},
	author = {Ioannidis, John P. A.},
	month = jul,
	year = {2005},
	pages = {218--228},
	file = {Snapshot:/Users/luca/Zotero/storage/SXG7Y2Y9/201218.html:text/html},
}

@article{youyou_discipline-wide_2023,
	title = {A discipline-wide investigation of the replicability of {Psychology} papers over the past two decades},
	volume = {120},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.2208863120},
	doi = {10.1073/pnas.2208863120},
	abstract = {Conjecture about the weak replicability in social sciences has made scholars eager to quantify the scale and scope of replication failure for a discipline. Yet small-scale manual replication methods alone are ill-suited to deal with this big data problem. Here, we conduct a discipline-wide replication census in science. Our sample (N = 14,126 papers) covers nearly all papers published in the six top-tier Psychology journals over the past 20 y. Using a validated machine learning model that estimates a paper’s likelihood of replication, we found evidence that both supports and refutes speculations drawn from a relatively small sample of manual replications. First, we find that a single overall replication rate of Psychology poorly captures the varying degree of replicability among subfields. Second, we find that replication rates are strongly correlated with research methods in all subfields. Experiments replicate at a significantly lower rate than do non-experimental studies. Third, we find that authors’ cumulative publication number and citation impact are positively related to the likelihood of replication, while other proxies of research quality and rigor, such as an author’s university prestige and a paper’s citations, are unrelated to replicability. Finally, contrary to the ideal that media attention should cover replicable research, we find that media attention is positively related to the likelihood of replication failure. Our assessments of the scale and scope of replicability are important next steps toward broadly resolving issues of replicability.},
	number = {6},
	urldate = {2024-02-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Youyou, Wu and Yang, Yang and Uzzi, Brian},
	month = feb,
	year = {2023},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2208863120},
	file = {Full Text PDF:/Users/luca/Zotero/storage/4G5UYLYL/Youyou et al. - 2023 - A discipline-wide investigation of the replicabili.pdf:application/pdf},
}

@article{prinz_believe_2011,
	title = {Believe it or not: how much can we rely on published data on potential drug targets?},
	volume = {10},
	copyright = {2011 Springer Nature Limited},
	issn = {1474-1784},
	shorttitle = {Believe it or not},
	url = {https://www.nature.com/articles/nrd3439-c1},
	doi = {10.1038/nrd3439-c1},
	language = {en},
	number = {9},
	urldate = {2024-02-17},
	journal = {Nature Reviews Drug Discovery},
	author = {Prinz, Florian and Schlange, Thomas and Asadullah, Khusru},
	month = sep,
	year = {2011},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Drug discovery},
	pages = {712--712},
	file = {Full Text PDF:/Users/luca/Zotero/storage/X96RK323/Prinz et al. - 2011 - Believe it or not how much can we rely on publish.pdf:application/pdf},
}

@article{begley_raise_2012,
	title = {Raise standards for preclinical cancer research},
	volume = {483},
	copyright = {2012 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/483531a},
	doi = {10.1038/483531a},
	abstract = {C. Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit.},
	language = {en},
	number = {7391},
	urldate = {2024-02-17},
	journal = {Nature},
	author = {Begley, C. Glenn and Ellis, Lee M.},
	month = mar,
	year = {2012},
	note = {Number: 7391
Publisher: Nature Publishing Group},
	keywords = {Cancer, Drug development},
	pages = {531--533},
	file = {Full Text PDF:/Users/luca/Zotero/storage/A9Z53KQN/Begley und Ellis - 2012 - Raise standards for preclinical cancer research.pdf:application/pdf},
}

@article{camerer_evaluating_2016,
	title = {Evaluating replicability of laboratory experiments in economics},
	volume = {351},
	url = {https://www.science.org/doi/10.1126/science.aaf0918},
	doi = {10.1126/science.aaf0918},
	abstract = {The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61\%); on average, the replicated effect size is 66\% of the original. The replicability rate varies between 67\% and 78\% for four additional replicability indicators, including a prediction market measure of peer beliefs.},
	number = {6280},
	urldate = {2024-02-17},
	journal = {Science},
	author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
	month = mar,
	year = {2016},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1433--1436},
	file = {Akzeptierte Version:/Users/luca/Zotero/storage/WKPCM6CV/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf:application/pdf},
}

@article{klein_investigating_2014,
	title = {Investigating variation in replicability: {A} "many labs" replication project},
	volume = {45},
	issn = {1864-9335},
	shorttitle = {Investigating variation in replicability},
	url = {http://www.scopus.com/inward/record.url?scp=84901716350&partnerID=8YFLogxK},
	doi = {10.1027/1864-9335/a000178},
	abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect - imagined contact reducing prejudice - showed weak support for replicability. And two effects - flag priming influencing conservatism and currency priming influencing system justification - did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
	number = {3},
	urldate = {2024-02-17},
	journal = {Social Psychology},
	author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahník, Štěpán and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and Ijzerman, Hans and John, Melissa Sue and Joy-Gaba, Jennifer A. and Kappes, Heather Barry and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and Van 'T Veer, A. E. and Vaughn, Leigh Ann and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
	year = {2014},
	pages = {142--152},
	file = {Akzeptierte Version:/Users/luca/Zotero/storage/AR6KNAU5/Klein et al. - 2014 - Investigating variation in replicability A many .pdf:application/pdf},
}

@article{ebersole_many_2016,
	series = {Special {Issue}: {Confirmatory}},
	title = {Many {Labs} 3: {Evaluating} participant pool quality across the academic semester via replication},
	volume = {67},
	issn = {0022-1031},
	shorttitle = {Many {Labs} 3},
	url = {https://www.sciencedirect.com/science/article/pii/S0022103115300123},
	doi = {10.1016/j.jesp.2015.10.012},
	abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences—conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
	urldate = {2024-02-17},
	journal = {Journal of Experimental Social Psychology},
	author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and Joy-Gaba, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and van Allen, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
	month = nov,
	year = {2016},
	keywords = {Social psychology, Replication, Cognitive psychology, Individual differences, Participant pool, Sampling effects, Situational effects},
	pages = {68--82},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/PLI7JT4Z/Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf:application/pdf;ScienceDirect Snapshot:/Users/luca/Zotero/storage/L8W73Y5X/S0022103115300123.html:text/html},
}

@article{ebersole_many_2020,
	title = {Many {Labs} 5: {Testing} {Pre}-{Data}-{Collection} {Peer} {Review} as an {Intervention} to {Increase} {Replicability}},
	volume = {3},
	issn = {2515-2459},
	shorttitle = {Many {Labs} 5},
	url = {https://doi.org/10.1177/2515245920958687},
	doi = {10.1177/2515245920958687},
	abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect (p {\textless} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3–9; median total sample = 1,279.5, range = 276–3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols (Δr = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00–.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19–.50).},
	language = {en},
	number = {3},
	urldate = {2024-02-17},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Ebersole, Charles R. and Mathur, Maya B. and Baranski, Erica and Bart-Plange, Diane-Jo and Buttrick, Nicholas R. and Chartier, Christopher R. and Corker, Katherine S. and Corley, Martin and Hartshorne, Joshua K. and IJzerman, Hans and Lazarević, Ljiljana B. and Rabagliati, Hugh and Ropovik, Ivan and Aczel, Balazs and Aeschbach, Lena F. and Andrighetto, Luca and Arnal, Jack D. and Arrow, Holly and Babincak, Peter and Bakos, Bence E. and Baník, Gabriel and Baskin, Ernest and Belopavlović, Radomir and Bernstein, Michael H. and Białek, Michał and Bloxsom, Nicholas G. and Bodroža, Bojana and Bonfiglio, Diane B. V. and Boucher, Leanne and Brühlmann, Florian and Brumbaugh, Claudia C. and Casini, Erica and Chen, Yiling and Chiorri, Carlo and Chopik, William J. and Christ, Oliver and Ciunci, Antonia M. and Claypool, Heather M. and Coary, Sean and Čolić, Marija V. and Collins, W. Matthew and Curran, Paul G. and Day, Chris R. and Dering, Benjamin and Dreber, Anna and Edlund, John E. and Falcão, Filipe and Fedor, Anna and Feinberg, Lily and Ferguson, Ian R. and Ford, Máire and Frank, Michael C. and Fryberger, Emily and Garinther, Alexander and Gawryluk, Katarzyna and Ashbaugh, Kayla and Giacomantonio, Mauro and Giessner, Steffen R. and Grahe, Jon E. and Guadagno, Rosanna E. and Hałasa, Ewa and Hancock, Peter J. B. and Hilliard, Rias A. and Hüffmeier, Joachim and Hughes, Sean and Idzikowska, Katarzyna and Inzlicht, Michael and Jern, Alan and Jiménez-Leal, William and Johannesson, Magnus and Joy-Gaba, Jennifer A. and Kauff, Mathias and Kellier, Danielle J. and Kessinger, Grecia and Kidwell, Mallory C. and Kimbrough, Amanda M. and King, Josiah P. J. and Kolb, Vanessa S. and Kołodziej, Sabina and Kovacs, Marton and Krasuska, Karolina and Kraus, Sue and Krueger, Lacy E. and Kuchno, Katarzyna and Lage, Caio Ambrosio and Langford, Eleanor V. and Levitan, Carmel A. and de Lima, Tiago Jessé Souza and Lin, Hause and Lins, Samuel and Loy, Jia E. and Manfredi, Dylan and Markiewicz, Łukasz and Menon, Madhavi and Mercier, Brett and Metzger, Mitchell and Meyet, Venus and Millen, Ailsa E. and Miller, Jeremy K. and Montealegre, Andres and Moore, Don A. and Muda, Rafał and Nave, Gideon and Nichols, Austin Lee and Novak, Sarah A. and Nunnally, Christian and Orlić, Ana and Palinkas, Anna and Panno, Angelo and Parks, Kimberly P. and Pedović, Ivana and Pękala, Emilian and Penner, Matthew R. and Pessers, Sebastiaan and Petrović, Boban and Pfeiffer, Thomas and Pieńkosz, Damian and Preti, Emanuele and Purić, Danka and Ramos, Tiago and Ravid, Jonathan and Razza, Timothy S. and Rentzsch, Katrin and Richetin, Juliette and Rife, Sean C. and Rosa, Anna Dalla and Rudy, Kaylis Hase and Salamon, Janos and Saunders, Blair and Sawicki, Przemysław and Schmidt, Kathleen and Schuepfer, Kurt and Schultze, Thomas and Schulz-Hardt, Stefan and Schütz, Astrid and Shabazian, Ani N. and Shubella, Rachel L. and Siegel, Adam and Silva, Rúben and Sioma, Barbara and Skorb, Lauren and de Souza, Luana Elayne Cunha and Steegen, Sara and Stein, L. A. R. and Sternglanz, R. Weylin and Stojilović, Darko and Storage, Daniel and Sullivan, Gavin Brent and Szaszi, Barnabas and Szecsi, Peter and Szöke, Orsolya and Szuts, Attila and Thomae, Manuela and Tidwell, Natasha D. and Tocco, Carly and Torka, Ann-Kathrin and Tuerlinckx, Francis and Vanpaemel, Wolf and Vaughn, Leigh Ann and Vianello, Michelangelo and Viganola, Domenico and Vlachou, Maria and Walker, Ryan J. and Weissgerber, Sophia C. and Wichman, Aaron L. and Wiggins, Bradford J. and Wolf, Daniel and Wood, Michael J. and Zealley, David and Žeželj, Iris and Zrubka, Mark and Nosek, Brian A.},
	month = sep,
	year = {2020},
	note = {Publisher: SAGE Publications Inc},
	pages = {309--331},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/8QYMJ9XV/Ebersole et al. - 2020 - Many Labs 5 Testing Pre-Data-Collection Peer Revi.pdf:application/pdf},
}

@article{marks-anglin_historical_2020,
	title = {A historical review of publication bias},
	volume = {11},
	copyright = {© 2020 John Wiley \& Sons Ltd},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1452},
	doi = {10.1002/jrsm.1452},
	abstract = {Publication bias is a well-known threat to the validity of meta-analyses and, more broadly, the reproducibility of scientific findings. When policies and recommendations are predicated on an incomplete evidence base, it undermines the goals of evidence-based decision-making. Great strides have been made in the last 50 years to understand and address this problem, including calls for mandatory trial registration and the development of statistical methods to detect and correct for publication bias. We offer an historical account of seminal contributions by the evidence synthesis community, with an emphasis on the parallel development of graph-based and selection model approaches. We also draw attention to current innovations and opportunities for future methodological work.},
	language = {en},
	number = {6},
	urldate = {2024-01-02},
	journal = {Research Synthesis Methods},
	author = {Marks-Anglin, Arielle and Chen, Yong},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1452},
	keywords = {meta-analysis, reproducibility, publication bias, selection bias, evidence-based medicine},
	pages = {725--742},
	file = {Full Text PDF:/Users/luca/Zotero/storage/NUPDB6WI/Marks-Anglin und Chen - 2020 - A historical review of publication bias.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/XRLDJKE5/jrsm.html:text/html},
}

@article{pastore_measuring_2019,
	title = {Measuring {Distribution} {Similarities} {Between} {Samples}: {A} {Distribution}-{Free} {Overlapping} {Index}},
	volume = {10},
	issn = {1664-1078},
	shorttitle = {Measuring {Distribution} {Similarities} {Between} {Samples}},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.01089},
	abstract = {Every day cognitive and experimental researchers attempt to find evidence in support of their hypotheses in terms of statistical differences or similarities among groups. The most typical cases involve quantifying the difference of two samples in terms of their mean values using the t statistic or other measures, such as Cohen's d or U metrics. In both cases the aim is to quantify how large such differences have to be in order to be classified as notable effects. These issues are particularly relevant when dealing with experimental and applied psychological research. However, most of these standard measures require some distributional assumptions to be correctly used, such as symmetry, unimodality, and well-established parametric forms. Although these assumptions guarantee that asymptotic properties for inference are satisfied, they can often limit the validity and interpretability of results. In this article we illustrate the use of a distribution-free overlapping measure as an alternative way to quantify sample differences and assess research hypotheses expressed in terms of Bayesian evidence. The main features and potentials of the overlapping index are illustrated by means of three empirical applications. Results suggest that using this index can considerably improve the interpretability of data analysis results in psychological research, as well as the reliability of conclusions that researchers can draw from their studies.},
	urldate = {2024-02-18},
	journal = {Frontiers in Psychology},
	author = {Pastore, Massimiliano and Calcagnì, Antonio},
	year = {2019},
	file = {Full Text PDF:/Users/luca/Zotero/storage/4UKJHM52/Pastore und Calcagnì - 2019 - Measuring Distribution Similarities Between Sample.pdf:application/pdf},
}

@incollection{vevea_publication_2019-1,
	edition = {3},
	title = {Publication bias},
	booktitle = {The handbook of research synthesis and meta-analysis},
	publisher = {Russell Sage Foundation},
	author = {Vevea, Jack L. and Coburn, Kathleen and Sutton, Alexander J.},
	editor = {Cooper, Harris and Hedges, Larry V. and Valentine, Jeffrey C.},
	year = {2019},
	pages = {383--429},
	file = {Vevea et al. - 2019 - Publication bias.pdf:/Users/luca/Zotero/storage/IEY4KBAH/Vevea et al. - 2019 - Publication bias.pdf:application/pdf},
}

@article{sterne_publication_2000,
	title = {Publication and related bias in meta-analysis: {Power} of statistical tests and prevalence in the literature},
	volume = {53},
	issn = {0895-4356},
	shorttitle = {Publication and related bias in meta-analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435600002420},
	doi = {10.1016/S0895-4356(00)00242-0},
	abstract = {Publication and selection biases in meta-analysis are more likely to affect small studies, which also tend to be of lower methodological quality. This may lead to “small-study effects,” where the smaller studies in a meta-analysis show larger treatment effects. Small-study effects may also arise because of between-trial heterogeneity. Statistical tests for small-study effects have been proposed, but their validity has been questioned. A set of typical meta-analyses containing 5, 10, 20, and 30 trials was defined based on the characteristics of 78 published meta-analyses identified in a hand search of eight journals from 1993 to 1997. Simulations were performed to assess the power of a weighted regression method and a rank correlation test in the presence of no bias, moderate bias or severe bias. We based evidence of small-study effects on P {\textless} 0.1. The power to detect bias increased with increasing numbers of trials. The rank correlation test was less powerful than the regression method. For example, assuming a control group event rate of 20\% and no treatment effect, moderate bias was detected with the regression test in 13.7\%, 23.5\%, 40.1\% and 51.6\% of meta-analyses with 5, 10, 20 and 30 trials. The corresponding figures for the correlation test were 8.5\%, 14.7\%, 20.4\% and 26.0\%, respectively. Severe bias was detected with the regression method in 23.5\%, 56.1\%, 88.3\% and 95.9\% of meta-anlyses with 5, 10, 20 and 30 trials, as compared to 11.9\%, 31.1\%, 45.3\% and 65.4\% with the correlation test. Similar results were obtained in simulations incorporating moderate treatment effects. However the regression method gave false-positive rates which were too high in some situations (large treatment effects, or few events per trial, or all trials of similar sizes). Using the regression method, evidence of small-study effects was present in 21 (26.9\%) of the 78 published meta-analyses. Tests for small-study effects should routinely be performed in meta-analysis. Their power is however limited, particularly for moderate amounts of bias or meta-analyses based on a small number of small studies. When evidence of small-study effects is found, careful consideration should be given to possible explanations for these in the reporting of the meta-analysis.},
	number = {11},
	urldate = {2024-02-18},
	journal = {Journal of Clinical Epidemiology},
	author = {Sterne, Jonathan A. C and Gavaghan, David and Egger, Matthias},
	month = nov,
	year = {2000},
	keywords = {Publication bias, Regression, Meta-analysis, Correlation, Funnel plot, Simulation study},
	pages = {1119--1129},
	file = {ScienceDirect Snapshot:/Users/luca/Zotero/storage/2NZAUGLE/S0895435600002420.html:text/html},
}

@article{egger_bias_1997,
	title = {Bias in meta-analysis detected by a simple, graphical test},
	volume = {315},
	issn = {0959-8138},
	doi = {10.1136/bmj.315.7109.629},
	abstract = {OBJECTIVE: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses.
DESIGN: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30\% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews.
MAIN OUTCOME MEASURE: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision.
RESULTS: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38\%) journal meta-analyses and 5 (13\%) Cochrane reviews, funnel plot asymmetry indicated that there was bias.
CONCLUSIONS: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution.},
	language = {eng},
	number = {7109},
	journal = {BMJ (Clinical research ed.)},
	author = {Egger, M. and Davey Smith, G. and Schneider, M. and Minder, C.},
	month = sep,
	year = {1997},
	pmid = {9310563},
	pmcid = {PMC2127453},
	keywords = {Statistics as Topic, Meta-Analysis as Topic, Bias, Randomized Controlled Trials as Topic, Regression Analysis, Treatment Outcome},
	pages = {629--634},
	file = {Volltext:/Users/luca/Zotero/storage/MQ3PBZZJ/Egger et al. - 1997 - Bias in meta-analysis detected by a simple, graphi.pdf:application/pdf},
}

@article{stanley_meta-regression_2014,
	title = {Meta-regression approximations to reduce publication selection bias},
	volume = {5},
	copyright = {Copyright © 2013 John Wiley \& Sons, Ltd.},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1095},
	doi = {10.1002/jrsm.1095},
	abstract = {Publication selection bias is a serious challenge to the integrity of all empirical sciences. We derive meta-regression approximations to reduce this bias. Our approach employs Taylor polynomial approximations to the conditional mean of a truncated distribution. A quadratic approximation without a linear term, precision-effect estimate with standard error (PEESE), is shown to have the smallest bias and mean squared error in most cases and to outperform conventional meta-analysis estimators, often by a great deal. Monte Carlo simulations also demonstrate how a new hybrid estimator that conditionally combines PEESE and the Egger regression intercept can provide a practical solution to publication selection bias. PEESE is easily expanded to accommodate systematic heterogeneity along with complex and differential publication selection bias that is related to moderator variables. By providing an intuitive reason for these approximations, we can also explain why the Egger regression works so well and when it does not. These meta-regression methods are applied to several policy-relevant areas of research including antidepressant effectiveness, the value of a statistical life, the minimum wage, and nicotine replacement therapy. Copyright © 2013 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2024-02-18},
	journal = {Research Synthesis Methods},
	author = {Stanley, T. D. and Doucouliagos, Hristos},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1095},
	keywords = {publication selection bias, meta-regression, systematic reviews, truncation},
	pages = {60--78},
	file = {Full Text PDF:/Users/luca/Zotero/storage/XAKMMYZB/Stanley und Doucouliagos - 2014 - Meta-regression approximations to reduce publicati.pdf:application/pdf},
}

@article{duval_trim_2000,
	title = {Trim and fill: {A} simple funnel-plot-based method of testing and adjusting for publication bias in meta-analysis},
	volume = {56},
	issn = {0006-341X},
	shorttitle = {Trim and fill},
	doi = {10.1111/j.0006-341x.2000.00455.x},
	abstract = {We study recently developed nonparametric methods for estimating the number of missing studies that might exist in a meta-analysis and the effect that these studies might have had on its outcome. These are simple rank-based data augmentation techniques, which formalize the use of funnel plots. We show that they provide effective and relatively powerful tests for evaluating the existence of such publication bias. After adjusting for missing studies, we find that the point estimate of the overall effect size is approximately correct and coverage of the effect size confidence intervals is substantially improved, in many cases recovering the nominal confidence levels entirely. We illustrate the trim and fill method on existing meta-analyses of studies in clinical trials and psychometrics.},
	language = {eng},
	number = {2},
	journal = {Biometrics},
	author = {Duval, S. and Tweedie, R.},
	month = jun,
	year = {2000},
	pmid = {10877304},
	keywords = {Humans, Psychometrics, Analysis of Variance, Publishing, Meta-Analysis as Topic, Bias, Statistics, Nonparametric, Clinical Trials as Topic, Biometry},
	pages = {455--463},
	file = {Duval und Tweedie - 2000 - Trim and fill A simple funnel-plot-based method o.pdf:/Users/luca/Zotero/storage/WAKX2A7B/Duval und Tweedie - 2000 - Trim and fill A simple funnel-plot-based method o.pdf:application/pdf},
}

@article{orwin_fail-safe_1983,
	title = {A {Fail}-{Safe} {N} for {Effect} {Size} in {Meta}-{Analysis}},
	volume = {8},
	issn = {0362-9791},
	url = {https://www.jstor.org/stable/1164923},
	doi = {10.2307/1164923},
	abstract = {Rosenthan's (1979) concept of fail-safe N has thus far been applied to probability levels exclusively. This note introduces a fail-safe N for effect size.},
	number = {2},
	urldate = {2024-02-18},
	journal = {Journal of Educational Statistics},
	author = {Orwin, Robert G.},
	year = {1983},
	note = {Publisher: [Sage Publications, Inc., American Educational Research Association, American Statistical Association]},
	pages = {157--159},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/JSM6RHNZ/Orwin - 1983 - A Fail-Safe N for Effect Size in Meta-Analysis.pdf:application/pdf},
}

@article{obrien_fast_2016,
	title = {A fast and objective multidimensional kernel density estimation method: {fastKDE}},
	volume = {101},
	issn = {0167-9473},
	shorttitle = {A fast and objective multidimensional kernel density estimation method},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947316300408},
	doi = {10.1016/j.csda.2016.02.014},
	abstract = {Numerous facets of scientific research implicitly or explicitly call for the estimation of probability densities. Histograms and kernel density estimates (KDEs) are two commonly used techniques for estimating such information, with the KDE generally providing a higher fidelity representation of the probability density function (PDF). Both methods require specification of either a bin width or a kernel bandwidth. While techniques exist for choosing the kernel bandwidth optimally and objectively, they are computationally intensive, since they require repeated calculation of the KDE. A solution for objectively and optimally choosing both the kernel shape and width has recently been developed by Bernacchia and Pigolotti (2011). While this solution theoretically applies to multidimensional KDEs, it has not been clear how to practically do so. A method for practically extending the Bernacchia–Pigolotti KDE to multidimensions is introduced. This multidimensional extension is combined with a recently-developed computational improvement to their method that makes it computationally efficient: a 2D KDE on 105 samples only takes 1 s on a modern workstation. This fast and objective KDE method, called the fastKDE method, retains the excellent statistical convergence properties that have been demonstrated for univariate samples. The fastKDE method exhibits statistical accuracy that is comparable to state-of-the-science KDE methods publicly available in R, and it produces kernel density estimates several orders of magnitude faster. The fastKDE method does an excellent job of encoding covariance information for bivariate samples. This property allows for direct calculation of conditional PDFs with fastKDE. It is demonstrated how this capability might be leveraged for detecting non-trivial relationships between quantities in physical systems, such as transitional behavior.},
	urldate = {2024-02-19},
	journal = {Computational Statistics \& Data Analysis},
	author = {O’Brien, Travis A. and Kashinath, Karthik and Cavanaugh, Nicholas R. and Collins, William D. and O’Brien, John P.},
	month = sep,
	year = {2016},
	keywords = {ECF, Empirical characteristic function, Histogram, KDE, Kernel density estimation, Multidimensional, Nonuniform FFT, NuFFT},
	pages = {148--160},
	file = {ScienceDirect Snapshot:/Users/luca/Zotero/storage/BAVYRN2G/S0167947316300408.html:text/html;Volltext:/Users/luca/Zotero/storage/G9LUPWHL/O’Brien et al. - 2016 - A fast and objective multidimensional kernel densi.pdf:application/pdf},
}

@article{bradley_sweeping_2013,
	title = {Sweeping recommendations regarding effect size and sample size can miss important nuances: {A} comment on “{A} comprehensive review of reporting practices in psychological journals”},
	volume = {23},
	issn = {0959-3543},
	shorttitle = {Sweeping recommendations regarding effect size and sample size can miss important nuances},
	url = {https://doi.org/10.1177/0959354313491854},
	doi = {10.1177/0959354313491854},
	abstract = {Statistical significance tests done with low statistical power levels can result in reports of exaggerated effect sizes. Funnel graphs can show these exaggerations for a given area of research by revealing a negative correlation between a study’s sample size on the y-axis and the effect size obtain by the study on the x-axis. A recent paper by Fritz, Scherndl, and Kühberger (2013) recommended that effect sizes should not be reported when there is a negative correlation between sample size and effect size for a given research area. This recommendation fails to consider magnitudes of the negative correlations and thereby misses the opportunity to mitigate effect size exaggerations and approximate more correct effect size estimates. This comment explains both the incorrectness of the recommendation and the approach and calculations necessary to correct effect size estimates.},
	language = {en},
	number = {6},
	urldate = {2024-02-20},
	journal = {Theory \& Psychology},
	author = {Bradley, Michael T. and Brand, Andrew},
	month = dec,
	year = {2013},
	note = {Publisher: SAGE Publications Ltd},
	pages = {797--800},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/F6LDPKWS/Bradley und Brand - 2013 - Sweeping recommendations regarding effect size and.pdf:application/pdf},
}

@article{fritz_comprehensive_2013,
	title = {A comprehensive review of reporting practices in psychological journals: {Are} effect sizes really enough?},
	volume = {23},
	issn = {1461-7447},
	shorttitle = {A comprehensive review of reporting practices in psychological journals},
	doi = {10.1177/0959354312436870},
	abstract = {Over-reliance on significance testing has been heavily criticized in psychology. Therefore the American Psychological Association recommended supplementing the p value with additional elements such as effect sizes, confidence intervals, and considering statistical power seriously. This article elaborates the conclusions that can be drawn when these measures accompany the p value. An analysis of over 30 summary papers (including over 6,000 articles) reveals that, if at all, only effect sizes are reported in addition to p’s (38\%). Only every 10th article provides a confidence interval and statistical power is reported in only 3\% of articles. An increase in reporting frequency of the supplements to p’s over time owing to stricter guidelines was found for effect sizes only. Given these practices, research faces a serious problem in the context of dichotomous statistical decision making: since significant results have a higher probability of being published (publication bias), effect sizes reported in articles may be seriously overestimated. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	number = {1},
	journal = {Theory \& Psychology},
	author = {Fritz, Astrid and Scherndl, Thomas and Kühberger, Anton},
	year = {2013},
	note = {Place: US
Publisher: Sage Publications},
	keywords = {Decision Making, Statistical Analysis, Statistical Power, Statistical Probability},
	pages = {98--122},
	file = {Fritz et al. - 2013 - A comprehensive review of reporting practices in p.pdf:/Users/luca/Zotero/storage/LASPP6PT/Fritz et al. - 2013 - A comprehensive review of reporting practices in p.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/Z5QB55W7/2013-03308-006.html:text/html},
}

@article{schmidt_statistical_1996,
	title = {Statistical significance testing and cumulative knowledge in psychology: {Implications} for training of researchers},
	volume = {1},
	issn = {1939-1463},
	shorttitle = {Statistical significance testing and cumulative knowledge in psychology},
	doi = {10.1037/1082-989X.1.2.115},
	abstract = {Data analysis methods in psychology still emphasize statistical significance testing, despite numerous articles demonstrating its severe deficiencies. It is now possible to use meta-analysis to show that reliance on significance testing retards the development of cumulative knowledge. But reform of teaching and practice will also require that researchers learn that the benefits that they believe flow from use of significance testing are illusory. Teachers must revamp their courses to bring students to understand that (a) reliance on significance testing retards the growth of cumulative research knowledge; (b) benefits widely believed to flow from significance testing do not in fact exist; and (c) significance testing methods must be replaced with point estimates and confidence intervals in individual studies and with meta-analyses in the integration of multiple studies. This reform is essential to the future progress of cumulative knowledge in psychological research. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Methods},
	author = {Schmidt, Frank L.},
	year = {1996},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Meta Analysis, Statistical Significance, Statistical Tests, Graduate Psychology Education},
	pages = {115--129},
	file = {Schmidt - 1996 - Statistical significance testing and cumulative kn.pdf:/Users/luca/Zotero/storage/I478J6BK/Schmidt - 1996 - Statistical significance testing and cumulative kn.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/QMEEEFC3/1996-04469-001.html:text/html},
}

@article{van_assen_meta-plot_2023,
	title = {The {Meta}-{Plot}},
	volume = {231},
	issn = {2190-8370},
	url = {https://econtent.hogrefe.com/doi/full/10.1027/2151-2604/a000513},
	doi = {10.1027/2151-2604/a000513},
	abstract = {. The meta-plot is a descriptive visual tool for meta-analysis that provides information on the primary studies in the meta-analysis and the results of the meta-analysis. More precisely, the meta-plot portrays (1) the precision and statistical power of the primary studies in the meta-analysis, (2) the estimate and confidence interval of a random-effects meta-analysis, (3) the results of a cumulative random-effects meta-analysis yielding a robustness check of the meta-analytic effect size with respect to primary studies’ precision, and (4) evidence of publication bias. After explaining the underlying logic and theory, the meta-plot is applied to two cherry-picked meta-analyses that appear to be biased and to 10 randomly selected meta-analyses from the psychological literature. We recommend accompanying any meta-analysis of common effect size measures with the meta-plot.},
	number = {1},
	urldate = {2024-02-20},
	journal = {Zeitschrift für Psychologie},
	author = {van Assen, Marcel A. L. M. and van den Akker, Olmo R. and Augusteijn, Hilde E. M. and Bakker, Marjan and Nuijten, Michèle B. and Olsson-Collentine, Anton and Stoevenbelt, Andrea H. and Wicherts, Jelte M. and van Aert, Robbie C. M.},
	month = feb,
	year = {2023},
	note = {Publisher: Hogrefe Publishing},
	keywords = {meta-analysis, publication bias, statistical power, cumulative meta-analysis, funnel plot},
	pages = {65--78},
	file = {Full Text PDF:/Users/luca/Zotero/storage/L2T59LUX/van Assen et al. - 2023 - The Meta-Plot.pdf:application/pdf},
}

@article{fraley_n-pact_2014,
	title = {The {N}-{Pact} {Factor}: {Evaluating} the {Quality} of {Empirical} {Journals} with {Respect} to {Sample} {Size} and {Statistical} {Power}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {The {N}-{Pact} {Factor}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0109019},
	doi = {10.1371/journal.pone.0109019},
	abstract = {The authors evaluate the quality of research reported in major journals in social-personality psychology by ranking those journals with respect to their N-pact Factors (NF)—the statistical power of the empirical studies they publish to detect typical effect sizes. Power is a particularly important attribute for evaluating research quality because, relative to studies that have low power, studies that have high power are more likely to (a) to provide accurate estimates of effects, (b) to produce literatures with low false positive rates, and (c) to lead to replicable findings. The authors show that the average sample size in social-personality research is 104 and that the power to detect the typical effect size in the field is approximately 50\%. Moreover, they show that there is considerable variation among journals in sample sizes and power of the studies they publish, with some journals consistently publishing higher power studies than others. The authors hope that these rankings will be of use to authors who are choosing where to submit their best work, provide hiring and promotion committees with a superior way of quantifying journal quality, and encourage competition among journals to improve their NF rankings.},
	language = {en},
	number = {10},
	urldate = {2024-02-22},
	journal = {PLOS ONE},
	author = {Fraley, R. Chris and Vazire, Simine},
	month = oct,
	year = {2014},
	note = {Publisher: Public Library of Science},
	keywords = {Psychology, Experimental psychology, Scientific publishing, Research design, Social psychology, Cognitive psychology, Citation analysis, Personality},
	pages = {e109019},
	file = {Full Text PDF:/Users/luca/Zotero/storage/IEBPLSAZ/Fraley und Vazire - 2014 - The N-Pact Factor Evaluating the Quality of Empir.pdf:application/pdf},
}

@article{ioannidis_issues_1998,
	title = {Issues in {Comparisons} {Between} {Meta}-analyses and {Large} {Trials}},
	volume = {279},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.279.14.1089},
	doi = {10.1001/jama.279.14.1089},
	abstract = {Context.—The extent of concordance between meta-analyses and large trials on the same topic has been investigated with different protocols. Inconsistent conclusions created confusion regarding the validity of these major tools
of clinical evidence.Objective.—To evaluate protocols comparing meta-analyses and large trials in order
to understand if and why they disagree on the concordance of these 2 clinical
research methods.Design.—Systematic comparison of protocol designs, study selection, definitions
of agreement, analysis methods, and reported discrepancies between large trials
and meta-analyses.Results.—More discrepancies were claimed when large trials were selected from
influential journals (which may prefer trials disagreeing with prior evidence)
than from already performed meta-analyses (which may target homogeneous trials)
and when both primary and secondary (rather than only primary) end points
were considered. Depending on how agreement was defined, kappa coefficients
varied from 0.22 (low agreement) to 0.72 (excellent agreement). The correlation
of treatment effects between large trials and meta-analyses varied from −0.12
to 0.76, but was more similar (0.50-0.76) when only primary end points were
considered. When both the magnitude and uncertainty of treatment effects were
considered, large trials disagreed with meta-analyses 10\% to 23\% of the time.
Discrepancies were attributed to different disease risks, variable protocols,
quality, and publication bias.Conclusions.—Comparisons of large trials with meta-analyses may reach different conclusions
depending on how trials and meta-analyses are selected and how end points
and agreement are defined. Scrutiny of these 2 major research methods can
enhance our appreciation of both for guiding medical practice.},
	number = {14},
	urldate = {2024-02-22},
	journal = {JAMA},
	author = {Ioannidis, John P. A. and Cappelleri, Joseph C. and Lau, Joseph},
	month = apr,
	year = {1998},
	pages = {1089--1093},
	file = {Snapshot:/Users/luca/Zotero/storage/E7ECPXNZ/187421.html:text/html},
}

@article{sterne_publication_2000-1,
	title = {Publication and related bias in meta-analysis: {Power} of statistical tests and prevalence in the literature},
	volume = {53},
	issn = {0895-4356},
	shorttitle = {Publication and related bias in meta-analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435600002420},
	doi = {10.1016/S0895-4356(00)00242-0},
	abstract = {Publication and selection biases in meta-analysis are more likely to affect small studies, which also tend to be of lower methodological quality. This may lead to “small-study effects,” where the smaller studies in a meta-analysis show larger treatment effects. Small-study effects may also arise because of between-trial heterogeneity. Statistical tests for small-study effects have been proposed, but their validity has been questioned. A set of typical meta-analyses containing 5, 10, 20, and 30 trials was defined based on the characteristics of 78 published meta-analyses identified in a hand search of eight journals from 1993 to 1997. Simulations were performed to assess the power of a weighted regression method and a rank correlation test in the presence of no bias, moderate bias or severe bias. We based evidence of small-study effects on P {\textless} 0.1. The power to detect bias increased with increasing numbers of trials. The rank correlation test was less powerful than the regression method. For example, assuming a control group event rate of 20\% and no treatment effect, moderate bias was detected with the regression test in 13.7\%, 23.5\%, 40.1\% and 51.6\% of meta-analyses with 5, 10, 20 and 30 trials. The corresponding figures for the correlation test were 8.5\%, 14.7\%, 20.4\% and 26.0\%, respectively. Severe bias was detected with the regression method in 23.5\%, 56.1\%, 88.3\% and 95.9\% of meta-anlyses with 5, 10, 20 and 30 trials, as compared to 11.9\%, 31.1\%, 45.3\% and 65.4\% with the correlation test. Similar results were obtained in simulations incorporating moderate treatment effects. However the regression method gave false-positive rates which were too high in some situations (large treatment effects, or few events per trial, or all trials of similar sizes). Using the regression method, evidence of small-study effects was present in 21 (26.9\%) of the 78 published meta-analyses. Tests for small-study effects should routinely be performed in meta-analysis. Their power is however limited, particularly for moderate amounts of bias or meta-analyses based on a small number of small studies. When evidence of small-study effects is found, careful consideration should be given to possible explanations for these in the reporting of the meta-analysis.},
	number = {11},
	urldate = {2024-02-22},
	journal = {Journal of Clinical Epidemiology},
	author = {Sterne, Jonathan A. C and Gavaghan, David and Egger, Matthias},
	month = nov,
	year = {2000},
	keywords = {Publication bias, Regression, Meta-analysis, Correlation, Funnel plot, Simulation study},
	pages = {1119--1129},
	file = {ScienceDirect Snapshot:/Users/luca/Zotero/storage/IXIPT5FA/S0895435600002420.html:text/html},
}

@article{givens_publication_1997,
	title = {Publication {Bias} in {Meta}-{Analysis}: {A} {Bayesian} {Data}-{Augmentation} {Approach} to {Account} for {Issues} {Exemplified} in the {Passive} {Smoking} {Debate}},
	volume = {12},
	issn = {0883-4237},
	shorttitle = {Publication {Bias} in {Meta}-{Analysis}},
	url = {https://www.jstor.org/stable/2246205},
	abstract = {"Publication bias" is a relatively new statistical phenomenon that only arises when one attempts through a meta-analysis to review all studies, significant or insignificant, in order to provide a total perspective on a particular issue. This has recently received some notoriety as an issue in the evaluation of the relative risk of lung cancer associated with passive smoking, following legal challenges to a 1992 Environmental Protection Agency analysis which concluded that such exposure is associated with significant excess risk of lung cancer. We introduce a Bayesian approach which estimates and adjusts for publication bias. Estimation is based on a data-augmentation principle within a hierarchical model, and the number and outcomes of unobserved studies are simulated using Gibbs sampling methods. This technique yields a quantitative adjustment for the passive smoking meta-analysis. We estimate that there may be both negative and positive but insignificant studies omitted, and that failing to allow for these would mean that the estimated excess risk may be overstated by around 30\%, both in U.S. studies and in the global collection of studies.},
	number = {4},
	urldate = {2024-02-22},
	journal = {Statistical Science},
	author = {Givens, Geof H. and Smith, D. D. and Tweedie, R. L.},
	year = {1997},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {221--240},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/YS47PNG7/Givens et al. - 1997 - Publication Bias in Meta-Analysis A Bayesian Data.pdf:application/pdf},
}

@article{slavin_effective_2008,
	title = {Effective reading programs for middle and high schools: {A} best-evidence synthesis},
	volume = {43},
	issn = {1936-2722},
	shorttitle = {Effective reading programs for middle and high schools},
	doi = {10.1598/RRQ.43.3.4},
	abstract = {This article systematically reviews research on the achievement outcomes of four types of approaches to improving the reading of middle and high school students: (1) reading curricula, (2) mixed-method models (methods that combine large-and small-group instruction with computer activities), (3) computer-assisted instruction, and (4) instructional-process programs (methods that focus on providing teachers with extensive professional development to implement specific instructional methods). Criteria for inclusion in the study were use of randomized or matched control groups, a study duration of at least 12 weeks, and valid achievement measures that were independent of the experimental treatments. A total of 33 studies met these criteria. The review concludes that programs designed to change daily teaching practices have substantially greater research support than those focused on curriculum or technology alone. Positive achievement effects were found for instructional-process programs, especially for those involving cooperative learning, and for mixed-method programs. The effective approaches provided extensive professional development and significantly affected teaching practices. In contrast, no studies of reading curricula met the inclusion criteria, and the effects of supplementary computer-assisted instruction were small. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Reading Research Quarterly},
	author = {Slavin, Robert E. and Cheung, Alan and Groff, Cynthia and Lake, Cynthia},
	year = {2008},
	note = {Place: US
Publisher: International Reading Association},
	keywords = {Educational Programs, High Schools, Middle Schools, Reading Education},
	pages = {290--322},
	file = {Snapshot:/Users/luca/Zotero/storage/Y4YLJCZG/2008-09872-004.html:text/html},
}

@article{lovakov_empirically_2021-3,
	title = {Empirically derived guidelines for effect size interpretation in social psychology},
	volume = {51},
	copyright = {© 2021 John Wiley \& Sons, Ltd.},
	issn = {1099-0992},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2752},
	doi = {10.1002/ejsp.2752},
	abstract = {This study estimates empirically derived guidelines for effect size interpretation for research in social psychology overall and sub-disciplines within social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis of empirically derived distributions of 12,170 correlation coefficients and 6,447 Cohen's d statistics extracted from studies included in 134 published meta-analyses revealed that the 25th, 50th, and 75th percentiles corresponded to correlation coefficient values of 0.12, 0.24, and 0.41 and to Cohen's d values of 0.15, 0.36, and 0.65 respectively. The analysis suggests that the widely used Cohen's guidelines tend to overestimate medium and large effect sizes. Empirically derived effect size distributions in social psychology overall and its sub-disciplines can be used both for effect size interpretation and for sample size planning when other information about effect size is not available.},
	language = {en},
	number = {3},
	urldate = {2024-02-25},
	journal = {European Journal of Social Psychology},
	author = {Lovakov, Andrey and Agadullina, Elena R.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ejsp.2752},
	keywords = {effect size, sample size, correlation, Cohen's d},
	pages = {485--504},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/88RAV22I/Lovakov und Agadullina - 2021 - Empirically derived guidelines for effect size int.pdf:application/pdf},
}

@article{van_assen_meta-analysis_2015,
	title = {Meta-analysis using effect size distributions of only statistically significant studies},
	volume = {20},
	issn = {1939-1463},
	doi = {10.1037/met0000025},
	abstract = {Publication bias threatens the validity of meta-analytic results and leads to overestimation of the effect size in traditional meta-analysis. This particularly applies to meta-analyses that feature small studies, which are ubiquitous in psychology. Here we develop a new method for meta-analysis that deals with publication bias. This method, p-uniform, enables (a) testing of publication bias, (b) effect size estimation, and (c) testing of the null-hypothesis of no effect. No current method for meta-analysis possesses all 3 qualities. Application of p-uniform is straightforward because no additional data on missing studies are needed and no sophisticated assumptions or choices need to be made before applying it. Simulations show that p-uniform generally outperforms the trim-and-fill method and the test of excess significance (TES; Ioannidis \& Trikalinos, 2007b) if publication bias exists and population effect size is homogenous or heterogeneity is slight. For illustration, p-uniform and other publication bias analyses are applied to the meta-analysis of McCall and Carriger (1993) examining the association between infants' habituation to a stimulus and their later cognitive ability (IQ). We conclude that p-uniform is a valuable technique for examining publication bias and estimating population effects in fixed-effect meta-analyses, and as sensitivity analysis to draw inferences about publication bias.},
	language = {eng},
	number = {3},
	journal = {Psychological Methods},
	author = {van Assen, Marcel A. L. M. and van Aert, Robbie C. M. and Wicherts, Jelte M.},
	month = sep,
	year = {2015},
	pmid = {25401773},
	keywords = {Humans, Publication Bias, Data Interpretation, Statistical, Meta-Analysis as Topic, Statistical Distributions},
	pages = {293--309},
	file = {van Assen et al. - 2015 - Meta-analysis using effect size distributions of o.pdf:/Users/luca/Zotero/storage/45LRXBX6/van Assen et al. - 2015 - Meta-analysis using effect size distributions of o.pdf:application/pdf},
}

@misc{posit_team_rstudio_2023,
	address = {Boston},
	title = {{RStudio}: {Integrated} {Development} {Environment} for {R}},
	url = {http://www.posit.co/},
	publisher = {Posit Software, PBC},
	author = {Posit Team},
	year = {2023},
}

@misc{r_core_team_r_2023,
	address = {Vienna},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org},
	publisher = {R Foundation for Statistical Computing},
	author = {R Core Team},
	year = {2023},
}

@misc{zeileis_betareg_2021,
	title = {betareg: {Beta} {Regression}},
	copyright = {GPL-2 {\textbar} GPL-3},
	shorttitle = {betareg},
	url = {https://cran.r-project.org/web/packages/betareg/index.html},
	abstract = {Beta regression for modeling beta-distributed dependent variables, e.g., rates and proportions. In addition to maximum likelihood regression (for both mean and precision of a beta-distributed response), bias-corrected and bias-reduced estimation as well as finite mixture models and recursive partitioning for beta regressions are provided.},
	urldate = {2024-03-04},
	author = {Zeileis, Achim and Cribari-Neto, Francisco and Gruen, Bettina and Kosmidis, Ioannis and by), Alexandre B. Simas (earlier version and by), Andrea V. Rocha (earlier version},
	month = feb,
	year = {2021},
	keywords = {Psychometrics, Econometrics},
}

@misc{lakens_toster_2023,
	title = {{TOSTER}: {Two} {One}-{Sided} {Tests} ({TOST}) {Equivalence} {Testing}},
	copyright = {GPL-3},
	shorttitle = {{TOSTER}},
	url = {https://cran.r-project.org/web/packages/TOSTER/index.html},
	abstract = {Two one-sided tests (TOST) procedure to test equivalence for t-tests, correlations, differences between proportions, and meta-analyses, including power analysis for t-tests and correlations. Allows you to specify equivalence bounds in raw scale units or in terms of effect sizes. See: Lakens (2017) {\textless}doi:10.1177/1948550617697177{\textgreater}.},
	urldate = {2024-03-04},
	author = {Lakens, Daniel and Caldwell, Aaron},
	month = sep,
	year = {2023},
}

@article{cribari-neto_beta_2010,
	title = {Beta {Regression} in {R}},
	volume = {34},
	copyright = {Copyright (c) 2009 Francisco Cribari-Neto, Achim Zeileis},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v034.i02},
	doi = {10.18637/jss.v034.i02},
	abstract = {The class of beta regression models is commonly used by practitioners to model variables that assume values in the standard unit interval (0, 1). It is based on the assumption that the dependent variable is beta-distributed and that its mean is related to a set of regressors through a linear predictor with unknown coefficients and a link function. The model also includes a precision parameter which may be constant or depend on a (potentially different) set of regressors through a link function as well. This approach naturally incorporates features such as heteroskedasticity or skewness which are commonly observed in data taking values in the standard unit interval, such as rates or proportions. This paper describes the betareg package which provides the class of beta regressions in the R system for statistical computing. The underlying theory is briefly outlined, the implementation discussed and illustrated in various replication exercises.},
	language = {en},
	urldate = {2024-03-04},
	journal = {Journal of Statistical Software},
	author = {Cribari-Neto, Francisco and Zeileis, Achim},
	month = apr,
	year = {2010},
	pages = {1--24},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/PG276BDN/Cribari-Neto und Zeileis - 2010 - Beta Regression in R.pdf:application/pdf},
}

@misc{noauthor_better_nodate,
	title = {A better lemon squeezer? {Maximum}-likelihood regression with beta-distributed dependent variables - {PubMed}},
	url = {https://pubmed.ncbi.nlm.nih.gov/16594767/},
	urldate = {2024-03-04},
	file = {A better lemon squeezer? Maximum-likelihood regression with beta-distributed dependent variables - PubMed:/Users/luca/Zotero/storage/HNUAPM73/16594767.html:text/html},
}

@article{smithson_better_2006,
	title = {A better lemon squeezer? {Maximum}-likelihood regression with beta-distributed dependent variables},
	volume = {11},
	issn = {1082-989X},
	shorttitle = {A better lemon squeezer?},
	doi = {10.1037/1082-989X.11.1.54},
	abstract = {Uncorrectable skew and heteroscedasticity are among the "lemons" of psychological data, yet many important variables naturally exhibit these properties. For scales with a lower and upper bound, a suitable candidate for models is the beta distribution, which is very flexible and models skew quite well. The authors present maximum-likelihood regression models assuming that the dependent variable is conditionally beta distributed rather than Gaussian. The approach models both means (location) and variances (dispersion) with their own distinct sets of predictors (continuous and/or categorical), thereby modeling heteroscedasticity. The location sub-model link function is the logit and thereby analogous to logistic regression, whereas the dispersion sub-model is log linear. Real examples show that these models handle the independent observations case readily. The article discusses comparisons between beta regression and alternative techniques, model selection and interpretation, practical estimation, and software.},
	language = {eng},
	number = {1},
	journal = {Psychological Methods},
	author = {Smithson, Michael and Verkuilen, Jay},
	month = mar,
	year = {2006},
	pmid = {16594767},
	keywords = {Humans, Analysis of Variance, Reproducibility of Results, Normal Distribution, Models, Statistical, Data Interpretation, Statistical, Bias, Linear Models, Regression Analysis, Child, Dyslexia, Least-Squares Analysis, Likelihood Functions},
	pages = {54--71},
	file = {Smithson und Verkuilen - 2006 - A better lemon squeezer Maximum-likelihood regres.pdf:/Users/luca/Zotero/storage/J6BNSW7K/Smithson und Verkuilen - 2006 - A better lemon squeezer Maximum-likelihood regres.pdf:application/pdf},
}

@article{kirkpatrick_optimization_1983,
	title = {Optimization by {Simulated} {Annealing}},
	volume = {220},
	url = {https://www.science.org/doi/10.1126/science.220.4598.671},
	doi = {10.1126/science.220.4598.671},
	abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
	number = {4598},
	urldate = {2024-03-06},
	journal = {Science},
	author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
	month = may,
	year = {1983},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {671--680},
	file = {Full Text PDF:/Users/luca/Zotero/storage/SIKJXMSB/Kirkpatrick et al. - 1983 - Optimization by Simulated Annealing.pdf:application/pdf},
}

@unpublished{husmann_r_2017,
	title = {The {R} {Package} optimization: {Flexible} {Global} {Optimization} with {Simulated}-{Annealing}},
	shorttitle = {The {R} {Package} optimization},
	abstract = {Standard numerical optimization approaches require several restrictions. So do exact optimization methods such as the Linear Programming approach appeal for linearity and Nelder-Mead for unimodality of the loss function. One method to relax these assumptions is the Simulated An-nealing approach, which reduces the risk of getting trapped in a local optimum. However, the standard implementation still requires regular parameter spaces and continuous loss functions. To address this issue, we implemented a version of the Simulated Annealing method that is able to deal with irregular and complex parameter spaces as well as with non-continuous and sophisticated loss functions. Moreover, in order to gain fast but reliable solutions, we included steps to shrink the parameter space during the iterations. All these steps are summarized in the R package optimization, which we will introduce in the following article. We also included generic and real world applications in order to test our approach.},
	author = {Husmann, Kai and Lange, Alexander and Spiegel, Elmar},
	month = oct,
	year = {2017},
	doi = {10.13140/RG.2.2.16976.81927},
	file = {Full Text PDF:/Users/luca/Zotero/storage/EVDILAVA/Husmann et al. - 2017 - The R Package optimization Flexible Global Optimi.pdf:application/pdf},
}

@misc{husmann_optimization_2022,
	title = {optimization: {Flexible} {Optimization} of {Complex} {Loss} {Functions} with {State} and {Parameter} {Space} {Constraints}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {optimization},
	url = {https://cran.r-project.org/web/packages/optimization/index.html},
	abstract = {Flexible optimizer with numerous input specifications for detailed parameterisation. Designed for complex loss functions with state and parameter space constraints. Visualization tools for validation and analysis of the convergence are included.},
	urldate = {2024-03-06},
	author = {Husmann, Kai and Lange, Alexander},
	month = feb,
	year = {2022},
}

@article{sheather_reliable_1991,
	title = {A {Reliable} {Data}-{Based} {Bandwidth} {Selection} {Method} for {Kernel} {Density} {Estimation}},
	volume = {53},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2345597},
	abstract = {We present a new method for data-based selection of the bandwidth in kernel density estimation which has excellent properties. It improves on a recent procedure of Park and Marron (which itself is a good method) in various ways. First, the new method has superior theoretical performance; second, it also has a computational advantage; third, the new method has reliably good performance for smooth densities in simulations, performance that is second to none in the existing literature. These methods are based on choosing the bandwidth to (approximately) minimize good quality estimates of the mean integrated squared error. The key to the success of the current procedure is the reintroduction of a non-stochastic term which was previously omitted together with use of the bandwidth to reduce bias in estimation without inflating variance.},
	number = {3},
	urldate = {2024-03-08},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Sheather, S. J. and Jones, M. C.},
	year = {1991},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {683--690},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/8MBB9P2D/Sheather und Jones - 1991 - A Reliable Data-Based Bandwidth Selection Method f.pdf:application/pdf},
}

@misc{wand_kernsmooth_2023,
	title = {{KernSmooth}: {Functions} for {Kernel} {Smoothing} {Supporting} {Wand} \& {Jones} (1995)},
	copyright = {Unlimited},
	shorttitle = {{KernSmooth}},
	url = {https://cran.r-project.org/web/packages/KernSmooth/index.html},
	abstract = {Functions for kernel smoothing (and density estimation) corresponding to the book: Wand, M.P. and Jones, M.C. (1995) "Kernel Smoothing".},
	urldate = {2024-03-09},
	author = {Wand, Matt and src/d*), Cleve Moler (LINPACK routines in and updates), Brian Ripley (R port and},
	month = jul,
	year = {2023},
}

@article{storn_differential_1997,
	title = {Differential {Evolution} – {A} {Simple} and {Efficient} {Heuristic} for global {Optimization} over {Continuous} {Spaces}},
	volume = {11},
	issn = {1573-2916},
	url = {https://doi.org/10.1023/A:1008202821328},
	doi = {10.1023/A:1008202821328},
	abstract = {A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
	language = {en},
	number = {4},
	urldate = {2024-03-25},
	journal = {Journal of Global Optimization},
	author = {Storn, Rainer and Price, Kenneth},
	month = dec,
	year = {1997},
	keywords = {evolution strategy, genetic algorithm, global optimization, nonlinear optimization, Stochastic optimization},
	pages = {341--359},
	file = {Full Text PDF:/Users/luca/Zotero/storage/PZJSZHPN/Storn und Price - 1997 - Differential Evolution – A Simple and Efficient He.pdf:application/pdf},
}

@article{mullen_deoptim_2011,
	title = {{DEoptim}: {An} {R} {Package} for {Global} {Optimization} by {Differential} {Evolution}},
	volume = {40},
	copyright = {Copyright (c) 2010 Katharine M. Mullen, David Ardia, David L. Gil, Donald Windover, James Cline},
	issn = {1548-7660},
	shorttitle = {{DEoptim}},
	url = {https://doi.org/10.18637/jss.v040.i06},
	doi = {10.18637/jss.v040.i06},
	abstract = {This article describes the R package DEoptim, which implements the differential evolution algorithm for global optimization of a real-valued function of a real-valued parameter vector. The implementation of differential evolution in DEoptim interfaces with C code for efficiency. The utility of the package is illustrated by case studies in fitting a Parratt model for X-ray reflectometry data and a Markov-switching generalized autoregressive conditional heteroskedasticity model for the returns of the Swiss Market Index.},
	language = {en},
	urldate = {2024-03-25},
	journal = {Journal of Statistical Software},
	author = {Mullen, Katharine M. and Ardia, David and Gil, David L. and Windover, Donald and Cline, James},
	month = apr,
	year = {2011},
	pages = {1--26},
	file = {Mullen et al. - 2011 - DEoptim An R Package for Global Optimization by D.pdf:/Users/luca/Zotero/storage/RSNU22TB/Mullen et al. - 2011 - DEoptim An R Package for Global Optimization by D.pdf:application/pdf},
}

@misc{edelbuettel_rcppde_2022,
	title = {{RcppDE}: {Global} {Optimization} by {Differential} {Evolution} in {C}++},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {{RcppDE}},
	url = {https://cran.r-project.org/web/packages/RcppDE/index.html},
	abstract = {An efficient C++ based implementation of the 'DEoptim' function which performs global optimization by differential evolution. Its creation was motivated by trying to see if the old approximation "easier, shorter, faster: pick any two" could in fact be extended to achieving all three goals while moving the code from plain old C to modern C++. The initial version did in fact do so, but a good part of the gain was due to an implicit code review which eliminated a few inefficiencies which have since been eliminated in 'DEoptim'.},
	urldate = {2024-03-25},
	author = {Edelbuettel, Dirk},
	month = dec,
	year = {2022},
	keywords = {Optimization},
}

@article{ferrari_beta_2004,
	title = {Beta {Regression} for {Modelling} {Rates} and {Proportions}},
	volume = {31},
	issn = {0266-4763},
	url = {https://doi.org/10.1080/0266476042000214501},
	doi = {10.1080/0266476042000214501},
	abstract = {This paper proposes a regression model where the response is beta distributed using a parameterization of the beta law that is indexed by mean and dispersion parameters. The proposed model is useful for situations where the variable of interest is continuous and restricted to the interval (0, 1) and is related to other variables through a regression structure. The regression parameters of the beta regression model are interpretable in terms of the mean of the response and, when the logit link is used, of an odds ratio, unlike the parameters of a linear regression that employs a transformed response. Estimation is performed by maximum likelihood. We provide closed-form expressions for the score function, for Fisher's information matrix and its inverse. Hypothesis testing is performed using approximations obtained from the asymptotic normality of the maximum likelihood estimator. Some diagnostic measures are introduced. Finally, practical applications that employ real data are presented and discussed.},
	number = {7},
	urldate = {2024-04-07},
	journal = {Journal of Applied Statistics},
	author = {Ferrari, Silvia and Cribari-Neto, Francisco},
	month = aug,
	year = {2004},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0266476042000214501},
	keywords = {Beta Distribution, Leverage, Maximum Likelihood Estimation, Proportions, Residuals},
	pages = {799--815},
}

@article{benjamini_controlling_1995,
	title = {Controlling the {False} {Discovery} {Rate}: {A} {Practical} and {Powerful} {Approach} to {Multiple} {Testing}},
	volume = {57},
	issn = {0035-9246},
	shorttitle = {Controlling the {False} {Discovery} {Rate}},
	url = {https://www.jstor.org/stable/2346101},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	number = {1},
	urldate = {2024-04-07},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	year = {1995},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {289--300},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/MMWCXJI8/Benjamini und Hochberg - 1995 - Controlling the False Discovery Rate A Practical .pdf:application/pdf},
}

@misc{arel-bundock_marginaleffects_2024,
	title = {marginaleffects: {Predictions}, {Comparisons}, {Slopes}, {Marginal} {Means}, and {Hypothesis} {Tests}},
	url = {https://marginaleffects.com/},
	author = {Arel-Bundock, Vincent},
	year = {2024},
}

@article{chambers_past_2022,
	title = {The past, present and future of {Registered} {Reports}},
	volume = {6},
	copyright = {2021 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01193-7},
	doi = {10.1038/s41562-021-01193-7},
	abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
	language = {en},
	number = {1},
	urldate = {2024-04-15},
	journal = {Nature Human Behaviour},
	author = {Chambers, Christopher D. and Tzavella, Loukia},
	month = jan,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Culture, Publishing},
	pages = {29--42},
	file = {Full Text PDF:/Users/luca/Zotero/storage/WWV7HBP5/Chambers und Tzavella - 2022 - The past, present and future of Registered Reports.pdf:application/pdf},
}

@article{nosek_registered_2014,
	title = {Registered {Reports}: {A} {Method} to {Increase} the {Credibility} of {Published} {Results}},
	volume = {45},
	issn = {1864-9335},
	url = {https://econtent.hogrefe.com/doi/full/10.1027/1864-9335/a000192},
	doi = {10.1027/1864-9335/a000192},
	number = {3},
	urldate = {2024-04-15},
	journal = {Social Psychology},
	author = {Nosek, Brian A. and Lakens, Daniël},
	month = may,
	year = {2014},
	note = {Publisher: Hogrefe Publishing},
	pages = {137--141},
	file = {Full Text PDF:/Users/luca/Zotero/storage/Y2R6QK3I/Nosek und Lakens - 2014 - Registered Reports.pdf:application/pdf},
}

@article{nosek_registered_2014-1,
	title = {Registered {Reports}},
	volume = {45},
	issn = {1864-9335},
	url = {https://doi.org/10.1027/1864-9335/a000192},
	doi = {10.1027/1864-9335/a000192},
	number = {3},
	urldate = {2024-04-14},
	journal = {Social Psychology},
	author = {Nosek, Brian A. and Lakens, Daniël},
	month = may,
	year = {2014},
	note = {Publisher: Hogrefe Publishing},
	pages = {137--141},
}

@book{feoktistov_differential_2006,
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Differential {Evolution} – {In} {Search} of {Solutions}},
	volume = {5},
	url = {https://doi.org/10.1007/978-0-387-36896-2},
	number = {5},
	publisher = {Springer},
	author = {Feoktistov, Vitaliy},
	month = jan,
	year = {2006},
	file = {(Springer Optimization and Its Applications) Vitaliy Feoktistov - Differential Evolution. In Search of Solutions-Springer (2010).pdf:/Users/luca/Zotero/storage/W7EUZ68U/(Springer Optimization and Its Applications) Vitaliy Feoktistov - Differential Evolution. In Search of Solutions-Springer (2010).pdf:application/pdf;Volltext:/Users/luca/Zotero/storage/4UBFD6CM/(Springer Optimization and Its Applications) Vitaliy Feoktistov - Differential Evolution. In Search of Solutions-Springer (2010).pdf:application/pdf},
}

@incollection{zielinski_stopping_2008,
	address = {Berlin, Heidelberg},
	title = {Stopping {Criteria} for {Differential} {Evolution} in {Constrained} {Single}-{Objective} {Optimization}},
	isbn = {978-3-540-68830-3},
	url = {https://doi.org/10.1007/978-3-540-68830-3_4},
	abstract = {Because real-world problems generally include computationally expensive objective and constraint functions, an optimization run should be terminated as soon as convergence to the optimum has been obtained. However, detection of this condition is not a trivial task. Because the global optimum is usually unknown, distance measures cannot be applied for this purpose. Stopping after a predefined number of function evaluations has not only the disadvantage that trial-and-error methods have to be applied for determining a suitable number of function evaluations, but the number of function evaluations at which convergence occurs may also be subject to large fluctuations due to the randomness involved in evolutionary algorithms. Therefore, stopping criteria should be applied which react adaptively to the state of the optimization run. In this work several stopping criteria are introduced that consider the improvement, movement or distribution of population members to derive a suitable time for terminating the Differential Evolution algorithm. Their application for other evolutionary algorithms is also discussed. Based on an extensive test set the criteria are evaluated using Differential Evolution, and it is shown that a distribution-based criterion considering objective space yields the best results concerning the convergence rate as well as the additional computational effort.},
	language = {en},
	urldate = {2024-04-20},
	booktitle = {Advances in {Differential} {Evolution}},
	publisher = {Springer},
	author = {Zielinski, Karin and Laur, Rainer},
	editor = {Chakraborty, Uday K.},
	year = {2008},
	doi = {10.1007/978-3-540-68830-3_4},
	pages = {111--138},
	file = {Zielinski und Laur - 2008 - Stopping Criteria for Differential Evolution in Co.pdf:/Users/luca/Zotero/storage/2RGCMIWW/Zielinski und Laur - 2008 - Stopping Criteria for Differential Evolution in Co.pdf:application/pdf},
}

@inproceedings{liu_termination_2018,
	address = {New York, NY, USA},
	series = {{GECCO} '18},
	title = {Termination detection strategies in evolutionary algorithms: a survey},
	isbn = {978-1-4503-5618-3},
	shorttitle = {Termination detection strategies in evolutionary algorithms},
	url = {https://doi.org/10.1145/3205455.3205466},
	doi = {10.1145/3205455.3205466},
	abstract = {This paper provides an overview of developments on termination conditions in evolutionary algorithms (EAs). It seeks to give a representative picture of the termination conditions in EAs over the past decades, segment the contributions of termination conditions into progress indicators and termination criteria. With respect to progress indicators, we consider a variety of indicators, in particular in convergence indicators and diversity indicators. With respect to termination criteria, this paper reviews recent research on threshold strategy, statistical inference, i.e., Kalman filters, as well as Fuzzy methods, and other methods. Key developments on termination conditions over decades include: (i) methods of judging the algorithm's search behavior based on statistics, and (ii) methods of detecting the termination based on different distance formulations.},
	urldate = {2024-04-20},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yanfeng and Zhou, Aimin and Zhang, Hu},
	month = jul,
	year = {2018},
	keywords = {evolutionary algorithms, progress indicators, termination conditions, termination criteria},
	pages = {1063--1070},
}

@inproceedings{zielinski_stopping_2005,
	title = {Stopping {Criteria} for {Single}-{Objective} {Optimization}},
	url = {https://www.semanticscholar.org/paper/Stopping-Criteria-for-Single-Objective-Optimization-Zielinski-Peters/29ad325712735f8c0c617f7430e2092630abac7e},
	abstract = {In most literature dealing with evolutionary algorithms the stopping criterion consists of reaching a certain number of objective function evaluations (or a number of generations, respectively). A disadvantage is that the number of function evaluations that is necessary for convergence is unknown a priori, so trialand-error methods have to be applied for finding a suitable number. By using other stopping criteria that include knowledge about the state of the optimization run this process can be avoided. In this work a promising new criterion is introduced and compared with criteria from literature. Examinations are realized using two relatively new algorithms, Differential Evolution and Particle Swarm Optimization. The study is performed on the basis of eight well-known single-objective unconstrained test functions. Depending on the applied stopping criterion considerable performance variations are observed. Recommendations concerning suitable stopping criteria for both algorithms are given.},
	urldate = {2024-04-20},
	author = {Zielinski, K. and Peters, D. and Laur, R.},
	year = {2005},
}

@incollection{zielinski_stopping_2008-1,
	address = {Berlin, Heidelberg},
	title = {Stopping {Criteria} for {Differential} {Evolution} in {Constrained} {Single}-{Objective} {Optimization}},
	isbn = {978-3-540-68830-3},
	url = {https://doi.org/10.1007/978-3-540-68830-3_4},
	abstract = {Because real-world problems generally include computationally expensive objective and constraint functions, an optimization run should be terminated as soon as convergence to the optimum has been obtained. However, detection of this condition is not a trivial task. Because the global optimum is usually unknown, distance measures cannot be applied for this purpose. Stopping after a predefined number of function evaluations has not only the disadvantage that trial-and-error methods have to be applied for determining a suitable number of function evaluations, but the number of function evaluations at which convergence occurs may also be subject to large fluctuations due to the randomness involved in evolutionary algorithms. Therefore, stopping criteria should be applied which react adaptively to the state of the optimization run. In this work several stopping criteria are introduced that consider the improvement, movement or distribution of population members to derive a suitable time for terminating the Differential Evolution algorithm. Their application for other evolutionary algorithms is also discussed. Based on an extensive test set the criteria are evaluated using Differential Evolution, and it is shown that a distribution-based criterion considering objective space yields the best results concerning the convergence rate as well as the additional computational effort.},
	language = {en},
	urldate = {2024-04-20},
	booktitle = {Advances in {Differential} {Evolution}},
	publisher = {Springer},
	author = {Zielinski, Karin and Laur, Rainer},
	editor = {Chakraborty, Uday K.},
	year = {2008},
	doi = {10.1007/978-3-540-68830-3_4},
	pages = {111--138},
	file = {Zielinski und Laur - 2008 - Stopping Criteria for Differential Evolution in Co.pdf:/Users/luca/Zotero/storage/2AVQRYP3/Zielinski und Laur - 2008 - Stopping Criteria for Differential Evolution in Co.pdf:application/pdf},
}

@inproceedings{liu_termination_2018-1,
	address = {New York, NY, USA},
	series = {{GECCO} '18},
	title = {Termination detection strategies in evolutionary algorithms: a survey},
	isbn = {978-1-4503-5618-3},
	shorttitle = {Termination detection strategies in evolutionary algorithms},
	url = {https://dl.acm.org/doi/10.1145/3205455.3205466},
	doi = {10.1145/3205455.3205466},
	abstract = {This paper provides an overview of developments on termination conditions in evolutionary algorithms (EAs). It seeks to give a representative picture of the termination conditions in EAs over the past decades, segment the contributions of termination conditions into progress indicators and termination criteria. With respect to progress indicators, we consider a variety of indicators, in particular in convergence indicators and diversity indicators. With respect to termination criteria, this paper reviews recent research on threshold strategy, statistical inference, i.e., Kalman filters, as well as Fuzzy methods, and other methods. Key developments on termination conditions over decades include: (i) methods of judging the algorithm's search behavior based on statistics, and (ii) methods of detecting the termination based on different distance formulations.},
	urldate = {2024-04-20},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yanfeng and Zhou, Aimin and Zhang, Hu},
	month = jul,
	year = {2018},
	keywords = {evolutionary algorithms, progress indicators, termination conditions, termination criteria},
	pages = {1063--1070},
	file = {Full Text PDF:/Users/luca/Zotero/storage/YKYY9M7N/Liu et al. - 2018 - Termination detection strategies in evolutionary a.pdf:application/pdf},
}

@inproceedings{ghoreishi_termination_2017,
	title = {Termination {Criteria} in {Evolutionary} {Algorithms}: {A} {Survey}},
	volume = {1},
	shorttitle = {Termination {Criteria} in {Evolutionary} {Algorithms}},
	doi = {10.5220/0006577903730384},
	abstract = {Over the last decades, evolutionary algorithms have been extensively used to solve multi-objective optimization problems. However, the number of required function evaluations is not determined by nature of these algorithms which is often seen as a drawback. Therefore, a robust and reliable termination criterion is needed to stop the algorithm. There is a huge amount of knowledge encapsulated in the studies targeting termination criteria in evolutionary algorithms, but an updated integrated overview of this knowledge is missing. For this reason, we aim to conduct a systematic research through a comprehensive literature study. We extended the basic categorization of termination criteria to a more advanced one that takes the most common used termination criteria into consideration based on their specifications and the way they have been evolved over time. The survey is concluded by suggesting a road-map for future research directions.},
	booktitle = {Proceedings of 9th {International} {Joint} {Conference} on {Computational} {Intelligence}},
	author = {Ghoreishi, Newsha and Clausen, Anders and Jørgensen, Bo Nørregaard},
	year = {2017},
	note = {Publisher: SCITEPRESS Digital Library},
	keywords = {Convergence, Evolutionary Algorithm, Evolutionary Computation, Performance Indicator, Progress Indicator, Stopping Criterion, Termination Criterion},
	pages = {373--384},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/8TNYRIWE/Ghoreishi et al. - 2017 - Termination Criteria in Evolutionary Algorithms A.pdf:application/pdf},
}

@inproceedings{jain_termination_2001,
	title = {On termination criteria of evolutionary algorithms},
	booktitle = {Proceedings of the 3rd {Annual} {Conference} on {Genetic} and {Evolutionary} {Computation}},
	author = {Jain, Brijnesh J. and Pohlheim, Hartmut and Wegener, Joachim},
	year = {2001},
	pages = {768--768},
	file = {Jain et al. - 2001 - On termination criteria of evolutionary algorithms.pdf:/Users/luca/Zotero/storage/XJ38H3UP/Jain et al. - 2001 - On termination criteria of evolutionary algorithms.pdf:application/pdf},
}

@article{nelder_simplex_1965,
	title = {A {Simplex} {Method} for {Function} {Minimization}},
	volume = {7},
	issn = {0010-4620},
	url = {https://doi.org/10.1093/comjnl/7.4.308},
	doi = {10.1093/comjnl/7.4.308},
	abstract = {A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.},
	number = {4},
	urldate = {2024-04-20},
	journal = {The Computer Journal},
	author = {Nelder, J. A. and Mead, R.},
	month = jan,
	year = {1965},
	pages = {308--313},
	file = {Nelder und Mead - 1965 - A Simplex Method for Function Minimization.pdf:/Users/luca/Zotero/storage/LDX5PDQ7/Nelder und Mead - 1965 - A Simplex Method for Function Minimization.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/3H9VYS9J/354237.html:text/html},
}

@article{lakens_improving_2020,
	title = {Improving {Inferences} {About} {Null} {Effects} {With} {Bayes} {Factors} and {Equivalence} {Tests}},
	volume = {75},
	issn = {1079-5014},
	url = {https://doi.org/10.1093/geronb/gby065},
	doi = {10.1093/geronb/gby065},
	abstract = {Researchers often conclude an effect is absent when a null-hypothesis significance test yields a nonsignificant p value. However, it is neither logically nor statistically correct to conclude an effect is absent when a hypothesis test is not significant. We present two methods to evaluate the presence or absence of effects: Equivalence testing (based on frequentist statistics) and Bayes factors (based on Bayesian statistics). In four examples from the gerontology literature, we illustrate different ways to specify alternative models that can be used to reject the presence of a meaningful or predicted effect in hypothesis tests. We provide detailed explanations of how to calculate, report, and interpret Bayes factors and equivalence tests. We also discuss how to design informative studies that can provide support for a null model or for the absence of a meaningful effect. The conceptual differences between Bayes factors and equivalence tests are discussed, and we also note when and why they might lead to similar or different inferences in practice. It is important that researchers are able to falsify predictions or can quantify the support for predicted null effects. Bayes factors and equivalence tests provide useful statistical tools to improve inferences about null effects.},
	number = {1},
	urldate = {2024-04-23},
	journal = {The Journals of Gerontology: Series B},
	author = {Lakens, Daniël and McLatchie, Neil and Isager, Peder M and Scheel, Anne M and Dienes, Zoltan},
	month = jan,
	year = {2020},
	pages = {45--57},
	file = {Full Text PDF:/Users/luca/Zotero/storage/FGS7RWR6/Lakens et al. - 2020 - Improving Inferences About Null Effects With Bayes.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/RD8RSM5X/5033832.html:text/html},
}

@article{delacre_why_2017,
	title = {Why psychologists should by default use {Welch}'s t-test instead of student's t-test},
	volume = {30},
	issn = {2397-8570},
	url = {http://www.scopus.com/inward/record.url?scp=85019141079&partnerID=8YFLogxK},
	doi = {10.5334/irsp.82},
	abstract = {When comparing two independent groups, psychology researchers commonly use Student's t-Tests. Assumptions of normality and homogeneity of variance underlie this test. More often than not, when these conditions are not met, Student's t-Test can be severely biased and lead to invalid statistical inferences. Moreover, we argue that the assumption of equal variances will seldom hold in psychological research, and choosing between Student's t-Test and Welch's t-Test based on the outcomes of a test of the equality of variances often fails to provide an appropriate answer. We show that the Welch's t-Test provides a better control of Type 1 error rates when the assumption of homogeneity of variance is not met, and it loses little robustness compared to Student's t-Test when the assumptions are met. We argue that Welch's t-Test should be used as a default strategy.},
	number = {1},
	urldate = {2024-04-24},
	journal = {International Review of Social Psychology},
	author = {Delacre, M. and Lakens, D. and Leys, C.},
	month = apr,
	year = {2017},
	keywords = {homogeneity of variance, Homogeneity of variance, Homoscedasticity, Levene's test, statistical power, Statistical power, Student's t-test, Student's t-Test, type 1 error, Type 1 error, type 2 error, Type 2 error, Welch's t-test, Welch's t-Test},
	pages = {92--101},
	file = {Volltext:/Users/luca/Zotero/storage/ZJM9U9MD/Delacre et al. - 2017 - Why psychologists should by default use Welch's t-.pdf:application/pdf},
}

@article{schuirmann_comparison_1987,
	title = {A comparison of the {Two} {One}-{Sided} {Tests} {Procedure} and the {Power} {Approach} for assessing the equivalence of average bioavailability},
	volume = {15},
	issn = {0090-466X},
	url = {https://doi.org/10.1007/BF01068419},
	doi = {10.1007/BF01068419},
	abstract = {The statistical test of the hypothesis of no difference between the average bioavailabilities of two drug formulations, usually supplemented by an assessment of what the power of the statistical test would have been if the true averages had been inequivalent, continues to be used in the statistical analysis of bioavailability/bioequivalence studies. In the present article, this Power Approach (which in practice usually consists of testing the hypothesis of no difference at level 0.05 and requiring an estimated power of 0.80) is compared to another statistical approach, the Two One-Sided Tests Procedure, which leads to the same conclusion as the approach proposed by Westlake (2) based on the usual (shortest) 1–2α confidence interval for the true average difference. It is found that for the specific choice of α=0.05 as the nominal level of the one-sided tests, the two one-sided tests procedure has uniformly superior properties to the power approach in most cases. The only cases where the power approach has superior properties when the true averages are equivalent correspond to cases where the chance of concluding equivalence with the power approach when the true averages are notequivalent exceeds 0.05. With appropriate choice of the nominal level of significance of the one-sided tests, the two one-sided tests procedure always has uniformly superior properties to the power approach. The two one-sided tests procedure is compared to the procedure proposed by Hauck and Anderson (1).},
	language = {en},
	number = {6},
	urldate = {2024-04-24},
	journal = {Journal of Pharmacokinetics and Biopharmaceutics},
	author = {Schuirmann, Donald J.},
	month = dec,
	year = {1987},
	keywords = {bioavailability, bioequivalence, hypothesis testing, interval hypotheses},
	pages = {657--680},
	file = {Full Text PDF:/Users/luca/Zotero/storage/GTN6I5NX/Schuirmann - 1987 - A comparison of the Two One-Sided Tests Procedure .pdf:application/pdf},
}

@misc{ushey_renv_2024,
	title = {renv: {Project} {Environments}},
	copyright = {MIT + file LICENSE},
	shorttitle = {renv},
	url = {https://cran.r-project.org/web/packages/renv/index.html},
	urldate = {2024-04-24},
	author = {Ushey, Kevin and Wickham, Hadley},
	month = apr,
	year = {2024},
	keywords = {ReproducibleResearch},
}

@misc{molder_sustainable_2021,
	title = {Sustainable data analysis with {Snakemake}},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	url = {https://f1000research.com/articles/10-33},
	doi = {10.12688/f1000research.29032.1},
	abstract = {Data analysis often entails a multitude of heterogeneous steps, from the application of various command line tools to the usage of scripting languages like R or Python for the generation of plots and tables. It is widely recognized that data analyses should ideally be conducted in a reproducible way.\&nbsp;Reproducibility enables technical validation and regeneration of results on the original or even new data. However, reproducibility alone is by no means sufficient to deliver an analysis that is of lasting impact (i.e., sustainable) for the field, or even just one research group. We postulate that it is equally important to ensure adaptability and transparency. The former describes the ability to modify the analysis to answer extended or slightly different research questions. The latter describes the ability to understand the analysis in order to judge whether it is not only technically, but methodologically valid. Here, we analyze the properties needed for a data analysis to become reproducible, adaptable, and transparent. We show how the popular workflow management system Snakemake can be used to guarantee this, and how it enables an ergonomic, combined, unified representation of all steps involved in data analysis, ranging from raw data processing, to quality control and fine-grained, interactive exploration and plotting of final results.},
	language = {en},
	urldate = {2024-04-24},
	publisher = {F1000Research},
	author = {Mölder, Felix and Jablonski, Kim Philipp and Letcher, Brice and Hall, Michael B. and Tomkins-Tinch, Christopher H. and Sochat, Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O. and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and Rahmann, Sven and Nahnsen, Sven and Köster, Johannes},
	month = jan,
	year = {2021},
	keywords = {adaptability, data analysis, reproducibility, scalability, sustainability, transparency, workflow management},
	file = {Full Text PDF:/Users/luca/Zotero/storage/E8KEE5ZH/Mölder et al. - 2021 - Sustainable data analysis with Snakemake.pdf:application/pdf},
}

@misc{allaire_quarto_2024,
	title = {Quarto},
	url = {https://doi.org/10.5281/zenodo.5960048},
	author = {Allaire, J. J. and Teague, Charles and Scheidegger, Carlos and Xie, Yihui and Dervieux, Christophe},
	year = {2024},
}

@misc{allaire_quarto_2024-1,
	title = {Quarto},
	url = {https://github.com/quarto-dev/quarto-cli},
	abstract = {Open-source scientific and technical publishing system built on Pandoc.},
	urldate = {2024-04-24},
	author = {Allaire, J.J. and Teague, Charles and Scheidegger, Carlos and Xie, Yihui and Dervieux, Christophe},
	month = feb,
	year = {2024},
	doi = {10.5281/zenodo.5960048},
}

@article{merkel_docker_2014,
	title = {Docker: lightweight {Linux} containers for consistent development and deployment},
	volume = {2014},
	issn = {1075-3583},
	shorttitle = {Docker},
	abstract = {Docker promises the ability to package applications and their dependencies into lightweight containers that move easily between different distros, start up quickly and are isolated from each other.},
	number = {239},
	journal = {Linux Journal},
	author = {Merkel, Dirk},
	month = mar,
	year = {2014},
	pages = {2:2},
}
