
<!-- General introduction-->

Science is commonly conceived as a cumulative endeavour with the overarching goal of establishing robust knowledge about the world upon which the future of scientific inquiry may be constructed [@curran_seemingly_2009]. As part of this endeavour, the idea of *meta-analyses* - quantitatively synthesising research examining the same phenomena - has been attributed a critical role in contributing to the cumulative advancement of knowledge [@schmidt_what_1992; @schmidt_statistical_1996]. However, this premise rests on the fundamental assumption that the research published in the scientific literature is representative of all conducted research [@song_dissemination_2010; @rothstein_publication_2005]. Yet, scholars have been stressing for over half a century that systematic differences exist between the results of published and unpublished studies. [@smart_importance_1964; @sterling_publication_1959; @bozarth_signifying_1972; @bakan_test_1966]. In practice, the publication of a study often hinges on the strength or direction of its findings [@dickersin_existence_1990; @dickersin_publication_1993], collectively known as *publication bias*. Especially in a publishing culture that incentivises novelty and positive results, alongside the predominant reliance on null hypothesis significance testing (NHST), it has become common practice for the nominal false positive rate to serve as a de facto criterion for publication [@nosek_scientific_2012]. As a consequence, many statistically nonsignificant studies end up in the "file drawer" and never get published [@rosenthal_file_1979].

## The Impact of Publication Bias

<!-- Negative consequences of publication bias -->

The ramifications of publication bias are severe, leading to an inflation of meta-analytic effect sizes [@stanley_detecting_2021; @franco_publication_2014] and an elevated false-positive rate [@kicinski_how_2014; @munafo_how_2010; @ioannidis_why_2005], thus increasing the risk of erroneous conclusions that may jeopardise the validity of the research [@begg_publication_1994]. Moreover, the prevalence of questionable research practices such as *p*-hacking [@john_measuring_2012] exacerbates the issue, as they interact with publication bias to further collectively distort meta-analytical effect sizes [@friese_p-hacking_2020]. These ramifications become especially relevant in light of recent large-scale replication projects providing evidence for non-replicability of many psychological findings [@open_science_collaboration_estimating_2015; @klein_many_2018; @camerer_evaluating_2018; @klein_investigating_2014; @ebersole_many_2016; @ebersole_many_2020]. This underscores why publication bias is identified as a major threat to replicable science [@munafo_manifesto_2017] and thus is considered a significant driver of the replication crisis in psychology [@renkewitz_how_2019]. The myriad issues associated with publication bias and its widespread impact have fuelled a great deal of research focusing on statistical methods to detect and address publication bias.

## Methods to Assess Publication Bias and their Limitations

<!-- Introduction to different classes of methodologies -->

This attention has led to the development of numerous statistical methods to detect and address publication bias over the past decades [@marks-anglin_historical_2020]. These statistical techniques can generally be classified into methods based on the relationship between effect size and sample size in meta-analyses and those working with *p*-values [@vevea_publication_2019].

<!-- Small-study effects -->

The former class of methods also coined small-study effects [@sterne_publication_2000], relies on the idea that, in the presence of publication bias, studies with smaller sample sizes (lower precision) necessitate larger effect sizes to attain statistical significance compared to studies with larger effect sizes (higher precision). Consequently, there are disproportionally fewer studies with low sample sizes and low effect sizes because they are statistically nonsignificant. This results in an asymmetrical funnel plot and a correlation between the effect size and a measure of precision (e.g., standard error or sample size), which are then analysed using regression methods. Such methods include, for example, PET-PEESE [@stanley_meta-regression_2014], Egger´s regression [@egger_bias_1997], Begg´s rank correlation [@begg_operating_1994] or, in its most simplistic form, the correlation between effect size and sample size [e.g., @kuhberger_publication_2014].

<!-- Publication bias selection models -->

The latter class of methods prominently features publication bias selection methods, including earlier selection models [e.g., @hedges_modeling_1992;  @hedges_estimation_1984; @iyengar_selection_1988] alongside more recent developments such as *p*-uniform  [@van_assen_meta-analysis_2015] and *p*-curve analysis [@simonsohn_p-curve_2014]. Publication bias selection models aim to directly characterise the selective publication process and consider the likelihood of the publication of a study as a function of *p*-values [@marks-anglin_historical_2020].

<!-- Limitations of these methods -->

Despite the abundance of statistical methods to assess publication bias, some justified criticisms have been discussed in the literature. Firstly, small study effects methods are commonly criticised for their lack of an explicit model for publication bias [@mcshane_adjusting_2016] and for their only "indirect" approach as they omit the true mechanism of publication bias by being driven on effect sizes rather than *p* values [@harrer_doing_2021]. Additionally, regarding these methods, it has been discussed that publication bias may is not the only source that influences the distribution of effect size and sample size, as researchers may plan sample sizes before performing the study according to anticipated effect sizes [@linden_publication_2024; @schafer_meaningfulness_2019], which may compromise the validity of the interpretation of such methods. More broadly, several comprehensive simulation studies have demonstrated that many existing methods have poor performance across a range of scenarios with realistic settings commonly encountered in empirical meta-analyses [@carter_correcting_2019; @van_aert_publication_2019; @renkewitz_how_2019; @mcshane_adjusting_2016]. Such factors of influence include the extent of effect size heterogeneity, the prevalence of additional p-hacking and other questionable research practices, a limited number of individual studies included in the meta-analysis, and the severity of publication bias. These factors may, in turn, lead to reduced statistical power, elevated false positive rates, convergence issues, and unsatisfactory agreement among different methods.

Considering these limitations, an explicit modelling framework to assess publication bias could, therefore, be valuable, with explicit assumptions within this framework that can be flexibly adapted for different scenarios. This framework should be, in principle, capable of modelling relevant factors previously discussed in the context of modelling publication bias, such as heterogeneity, sample size planning based on anticipated effect sizes, and the potential modelling of *p*-hacking.

## The Present Study

The present study introduces $\mathrm{SPEEC}$ ($\mathbfsfup{S}$imulation-based $\mathbfsfup{P}$ublication bias $\mathbfsfup{E}$stimation and $\mathbfsfup{E}$ffect size $\mathbfsfup{C}$orrection), a novel simulation-based framework to assess the extent of publication bias in meta-analyses and estimate corrected effect sizes in the presence of publication bias based on the joint distribution of effect size and sample size. The thesis sets out two primary objectives. Firstly, it aims to introduce the reader to the SPEEC method and comprehensively describe its assumptions and procedure. Secondly, it aims to assess the SPEEC method in a proof of concept using secondary empirical meta-analytical data sourced from @linden_heterogeneity_2021 to preliminary assess the feasibility of the introduced approach. For this purpose, four theoretically justifiable hypotheses involving predictions about the estimated parameters of the SPEEC method that should apply to the empirical data are derived in the section [Hypotheses](Hypotheses). The thesis is structured as follows: In this section, a brief primer on the central ideas of the SPEEC method is provided, followed by a detailed derivation of the hypotheses. Next, a thorough introduction to the SPEEC method itself is offered. This is followed by the empirical analyses of the hypotheses and a discussion and evaluation of the results of the empirical analyses.

## A Primer on SPEEC

The fundamental concept underlying the SPEEC approach involves explicitly modelling the generative process of publication bias in a simulation framework and iteratively comparing how the distribution of effect size and sample size of simulated studies diverges from the actual empirical meta-analytical data to estimate the model's parameters. The publication bias model of the simulation framework integrates both assumptions concerning the marginal distribution of effect size and sample size and how publication bias influences their joint distribution. In terms of the former, the sample sizes are modelled by a Negative-Binomial distribution (with parameters $\mu_n$ and $\phi_n$), and the effect sizes are modelled by a Gaussian distribution (with parameters $\mu_d$ and $\sigma^2_d$), where mean parameter $\mu_d$ represents the publication bias-corrected effect size estimate. Regarding the latter, the extent of publication bias is modelled by a publication bias parameter, $\pbs$,  which captures the probability of selecting a statistically nonsignificant simulated study relative to a significant one for publication. From this generative publication bias model, individual studies' effect sizes and sample sizes are simulated. Subsequently, the estimated kernel density distributions of the simulated data from the generative model are compared to those of the empirical meta-analytical data. This comparison serves to quantify the statistical divergence between the data generated by the model and the empirical data, functioning as a loss function. This framework can be conceptualised as an optimisation problem aiming to find values for the distributional parameters and the publication bias parameter of the generative publication bias model such that the statistical divergence from the empirical data is minimised. Stochastic optimisation algorithms can then be employed to estimate the parameters of the generative publication bias model. 


## Confirmatory Hypotheses
   
To assess the SPEEC method, a set of four theoretical predictions is derived, constituting this study's hypotheses. These hypotheses serve as benchmarks for assessing the viability of the proposed method and are, therefore, expected to hold true if the approach works in principle. If the predictions fail to be corroborated by the empirical meta-analytical data, this would raise concerns about the viability of the SPEEC method and necessitate a further review of its implementation.

<!-- Hypothesis I  -->

Firstly, regarding $\hypothesis{i}{}$, we conducted a direct comparison between the discussed correlation of effect size and sample size, serving as an alternative indicator of publication bias, and the publication bias parameter $\pbs$ estimated within the SPEEC method. It can be expected that the estimated publication bias parameter $\epbs$ is positively associated with the Fisher *z*-transformed Spearman correlation coefficients of the association between unsigned effect size and sample size in each meta-analysis. In other words, when the proposed method estimates high publication bias (i.e., low probabilities for $\epbs$), the correlation coefficients for each meta-analysis are expected to be more negative and conversely. In statistical terms, this implies that the regression coefficient $\beta_{z_{r_s}}$ is expected to be greater than zero.
$$
\begin{gathered}
\hypothesis{i}{0}: \ \beta_{z_{r_s}} \leq 0 \qquad \hypothesis{i}{1}: \ \beta_{z_{r_s}} > 0
\end{gathered}
$$ {#eq-h1}

<!-- Hypothesis II  -->

In cases where substantial publication bias is present within the scientific literature of a particular research phenomenon, and the true effect size is precisely zero ($\delta=0$), the distribution of effect size and sample size exhibits increased symmetric sparsity around zero in areas where individual studies would not be statistically significant for a given effect size and sample size [@light_summing_1984]. This is explained by the fact that only studies with either large positive or large negative effects will be statistically significant and consequently have a higher likelihood of being published in the presence of publication bias. Because of this symmetry for a true effect size of zero, the average effect size $\widehat{\delta}$ should not be biased since negative and positive effects should, in theory, mutually cancel each other out. Consequently, the difference $\Delta_{\mu_d}$ between the average effect size $\widehat{\delta}$ and the estimated mean parameter $\mu_d$ of the effect size distribution from the SPEEC approach should remain invariant independent of the magnitude of publication bias. However, when the true effect size exceeds zero ($\delta>0$), publication bias leads to an overestimation of the true effect (i.e. $\widehat{\delta}>\delta$), and conversely, overestimation in the opposite direction (i.e., $\widehat{\delta} < \delta$) when $\delta<0$. Suppose the estimated mean parameter $\widehat{\mu}_d$ of the Gaussian effect size distribution obtained from the *SPEEC* approach is a more accurate estimate of the true effect size $\delta$ in the presence of publication bias compared to the mean effect size $\widehat{\delta}$. In that case, it follows from the prior reasoning that a curvilinear, inverted U-shaped pattern can be expected between the difference $\delta_{\mu_d}$ of these two parameters and the publication bias parameter $\pbs$. In other words, when the mean difference $\Delta_{\mu_d}$ is approaching zero, publication bias severity is expected to decrease (indicated by larger values for $\pbs$). Conversely, when the difference $\Delta_{\mu_d}$ diverges from zero in both negative and positive directions, publication bias severity is expected to increase (i.e. lower values for $\pbs$). In statistical terms,  the quadratic regression term $\beta_{\Delta_{\mu_d}}$ is expected to be smaller than zero ($\hypothesis{i}{}$).

$$
\begin{gathered}
\hypothesis{i}{0}: \ \beta_{\Delta_{\mu_d}} \geq 0 \qquad \hypothesis{i}{1}: \ \beta_{\Delta_{\mu_d}} < 0
\end{gathered}
$$ {#eq-h2}

<!-- Hypothesis III -->

Registered reports are an alternative two-stage publishing model, where study protocols are submitted, peer-reviewed and in-principle accepted prior to data collection [@chambers_past_2022; @nosek_registered_2014]. In-principle accepted studies are then published regardless of the study's outcome because the decision to publish was made before the results of the studies were known, eliminating publication bias [@simons_introduction_2014; @chambers_past_2022]. Consequently, the effect sizes within these multisite replication projects and registered replication reports published within this framework cannot be biased by publication bias. Following this reasoning, for the third hypothesis $\hypothesis{iii}{}$, it can expected that the average effect size of a registered replication report $\widehat{\delta}$ to be equivalent to the mean parameter of the Gaussian effect size distribution $\mu_d$ ($\Delta_{\mu_d}=\widehat{\delta}-\widehat{\mu}_d$) that is estimated using the SPEEC approach within specified equivalence bounds $\Delta_{EQ} = \{\Delta_L, ~ \Delta_U\}$. The equivalence bounds are defined by the smallest effect size of interest for this study [@lakens_equivalence_2018] and are set to $\Delta_{EQ}=\{-0.18, 0.18\}$ (see section [Smallest Effect Size of Interest] for the rationale of this decision).

$$
\begin{gathered}
\hypothesis{iii}{01}: \ \Delta_{\mu_d} \leq \Delta_{L} \quad \cap \quad \hypothesis{iii}{02}: \ \Delta_{\mu_d} \geq \Delta_{U} \\
\hypothesis{iii}{1}: \ \Delta_L >\Delta_{\mu_d} > \Delta_U
\end{gathered}
$$ {#eq-h3}

<!-- Hypothesis IV -->

In theory, one could posit that the publication bias parameter $\pbs$ in registered replication reports should precisely equal one, as individual replication studies are predetermined in the registration of the study and included in the final report independent of their statistical outcomes [@nosek_registered_2014]. However, due to the inherent upper limit of one for the publication bias parameter $\pbs$, testing this point prediction would not be sensible within a null hypothesis testing framework. Instead, a relative comparison between traditional meta-analyses and registered replication reports allows for making testable predictions. More specifically, it can be expected that the publication bias parameter $\pbs$ is greater for registered replication reports than for traditional meta-analyses. This indicates that relative to individual statistically nonsignificant primary studies within traditional meta-analyses, statistically nonsignificant replication studies should have a greater likelihood of being included in the final registered registered replication report. In statistical terms, when the regressor is a binary indicator of the type of research synthesis with the reference level being registered replication reports ($RRR$) and the outcome is the estimated publication bias parameter $\pbs$, the regression coefficient $\beta_{RRR}$ can be expected to be greater than zero ($\hypothesis{iii}{}$).

$$
\begin{gathered}
\hypothesis{iv}{0}: \ \beta_{MR} \leq 0 \qquad \hypothesis{iv}{1}: \ \beta_{MR} > 0
\end{gathered}
$$ {#eq-h4}