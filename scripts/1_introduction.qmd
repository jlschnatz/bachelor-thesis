---
bibliography: ../bibliography/tidy_references.bib
---

Science is commonly conceived as a cumulative endeavour with the overarching goal of establishing robust knowledge about the world upon which the future of scientific inquiry may be constructed [@curran_seemingly_2009]. As part of this endeavor, the idea of *meta-analyses* - quantitavely synthesizing research findings examining the same phenomena - has been attributed a particularly important role in contributing to the cumulative advancement of knowledge [@schmidt_what_1992; @schmidt_statistical_1996]. However, this premise rests on the fundamental assumption, that the published research in the scientific literature is representative for all conducted research [@song_dissemination_2010; @rothstein_publication_2005]. Yet, scientists have been pointing out for over half a century that results of published studies differ systematically from those of unpublished studies [@smart_importance_1964; @sterling_publication_1959; @bozarth_signifying_1972; @bakan_test_1966]. In practice, the publication of a study often hinges on the strength or direction of its findings [@dickersin_existence_1990; @dickersin_publication_1993] and has be collectively known as *publication bias*. Especially in a publishing culture that incentivizes novelty and positive results, alongside the prevalent use of the null hypothesis significance testing (NHST) framework, it has become common practice for the nominal false positive rate to serve as a de facto criterion for publication [@nosek_scientific_2012]. As a consequence, many statistically non-significant studies end up in the "file-drawer" and never get published [@rosenthal_file_1979].

## The Impact of Publication Bias

The ramifications of publication bias are severe, culminating in an inflation of meta-analytical effect sizes [@stanley_detecting_2021; @franco_publication_2014], heightened false-positive rate [@kicinski_how_2014; @munafo_how_2010; @ioannidis_why_2005] , thus increasing the risk of errorneous conclusions that may jeopardize the validity of research [@begg_publication_1994]. Moreover, the prevalence of questionable research practices such as p-hacking [@john_measuring_2012] exacerbates the issue, as they interact with publication bias to further distort meta-analytical effect sizes [@friese_p-hacking_2020]. These ramifications become especially relevant in light of recent large-scale replication projects providing evidence for non-replicability of many psychological findings [@open_science_collaboration_estimating_2015; @klein_many_2018; @camerer_evaluating_2018; @klein_investigating_2014; @ebersole_many_2016; @ebersole_many_2020]. This underscores why publication bias identified as a major threat to replicable science [@munafo_manifesto_2017] and thus considered as a significant contributor to the replication crisis [@renkewitz_how_2019]. The myriad issues associated with publication bias and its widespread impact have fuelled a great deal of research focusing on statistical methods to detect and address publication bias.

## Methods to Assess Publication Bias and their Limitations

<!-- Introduction to different classes of methodologies -->

To this regard, this attention has led to the development of numerous statistical methods to detect and address publication bias over the past decades [@marks-anglin_historical_2020]. These statistical techniques can generally be classified into methods that are based on the relationship between effect size and sample size in meta-analyses and those that operate with *p*-values [@vevea_publication_2019].

<!-- Small-study effects -->

The former class of methods, also coined small-study effects [@sterne_publication_2000], rely on the idea that, in the presence of publication bias, studies with smaller sample sizes (lower precision) necessitate larger effect sizes to attain statistical significance compared to studies with larger effect sizes (higher precision). Consequently, there are disproportionally less studies with low sample sizes and low effect sizes, because they are statistically nonsignificant. This results in an assymmetrical funnel plot and a correlation between the effect size and a measure of precision (e.g., standard error or sample size), that are then analyzed using regression methods. Such methods include for example PET-PEESE [@stanley_meta-regression_2014], Egger´s regression [@egger_bias_1997], Begg´s rank correlation [@begg_operating_1994] or in its most simplistic form the effect size sample size correlation [e.g., @kuhberger_publication_2014].

<!-- Publication bias selection models -->

The latter class of methods prominently features publication bias selection methods, including earlier selection models [e.g., @hedges_modeling_1992;  @hedges_estimation_1984; @iyengar_selection_1988] alongside more recent developments such as *p*-uniform  [@van_assen_meta-analysis_2015] and *p*-curve analysis [@simonsohn_p-curve_2014]. Publication bias selection models aim to directly characterize the selective publication process and consider the likelihood of the publication of a study as a function of *p*-values [@marks-anglin_historical_2020].

<!-- Limitations of these methods -->

- critic small-study effects: kein explizites publiaktion bias modell (@mcshane_adjusting_2016), und sample size adjustment gedöhns, p-hacking gedöhns



However, it has been also discussed that funnel plot assymetry captured by these methods, can not only be explained by publication bias but also other factors,

important factor for me: other factors influenceing the distribution of effect size and sample size -> sample size planning



performance of derzeitigen methoden sehr stark abhängig von heterogenität, ausmaß von publication bias -> so mäßig gut und übereinstimmung zwischen verschiedenen methoden auch nicht gut, power of tests low, also false positive rate of some tests pretty bad under certain conditions [@carter_correcting_2019; @van_aert_publication_2019; @renkewitz_how_2019]

extensive simulation studies have shown that:

- @renkewitz_how_2019: 
  - generally, no method outperforms the other in all conditions
  - effect size heterogeneity seems to be big problem
  - inflated false positive rate for some methods (TIVA) 
  - many conditions where publication bias detection fails
  - consistently low power when publication bias is moderate
  
- @mcshane_adjusting_2016:
  - Evaluation of selection models

- @carter_correcting_2019:
    - trim-and-fill and WAAP-WLS high FPR and overstimation
    - p-curve and p-uniform performed bad under heterogenetiy (increased fpr and oversetimation)
    - PET-PEESE and 3PSM reasonable FPR but reduced power when small sample sizes and high heterogeneity, high use of QROS or little publication bias
    - convergence issues or couldn´t be applied due to lack of significant studies in 3PSM and p-uniform, p-curve 

- @van_aert_publication_2019:
  - also simulation part in a big meta-meta-analysis




more flexibility in modeling of publication bias necessary, -> sample size planning, heterogeneity, p-hacking -> flexible framework verbose framework would be neat for modeling this stuff...

---

Thema Flexibilität:
- SPEEC Methode unterscheidet sich drastisch durch Simulationsbasierte Schätzmethodik große Flexibilität
- Important point  framework within which specific assumptions can be changed regarding marginal distributions e.g.
- Explicit statement of assumption of the publication bias model in the generative publication bias model

- FLexible framework (maybe also include p-hacking with the paper from Moss and collogues)

---

- Overwhelming amount of methods  but, problems: 
    - Heterogeneity
    - Disagreement between methods
- Often the assumption of homogeneity of effect sizes for publication bias detection methods
  
Research questions:

- How does publication bias influence the joint probability distribution of effect size and sample size? 
- How can the magnitude of publication bias in meta-analyses be estimated and effect sizes under publication bias be corrected from the joint distribution of sample size and effect size?

## The Present Study

The present study introduces $\mathrm{SPEEC}$ ($\mathbfsfup{S}$imulation-based $\mathbfsfup{P}$ublication bias $\mathbfsfup{E}$stimation and $\mathbfsfup{E}$ffect size $\mathbfsfup{C}$orrection), a novel simulation-based framework to assess the extent of publication bias in meta-analyses and estimate corrected effect sizes in the presence of publication bias based on the joint distribution of effect size and sample size. The thesis sets out with two primary objectives. Firstly, it aims to introduce the reader to the SPEEC method and provide a comphrensive description of its assumptions and its procedure. Secondly, it aims to assess the SPEEC method in a proof of concept using secondary empirical meta-analytical data sourced from @linden_heterogeneity_2021 to preliminary assess the initial feasability of the introduced approach. For this purpose, four theoretically justifiable hypotheses involving predictions about the estimated parameters of the SPEEC method that should apply to the empirical data are derived in the section [Hypotheses](Hypotheses). The thesis is structured as follows: In this section, a brief primer on the central ideas of the SPEEC method is provided, followed by a detailed derivation of the hypotheses. Next, a detailed introduction to the SPEEC method itself is offered. This is followed by the empirical analyses of the hypotheses and a discussion and evaluation of the results of the empirical analyses.

## A Primer on SPEEC

The fundamental concept underlying the SPEEC approach involves explicitely modeling the generative process of publication bias in a simulation framwork and iteratively comparing how the distribution of effect size and sample size of simulated studies diverges from the actual empircal meta-analytical data to estimate the parameters of the model. The publication bias model of the simulation framework integrates both asumption concerning the marginal distribution of effect size and sample size, as well as how publication bias influences their joint distribution. In terms of the former, the sample sizes are modeled by a Negative-Binomial distribution (with parameters $\mu_n$ and $\phi_n$) and the effect sizes are modeled by a Gaussian distribution (with parameters $\mu_d$ and $\sigma^2_d$), where mean parameter $\mu_d$ represents the publication bias corrected effect size estimate. Regarding the latter, the extent of publication bias is modeled by a publication bias parameter, $\pbs$,  which captures the probability of selecting a statistically non-significant simulated study relative to a significant one for publication. From this generative publication bias model, effect sizes and sample sizes of individual studies are simulated. Subsequently, the estimated kernel density distributions of the simulated data from the generative model are compared to those of the empirical meta-analytical data. This comparison serves to quantify the statistical divergence between the data generated by the model and the empirical data, functioning as a loss function. This framework can be conceptualized as an optimization problem aiming to find values for the distributional parameters and the publication bias parameter of the generative publication bias model such that the statistical divergences from the empirical data is minimized. Stochastic optimization algorithms can then be employed to estimate the parameters of the generative publication bias model.For this study, the method of choice is differential evolution.

## Confirmatory Hypotheses
   
To assess the SPEEC method, a set of four theoretical predictions are derived, that constitute the hypotheses of this study. These hypotheses serve as benchmarks for assessing the viability of the proposed method and are therefore expected to hold true if the approach works in principle. If the predictions fail to be corroborated by the empirical meta-analytical data, this would raise concerns about the viability of the SPEEC method and necessitate a further review of its implementation.

<!-- Hypothesis I (still open) -->

Firstly, we conducted a direct comparison between the correlation of effect size and sample size, serving as an indicator of publication bias, and the publication bias parameter $\pbs$ estimated within the SPEEC method. It can be expected that the estimated publication bias parameter $\epbs$ is positively associated with the Fisher *z*-transformed Spearman correlation coefficients of the association between unsigned effect size and sample size in each meta-analysis. In other words, when the proposed method estimates high publication bias (i.e., low probabilities for $\epbs$ ) it is expected that the correlation coefficients for each meta-analysis to be more negative and conversely. In statistical terms, this implies that the regression coefficient $\beta_{z_{r_s}}$ is expected to be greater than zero.

$$
\begin{gathered}
\hypothesis{i}{0}: \beta_{z_{r_s}} \leq 0 \\
\hypothesis{i}{1}: \beta_{z_{r_s}} > 0
\end{gathered}
$$ {#eq-h1}

<!-- Hypothesis II (finished!) -->

In cases where substantial publication bias is present within the scientific literature of a particular research phenomenon, and the true effect size is precisely zero ($\delta=0$), the distribution of effect size and sample size exhibits increased symmetric sparsity around zero in areas where individual studies would not be statistically significant for a given effect size and sample size [@light_summing_1984]. This is explained by the fact that only studies with either large positive or large negative effects will be statistically significant and consequently have a higher likelihood of being published in the presence of publication bias. Because of this symmetry for a true effect size of zero, the average effect size $\widehat{\delta}$ should not be biased since negative and positive effects should, in theory, mutually cancel each other out. Consequently, the difference $\Delta_{\mu_d}$ between the average effect size $\widehat{\delta}$ and the estimated mean parameter $\mu_d$ of the effect size distribution from the SPEEC approach should remain invariant independent of the magnitude of publication bias. However, when the true effect size exceeds zero ($\delta>0$), publication bias leads to an overestimation of the true effect (i.e. $\widehat{\delta}>\delta$), and conversely, overestimation in the opposite direction (i.e., $\widehat{\delta} < \delta$) when $\delta<0$. If the estimated mean parameter $\widehat{\mu}_d$ of the Gaussian effect size distribution obtained from the *SPEEC* approach is a more accurate estimate of the true effect size $\delta$ in the presence of publication bias compared to the mean effect size $\widehat{\delta}$, it follows from the prior reasoning that a curvilinear, inverted U-shaped pattern can be expected between the difference $\delta_{\mu_d}$ of these two parameters and the publication bias parameter $\pbs$. In other words, when the mean difference $\Delta_{\mu_d}$ is approaching zero, publication bias severity is expected to decrease (indicated by larger values for $\pbs$). Conversely, when the difference $\Delta_{\mu_d}$ diverges from zero in both negative and positive directions, publication bias severity is expected to increase (i.e. lower values for $\pbs$). In statistical terms, for the second hypothesis of this study $\hypothesis{i}{}$, the quadratic regression term $\beta_{\Delta_{\mu_d}}$ is expected to be smaller than zero.

$$
\begin{gathered}
\hypothesis{i}{0}: \quad \beta_{\Delta_{\mu_d}} \geq 0 \\ 
\hypothesis{i}{1}: \quad \beta_{\Delta_{\mu_d}} < 0
\end{gathered}
$$ {#eq-h2}

<!-- Hypothesis III -->

Registered reports are an alternative two-stage publishing model, where study protocols are submitted, peer-reviewed and in-princple accepted prior to data collection [@chambers_past_2022; @nosek_registered_2014]. In-princple accepted studies are then published regardless of the outcome of the study, because the decision to publish was made before the results of the studies are known, which eliminates publication bias [@simons_introduction_2014; @chambers_past_2022]. Consequently, the effect sizes within these multisite replication projects and registered replication reports that are publishied within this framework cannot be biased by publication bias. Following this reasoning, in the third hypothesis of the present study $\hypothesis{iii}{}$, it can expected that the average effect size of a registered replication report $\widehat{\delta}$ to be equivalent to the mean parameter of the Gaussian effect size distribution $\mu_d$ that is estimated using the SPEEC approach within specified equivalence bounds $\Delta_{EQ} = \{\Delta_L, ~ \Delta_U\}$. The equivalence bounds are defined by the smallest effect size of interest for this study [@lakens_equivalence_2018] and are set to $\Delta_{EQ}=\{-0.18, 0.18\}$ (see section [Smallest Effect Size of Interest] for the rationale of this decision).

$$
\begin{gathered}
\Delta_{\mu_d}=\widehat{\delta}-\widehat{\mu}_d \\
\hypothesis{iii}{01}: \Delta_{\mu_d} \leq \Delta_{L} \quad \cap \quad \hypothesis{iii}{02}: \Delta_{\mu_d} \geq \Delta_{U} \\
\hypothesis{iii}{1}: \Delta_L >\Delta_{\mu_d} > \Delta_U
\end{gathered}
$$ {#eq-h3}

<!-- Hypothesis IV -->

In theory, one could posit that the publication bias parameter $\pbs$ in registered replication reports should precicely equal one, as individual replication studies are predetermined in the registration of the study and included in the final report independent of their statistical outcomes. However, due to the inherent upper limits of one for the publication bias parameter $\pbs$, testing this point prediction would not be sensible within a null hypothesis testing framework. Instead, a relative comparison between traditional meta-analyses and registered replication reports allows for making testable predictions. More specifically, it can be expected that the publication bias parameter $\pbs$ is greater for multisite replication studies in comparison to traditional meta-analysis. This indicates that relative to individual statistically non-significant primary studies within traditional meta-analyses, statistically non-significant individual replication studies should have a greater likelihood of being of being included in the the final registered replication report (and thus being published). In statistical terms, when the regressor is a binary indicator of the type of research synthesis with the reference level being the publication biased absent multisite replication studies ($MR$) and the outcome is the estimated publication bias parameter $\pbs$, the regression coefficient $\beta_{MR}$ can be expected to be greater than zero.

$$
\begin{gathered}
\hypothesis{iv}{0}: \quad \beta_{MR} \leq 0 \\
\hypothesis{iv}{1}: \quad \beta_{MR} > 0
\end{gathered}
$$ {#eq-h4}