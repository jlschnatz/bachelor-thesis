---
bibliography: ../bibliography/tidy_references.bib
---

# General Discussion

The objective of the present thesis was to introduce a flexible simulation-based framework, to assess the extent of publication bias and estimate corrected effect sizes based on the joint distribution of effect size and sample size in the presence of publication bias (SPEEC). In a proof-of-concept study, the viability of the SPEEC method was evaluated through four theoretically justified hypotheses, which were should be corroborated by empirical data encompassing both classical meta-analyses and publication bias-free registered replication reports (RRRs) if the method works in principle. The confirmatory analyses revealed that, while three out of the four predictions were statistically significant, the effect sizes for these hypotheses were weak. Additionally, no evidence supported the fourth hypothesis pointing to potential issues with the parameter estimation in the SPEEC method.

To further diagnose these potential parameter estimation challenges, five exploratory diagnostic questions were derived, using the subset of publication bias-free RRRs to compare the distributional parameters estimated using SPEEC to those estimated using Maximum Likelihood Estimation (MLE). Additionally, it was examined how these divergences between SPEEC and MLE are associated with the publication bias parameter $\pbs$ and uncertainty-related factors such as the meta-analytical sample size $k$ and between-study heterogeneity $\tau$. Overall, this analysis aimed to quantify the extent and consistency of misestimations in SPEEC compared to MLE (questions 1-3) and to identify potential root causes of the parameter estimation issues (questions 4-5).

## Extent and Consistency of Misestimation of SPEEC Parameters

Regarding the first three exploratory diagnostic questions, it was found that, except for the mean $\mu_d$ of the effect size distribution, there were *systematic* divergences between SPEEC and MLE in the parameter estimation. This misestimation of SPEEC from MLE was not constant across the parameter space of the MLE estimates, displaying nonlinear functional forms and substantial heteroscedasticity. 

The correlational analyses indicated a consistency in the misestimation of SPEEC compared to MLE for the mean $|\Delta_{\widehat{\mu}_d}|$ and variance parameter $|\Delta_{\widehat{\sigma}^2_d}|$ of the effect size distribution. In other words, an increase in the absolute divergence between SPEEC and MLE in one parameter was associated with an increase in the absolute divergence in the other parameter. The absence of any substantial (and significant) correlations between the divergences of SPEEC and MLE for effect size distribution parameters ($|\Delta_{\widehat{\mu}_d}|$, $|\Delta_{\widehat{\sigma}^2_d}|$) and sample size distribution parameters ($|\Delta_{\widehat{\phi}_n}|$, $|\Delta_{\widehat{\mu}_n}|$) suggests there is no evidence of a consistency between the misestimation of of SPEEC from MLE of effect size parameters and sample size parameters. However, there was a strong and statistically significant negative correlation between the parameters of the effect size distribution ($|\Delta_{\widehat{\mu}_d}|$, $|\Delta_{\widehat{\sigma}^2_d}|$) and the publication bias parameter $\epbs$ such that larger divergences between MLE and SPEEC of the distributional parameters were associated with lower predicted values of the publication bias parameter. Because, as argued previously, $\epbs$ should in theory equal exactly 1, this negative correlation can be interpreted as a consistency in the misestimation of the effect size parameter of SPEEC from MLE and the misestimation of the publication bias parameter $\epbs$.

## Which Factors Drive the Parameter Misestimation in SPEEC?

The systematic discrepancy between MLE and SPEEC for the distribution parameters, together with the consistency in the misestimation of the effect size parameters and the publication bias parameter, begs the question of what factors are responsible for the parameter misestimation of the SPEEC method.

In this regard, no evidence was found that the sample size $k$ of the RRRs was associated with the misestimation of the distributional parameters of SPEEC from MLE. However, strong positive correlations were observed between the misestimation of SPEEC from MLE in the effect size distribution parameters ($|\Delta_{\widehat{\mu}d}|$, $|\Delta_{\widehat{\sigma}^2_d}|$) and between-study heterogeneity $\widehat{\tau}$. Additionally, a medium negative correlation was observed between the publication bias parameter $\epbs$ and between-study heterogeneity $\widehat{\tau}$. This indicates that increasing observed heterogenity $\widehat{\tau}$ is associated with larger misestimation of distributional parameter of SPEEC from MLE and larger misestimation of the publication bias parameter $\epbs$. Overall, these results highlight the potential significance of between-study heterogeneity in influencing parameter misestimation.

This is plausible from the perspective that the simulation framework of SPEEC currently assumes a fixed-effect meta-analytical model, where the only source of effect size variation is attributable to sampling error. Both the classical meta-analyses and the RRRs of the empirical data used for the proof of concept displayed moderate to large levels of between-study heterogeneity [@linden_heterogeneity_2021]. Thus, the the empirical data displayed an additional level of effect size variation that could not be adequately modeled in the SPEEC method due to sampling error alone. Especially due the definition of the loss function $D_{\text{KL}}$ as a statistical distance between kernel densities from empirical and simulated data, parameters may be erroneously adjusted in the presence of heterogeneity to minimize the loss function. 

This could be one potential explanation for the associated predicted decrease in $\epbs$ for high heterogeneity $\widehat{\tau}$, as more severe publication bias implies that the nonsignificant studies, that are likely to have effect sizes close to zero, are censored, thus artificially increasing the overall variance to account for heterogeneity in the minimzation of $D_{\text{KL}}$. Furthermore, the same line of reasoning could also account for the positive relationship between $| \Delta_{\widehat{\sigma}^2_d} |$ and $\sigma^2_d$, as the overestimation of $\sigma^2_d$ by SPEEC compared to MLE increases the overall variability of the effect size distribution to seamingly capture heterogeneity.


In summary, the challenges encountered by the current version of the SPEEC method, which are likely attributable to between-study heterogeneity, are parallel to those faced by other statistical techniques assuming a fixed-effects meta-analytical model, such as *p*-uniform and *p*-curve analysis [@simonsohn_p-curve_2014; @van_assen_meta-analysis_2015]. These methods tend to exhibit poor performance in the presence of substantial heterogeneity, resulting in an overestimation of the true effect size under such conditions [@mcshane_adjusting_2016; @carter_correcting_2019; @van_aert_conducting_2016]. This aligns with the positive correlation observed between $|\Delta_{\widehat{\mu}_d}|$ and $\widehat{\tau}$, suggesting that an increase heterogeneity is associated with a greater misestimation of SPEEC from MLE for the mean parameter $\mu_d$ of the effect size distribution.

## Limitations

Besides the previously discussed challenges in the parameter estimation of SPEEC, which are likely to result from heterogeneity, there are additional limitations of the present thesis that should be considered. These may be subdivided into objections against the SPEEC method itself and constraints of the study design.

A first major limitation of the SPEEC method itself, is the is the lack of quantification of uncertainty in the estimation of parameters in SPEEC. This lack of quantification of parameter uncertainty undermines the ability to accurately assess the confidence of these estimates [@lele_how_2020]. As an illustration, one would have varying degrees of confidence in the estimated extent of publication bias $\epbs$ and the corrected meta-analytical effect size $\mu_d$ for a small-scale meta-analysis with a sample size of $k=20$ in comparison to a large-scale meta-analysis with a sample size of $k=500$. This inability  can be considered an infeririority to other other common methods to assess publication bias that yield confidence estimates.

Secondly, while the existence of an explicit generative publication bias model in SPEEC may be seen as an advancement compared to other methods lacking such assumptions entirely, the model's assumptions remain oversimplistic. Real-world scenarios involve more intricate data models and selection mechanisms. Studies often encompass multiple dependent effects of interest, with selection likely based on properties of these effects taken jointly [@mcshane_adjusting_2016]. In addition, the presence of questionable research practices in existing scientific research can further complicate the data and selection models due to its interaction with publication bias [@friese_p-hacking_2020].

The constraints regarding the study design relate to the conception of the study as proof of concept. Although the investigation of the theoretically-derived hypotheses in the confirmatory analyses yielded important insights into potential parameter estimation challenges, and diagnostic exploratory findings underscored the potential significance of heterogeneity in explaining these challenges, the assessment was limited in its capacity to offer a holistic understanding of SPEEC's viability across various conditions. For this, large-scale simulation studies are necessary, because the parameter of interests are known and manipulable [@morris_using_2019].

## Directions for Future Research

Taking into account the results of both the confirmatory and the exploratory diagnostic analyses, there is a call for further investigations in at least two directions. This encompasses the further development of the SPEEC method and a comprehensive assessment of SPEEC within a simulation setting.

Regarding the former, considering the likely contribution of heterogeneity to the parameter misestimation in the investigated empirical data, as well as its common prevalence across meta-analyses [@linden_heterogeneity_2021; @van_erp_estimates_2017; @higgins_commentary_2008], highlights the importance of incorporating a heterogeneity parameter in the simulation framework of SPEEC. Failure to account for heterogeneity could otherwise potentially jeopardize the validity of SPEEC in settings where heterogeneity is present. Additionally, as briefly touched upon in the introduction, there exist other factors that influence the joint distribution of effect size and sample size, including *sample size planning*, where researchers plan sample sizes based on predicated effect sizes [@linden_publication_2024], and questionable research practices such as *p*-hacking, which can interact with publication bias [@friese_p-hacking_2020]. Incorporating these factors into the simulation framework in future research could lead to a more accurate modeling of publication bias.

A comprehensive assessment of SPEEC within a simulation setting should focus on the evaluating the accuracy of parameter estimation of SPEEC under various conditions that are commonly encountered in real-world meta-analytical data and that have been considered in previous simulation studies that assessed other publication bias detection methods. Key factors to consider include the extent of publication bias ($\pbs$), true effect size ($\mu_d$), amount of heterogeneity ($\tau$),  the number of primary studies $k$ within the meta-analysis and questionable research practices such as *p*-hacking.


## Conclusion


