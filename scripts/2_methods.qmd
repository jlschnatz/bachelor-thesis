---
bibliography: ../bibliography/tidy_references.bib
---

```{r}
#| echo: false
pacman::p_load(tidyverse, insight, here, knitr, kableExtra, glue)
source(here("R/functions.R"))
```

# Method

## Comprehensive Methodological Description of SPEEC

This section offers a comprehensive description and explanation of the SPEEC framework, which assesses the extent of publication bias and estimates effect sizes in the presence of publication bias in meta-analyses. The flowchart in $\autoref{fig:flowchart}$ provides an overview of the sequential steps of the SPEEC method. SPEEC is being developed as an open-source R package and is accessible on GitHub at https://github.com/jlschnatz/speec (alpha version).


```{=latex}
\begin{figure}[h]
\caption{Flowchart of the SPEEC Method}
\label{fig:flowchart}
```
```{r speec_workflow}
#| out-width: 80%
#| fig-align: center
knitr::include_graphics(here("figures/speec_workflow.png"))
```

```{=latex}
\begingroup
\scriptsize
\vspace{-5pt}
\setstretch{0.7}
\noindent
\textit{Note.} Flowchart illustrating the steps of the SPEEC method. The simulation framework of the publication bias model is depicted in the left box. Inputs are the publication bias parameter $\pbs$ and distributional parameters of the effect size and sample size distribution ($\mu_d$, $\sigma^2_d$, $\mu_n$, $\phi_n$). The output serves as the loss function for optimisation and is the Kullback-Leibler divergence $D_{\text{KL}}(\widehat{f}_e ~ || ~ \widehat{f}_t)$ between the estimated kernel density of the empirical meta-analytica data $\widehat{f}_e$ and theoretical simulated data $\widehat{f}_t$ from the publication bias model. Optimisation is performed using differential evolution to the find the minimum of  $D_{\text{KL}}(\widehat{f}_e ~ || ~ \widehat{f}_t)$.
\endgroup
\end{figure}
```

### Simulation Framework

The initial step in the SPEEC method entails defining the marginal distributions of effect size and sample size for the generative publication bias model. This requires additional assumptions regarding the type of study design from which the simulated effect sizes and sample sizes originate. For the purpose of this study, Cohenâ€˜s *d* is adopted as a widely used effect size measure for mean differences [@lakens_calculating_2013]. Accordingly, the simulated effect sizes are assumed to originate from a between-subjects, two-sample *t*-test study design. However, it is worth noting that the SPEEC framework is adaptable to different study designs where alternative effect size measures are typically employed (e.g., correlational effect sizes or effect sizes derived from proportional data). Depending on the effect size, this would require adapting the marginal distribution of the effect size and  how statistical significance is determined based on a different test statistic, which is required for the application of publication bias (see [Application of Publication Bias](#application-of-publication-bias)).

The marginal distribution for the total sample size *n* of a study should be inherently modelled as a discrete distribution. Count data of this nature are commonly modelled using either a Poisson or Negative-Binomial distribution. In various psychological domains, sample size distributions often exhibit considerable variance and skewness [e.g., @cafri_meta-meta-analysis_2010; @marszalek_sample_2011; @sassenberg_research_2019; @shen_samples_2011; @szucs_empirical_2017]. Considering this, the negative-binomial distribution was chosen because it can model overdispersion more effectively by introducing a second parameter [@lloyd-smith_maximum_2007]. The mean-dispersion parametrisation of the negative-binomial distribution is used where the probability of success $p_n$ and the target number of successes $r_n$ is reparametrised to mean $\mu_n = \frac{r_n\cdot(1-p_n)}{p_n}$ and dispersion $\phi_n=r_n$ to model the study-specific total sample sizes $n_i$.
$$
n_1, \,n_2,\, \ldots, \,n_k \quad \text{where} \quad N \sim \mathcal{NB}(\phi_n, \mu_n) \quad 
$$ {#eq-nb-dist}

Concerning the marginal distribution of effect size *d*, it is reasonable to assume a Gaussian distribution with mean $\mu_d$ and variance $\sigma^2_d$ [e.g., @borenstein_basic_2010]. To account for the increasing precision in estimating the true effect size mean $\mu_d$ as the sample size increases (i.e., the sampling error), the variance of the mean differences $\bar{x}_{i1} - \bar{x}_{i2}$ are computed, from which the effect sizes are assumed to originate in this type of study configuration. Subsequently, a normalisation factor $\gamma_i$ can be derived by scaling each individual variance $\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}$ with the overall mean of those variances such that $\bar{\gamma}=1$.

$$
\begin{gathered}
\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}=\sigma_d^2/n_i \\
\gamma_i=\frac{\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}}{\sum_{i=1}^{k}\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}/k} \\
\end{gathered}
$$ {#eq-normalisation}

\noindent
With this normalisation factor, the total variance of the individual variances is $\mathbb{E}(\gamma_i \cdot \sigma^2_d) = \sigma^2_d$. The study-specific effect sizes $d_i$ are subsequently modelled as

$$
d_1, \,d_2, \,\ldots, \,d_k \quad \text{where} \quad d_i \stackrel{}{\sim} \mathcal{N}(\mu_d, \gamma_i\cdot\sigma_d^2) \quad \text{for} \quad i = 1, \ldots, k.
$$ {#eq-norm-dist}


Using this definition for the marginal distribution of effect size, the SPEEC method assumes a fixed effect meta-analytical model, where the only source of variation in effect size is sampling error. However, the publication bias simulation model of the SPEEC method could be further extended to account for effect size heterogeneity by including an additional parameter $\tau^2$ in the simulation framework to account for variability beyond sampling error.

Conditional on the marginal distributions, $k$ studies are drawn from the joint distribution of effect size and sample size, where $k$ is user-defined. An increase in $k$ reduces the uncertainty in the joint distribution but also comes with a trade-off of increased computational complexity. For our analysis, we selected $k=10^4$ samples for each simulation iteration to analyse the hypotheses.


### Application of Publication Bias

Following the simulation step of sampling $k$ studies from the joint distribution of effect size and sample size, the subsequent stage involves the application of publication bias to the studies. As previously discussed, publication bias is operationalized in terms of the likelihood of a study being published conditional on the statistical significance of its results. Translated to this simulation setting, two-tailed *p*-values for each individual study *i* can be calculated from the corresponding effect size $d_i$ and sample size $n_i$. To implement this, it is assumed that the individual studies $i$ originate from a balanced sample size design. When the total sample size $n_i$ is even, the between-subject group sample sizes $n_{1i}$ and $n_{2i}$ are defined as $n_i/2$. Otherwise, when the total sample size $n_i$ is odd, the group sample sizes are determined as the ceilinged $\lceil n_i/2 \rceil$ and floored $\lfloor n_i/2 \rfloor$ values. Subsequently, the *p*-value $p_i$ of each simulated study can be derived from its corresponding *t*-value 

$$
\begin{gathered}
t_i=\Big\lvert \frac{d_i}{\sqrt{1/n_{1i} +1/n_{2i}}} \Big\rvert \\
p_i=2 \cdot P(t_i~|~df_i)
\end{gathered}
$$ {#eq-t-value}

\noindent
where $P(t_i , \, df_i)$ is the cumulative *t*-distribution with degrees of freedom $df_i=n_i-2$. Given each *p*-value $p_i$, publication bias is introduced by assigning each study $i$ a weight

$$
\omega_{\text{PBS}_i}(p_i)= \begin{cases}  
\pbs & \text{for}~~p_i \geq \alpha \\
1 & \text{otherwise}\end{cases}
$$ {#eq-pbs}

\noindent
with the constraint $\pbs \in \mathbb{R} : 0 \leq \pbs \leq 1$. This weight denotes the probability of a study $i$ being selected conditional on the *p*-value and the type I error rate $\alpha$. This definition of the publication bias weight is directly analogous to the step weighting function used in publication bias selection models [see @hedges_modeling_1992; @iyengar_selection_1988]. If a study $i$ is statistically nonsignificant (i.e., $p_i \geq \alpha$), the publication bias parameter $\pbs$ is assigned; otherwise the probability of a study being selected is equal to 1, indicating no publication bias. Thus, the publication bias parameter $\pbs$ denotes the probability of selecting a nonsignificant study relative to a statistically significant one. Importantly, the type I error rate needs to be fixed across all simulated studies and is set to the common nominal value of $\alpha = 0.05$ for the analyses of the hypotheses. Following the computation of the publication bias weight $\omega_{\text{PBS}_i}$ for each study $i$, the publication bias selection process can be defined in terms of an indicator function $1_{\omega_{\text{PBS}}}(\omega_{\text{PBS}_i})$ and $k$ draws from a uniform distribution $u_1, u_2, \dots, u_k \sim \mathcal{U}_{[0, 1]}$ where

$$
\begin{gathered}
1_{\omega_{\text{PBS}}}(\omega_{\text{PBS}_i}) 
\begin{cases}
1 & \text{if} \quad \omega_{\text{PBS}_i}\geq u_i \quad \text{study selected} \\
0 & \text{if} \quad \omega_{\text{PBS}_i} < u_i  \quad \text{study not selected.}
\end{cases}
\end{gathered}
$$ {#eq-indicator-function}


\noindent
Using this indicator function, the resulting subsets for the selected $\{d_i^\prime, \,n_i^\prime\}$ and non-selected samples $\{d_i^{\prime\prime}, \,n_i^{\prime\prime}\}$ from the initial $k$ simulated studies can be defined as

$$
\begin{gathered}
\big\{d_i^\prime, n_i^\prime\big\} = \big\{d_i, n_i ~ | ~ 1_{\omega_{\text{PBS}}}(\omega_{\text{PBS}_i})= 1\big\} \quad \text{for} \quad i = 1, \dots, k^\prime \quad \text{and} \\
\big\{d_i^{\prime\prime}, n_i^{\prime\prime}\big\} = \big\{d_i, n_i ~ | ~ 1_{\omega_{\text{PBS}}}(\omega_{\text{PBS}_i})= 0\big\} \quad \text{for} \quad i = 1, \dots, k^{\prime\prime}.
\end{gathered}
$$ {#eq-subsets}


One challenge in simulating a fixed number of $k$ studies is that, ceteris paribus, with increasing publication bias (i.e. lower values for $\pbs$), increasingly fewer studies $k^\prime$ remain after the selection process compared to the original number of simulations $k$. This process would lead to a loss of precision in the parameter estimation for decreasing values of $\pbs$. To address this, the steps mentioned above (from @eq-nb-dist to @eq-subsets) are repeated a second time with an adjusted number of simulations $k_{\small{\text{adj}}} = \big\lceil {k^2}/{k'} \big\rceil$, thereby ensuring that the number of selected studies $k^\prime_{\small{\text{adj}}}$ is approximately equal over the entire range of $\pbs$.

### Divergence Between Empirical and Simulated Data from the Publication Bias Model
 
After the simulation of $k_{\small{\text{adj}}}$ samples from the publication bias model conditional on the marginal distributional parameters ($\mu_d$, $\sigma^2_d$, $\phi_n$, $\mu_n$) and the publication bias parameter $\pbs$ to determine the subset $\{d_i^\prime, \,n_i^\prime\}$ for $i=1, \dots,i=k_{\small{\text{adj}}}^\prime$, the following step involves quantifying how closely the simulated data from the publication bias model aligns with the empirical meta-analytical data. More specifically, this step involves measuring the statistical dissimilarity of the bivariate kernel density estimates between the empirical meta-analytical data and the simulated data of the publication bias model. While various measures of statistical dissimilarity between probability distributions exist [for an overview, see @cha_comprehensive_2007], the Kullback-Leibler divergence [$D_{\text{KL}}$, @kullback_information_1951] was chosen as a dissimilarity measure for SPEEC. This choice was motivated by superior performance of KLD in capturing the dissimilarity between estimated kernel densities compared to alternative measures (total variation distance and earthmover distance) tested for the SPEEC method, particularly across boundary conditions. In addition, $D_{\text{KL}}$ has an intuitive interpretation in this context. It can be interpreted as the expected value of the log-likelihood ratio favouring the true model over a candidate model [@etz_technical_2018]. Here, the estimated joint density of effect size and sample size from the meta-analytical data $\widehat{f}_{e}$ is regarded as the true data generating distribution and the estimated kernel density of the simulated data $\widehat{f}_{t}$ from the publication bias model serves as the approximate candidate distribution.

```{r}
kernsmooth_ver <- as.character(packageVersion("KernSmooth"))
```

To implement this step, the *KernSmooth* R package [version `r kernsmooth_ver`, @wand_kernsmooth_2023] was used to estimate the joint kernel density distribution for both empirical and simulated data using a bivariate standard Gaussian kernel that is evaluated on a linearly-binned square grid [@wand_fast_1994; @wand_kernel_1994]. The grid size was chosen as $n_{\text{grid}}=2^7+1$ equidistant grid points in each dimension such that the total number of cells is $n_\text{grid} \times n_\text{grid}$, but is user-definable in the R package for SPEEC. The bandwidth of the kernel function was determined using the reliable plug-in method proposed by @sheather_reliable_1991, but the R package also offers other common bandwidth selection methods. To ensure the comparability of the estimated kernel densities between the empirical and simulated data, the bounds of the square grid, $\mathrm{b}_\mathrm{n} \times \mathrm{b}_\mathrm{d}$, must be identical and are determined from the empirical meta-analytical data. 

For defining these bounds, the maximum likelihood values for the parameters of the marginal distribution of effect size ($\hat{\mu}_d,\hat{\sigma}^2_d$) and sample size ($\hat{\phi}_n, \hat{\mu}_n$) are estimated. These estimates are used to determine the quantiles that cover the inner 99% of the cumulative distribution from the quantile functions $Q_d(p ~ | ~ \hat{\mu}_d, ~ \hat{\sigma}^2_d)$ and $Q_d(p ~ | ~ \hat{\phi}_n, ~ \hat{\mu}_n)$ for the percentiles $p_1=0.5$ and $p_2 = 99.5$. Subsequently, the bounds for the effect size $\mathrm{b}_\mathrm{d}$ and sample size $\mathrm{b}_\mathrm{n}$ are defined as the minimum and maximum values of these quantiles and the range of the empirical data, respectively.

$$
\begin{gathered}
 \mathrm{b}_\mathrm{d}=\left\{Q_d(p_1 ~ | ~ \hat{\mu}_d, \, \hat{\sigma}_d^2) \land \min(d), \, Q_d(p_2 ~ | ~ \hat{\mu}_d, \, \hat{\sigma}_d^2) \lor \max(d) \right\} \\
\mathrm{b}_\mathrm{n} = \left\{Q_n(p_1 ~ | ~ \hat{\phi}_n, \, \hat{\mu}_n) \land \min(n), \, Q_n(p_2 ~ | ~ \hat{\phi}_n, \, \hat{\mu}_n) \lor \max(n) \right\}
\end{gathered}
$$ {#eq-bounds-kde}

This ensures that an adequate range is covered for the kernel density estimation. Finally, $D_{\text{KL}}$ is calculated from the binned kernel density estimates of the empirical $\hat{f}_e$ and simulated theoretical $\hat{f}_t$ data.

$$
D_{\text{KL}}(\widehat{f}_e ~ \lVert ~ \widehat{f}_t)=\nsum_{u=1}^{n_{\text{grid}}}\nsum_{v=1}^{n_{\text{grid}}}\widehat{f}_e(u, v)\ln\left(\frac{\widehat{f}_e(u,v)}{\widehat{f}_t(u,v)}\right)
$$ {#eq-kld}


### Formulation as an Optimisation Problem

Summarising the previous sections, the simulation framework within the publication bias model of SPEEC requires the distributional parameters for the marginal distributions of effect size and sample size and the publication bias parameter as input parameters. It returns a single scalar value representing $D_{\text{KL}}$ between the estimated joint kernel density of the simulated theoretical data and the empirical meta-analytical data. This framework can be considered as an optimisation problem of finding parameter values such that 

<!--\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt} -->


$$
\begin{gathered}
\min_{\mu_d,  ~ \sigma^2_d,  ~ \mu_n, ~ \phi_n, ~ \omega_{PBS}} \Big\{D_{\text{KL}}(\widehat{f}_e ~ \| ~ \widehat{f}_t) \Big\},  \\ \text{subject to:} ~ \mu_d, ~ \sigma^2_d, ~ \mu_n, ~ \phi_n,~ \omega_{PBS} \in \mathbb{R}, \quad \text{where}  \quad 0 \leq \omega_{PBS} \leq 1, \\
-4 \leq \mu_d \leq 4, \quad 0 \leq \sigma^2_d \leq 6,  \quad 10 \leq \mu_n \leq 15000, \quad 0.01 \leq \phi_n \leq 1000.
\end{gathered}
$$ {#eq-optim}

### Parameter Optimisation with Differential Evolution

For this purpose, we use differential evolution [DE, @storn_differential_1997], a simple global optimisation algorithm [@feoktistov_differential_2006]. DE is an evolutionary metaheuristic based on principles such as mutation, cross-over, and selection. It only requires a few control parameters that are generally straightforward to select to achieve favourable outcomes compared to other optimisation algorithms [@storn_differential_1997]. Importantly, all parameters of the DE algorithm, including the control parameters, the stopping criterion and boundary constraints of the differential evolution algorithm, were defined globally for the parameter estimation of all meta-analyses.

```{r}
rcppde_ver <- as.character(packageVersion("RcppDE"))
```

To apply DE for the optimisation, the R package *RcppDE* [version `r rcppde_ver`, @edelbuettel_rcppde_2022] was utilised, which implements the classical DE algorithm ${DE/rand/1}$ [@storn_differential_1997]. The control parameters for DE were chosen based on the recommendations of Storn and Price [-@storn_differential_1997], with additional adjustments informed by preliminary testing of simulated data from the simulation framework of SPEEC, setting the population size $NP$ to 150, the mutation constant $F$ to 0.9 and the cross-over constant $CR$ to 0.1. In applying the DE algorithm, we adopted a direct termination criteria approach [@jain_termination_2001; @ghoreishi_termination_2017], with the termination condition being the maximum number of generations. Since there are no universally applicable default values for the maximum number of generations, as it is contingent upon the optimisation problem at hand [@jain_termination_2001], the choice for $t_{\text{max}}$ was also informed by preliminary testing of the simulation framework of SPEEC. These tests suggested that $t_{\text{max}} = 1000$ is a reasonable decision. In establishing the bounds for the parameter search space (see @eq-optim), a balance was struck between avoiding bounds that were too wide, which could lead to ineffective exploration of the search space, and ensuring that the bounds were not too narrow to provide sufficient coverage of the potential parameters. More specifically, the minima and maxima for all distributional parameters were determined using maximumm likelihood (see [Appendix C](Appendix C: MLE Extrema Distributional Parameters) *Table C1*), and the boundaries were set slightly above those values to ensure good coverage. 

## Secondary Data Description

To examine the confirmatory hypotheses of this study, which aim to provide a preliminary assessment of the viability of the SPEEC method, we use secondary data sourced from previous research by Linden and HÃ¶nekopp [-@linden_heterogeneity_2021]. The dataset can be accessed both from its source (see https://osf.io/yr3xd/) and through the  repositories associated with this project (see section [Appendix A](Appendix A: Research Transparency Statement)). The dataset is comprehensive and includes both classical meta-analyses and registered replication reports for which publication bias is assumably absent. The dataset encompasses a total of 207 research syntheses covering various psychological phenomena. The dataset includes 150 meta-analyses, each subset encompassing 50 meta-analyses from different subfields of psychology (social psychology, organisational psychology, and cognitive psychology). Additionally, the dataset includes 57 registered replication reports, which are particularly relevant for investigating hypotheses $\hypothesis{3}{}$ and $\hypothesis{4}{}$. Information on the total sample size $n_i$ and effect size $d_i$ for each primary study was gathered for each research synthesis. The meta-analyses were selected using random sampling, with predefined inclusion criteria and specified journals from which the data were selected. One important inclusion criterion by Linden and HÃ¶nekopp [-@linden_heterogeneity_2021] was that effect sizes must be reported as standardised mean differences (Cohenâ€™s *d* or Hedgesâ€˜ *g*) or as correlations (Pearsonâ€˜s *r* or Fisherâ€˜s *z*). In cases where a different effect size measure than Cohenâ€˜s *d* was used, effect sizes were transformed accordingly [@linden_heterogeneity_2021].


## Statistical Analysis

```{r}
#| echo: false

r_ver <- glue("{version$major}.{version$minor} {version$nickname}")
betareg_ver <- as.character(packageVersion("betareg"))
toster_ver <- as.character(packageVersion("TOSTER"))
```

The confirmatory statistical analyses were preregistered based on an adaption of the secondary data analysis preregistration template by @van_den_akker_preregistration_2021. The preregistration includes additional metadata on the dataset and a transparency statement regarding prior data knowledge. Any supplementary analyses not preregistered and deviations of the preregistered confirmatory analyses are explicitly labelled as such. All data, analysis scripts and the preregistration are made available (see [Appendix A](Appendix A: Research Transparency Statement)). All confirmatory analyses were performed using the R programming language [version `r r_ver`, @r_core_team_r_2023].

Regarding the hypotheses, in which $\pbs$ was the dependent variable ($\hypothesis{1}{}$, $\hypothesis{2}{}$, $\hypothesis{4}{}$), beta regression as implemented in the *betareg* package [version `r betareg_ver`, @zeileis_betareg_2021] was used to analyse the data. This choice was motivated by the restriction of the parameter space for $\pbs$ to the standard unit interval, whereby non-normality, skewness, and heteroscedasticity can be anticipated [@cribari-neto_beta_2010; @smithson_better_2006]. Beta regression is recognised for its adaptability in handling such deviations. Because the optimisation approach permits $\widehat{\omega}_{\text{PBS}}$ values within the range $0 \leq \widehat{\omega}_{\text{PBS}} \leq 1$, and the beta regression model assumes that $0 < \widehat{\omega}_{\text{PBS}} < 1$, a common transformation proposed by Smithson and Verkuilen [-@smithson_better_2006] was employed. This transformation, denoted as $\pbs^\prime={(\pbs \cdot(n-1)+0.5)}/{n}$, subtly adjusts the bounds to slightly narrow the range between zero and one. We used a logit link for the mean parameter $\mu$ and an identity link for the fixed dispersion parameter such that the beta regression model can be described as
 
 
$$
\begin{aligned}
\omega_{\text{PBS}^\prime_i} \sim  \mathcal{B}(\mu_i, \phi) \\
\log{\bigg(\frac{\mu_i}{1 - \mu_i}\bigg)}= \mathbf{X_i}^\top \symbf{\beta}. 
\end{aligned}
$$ {#eq-beta-reg}


The independent variables for these three hypotheses were as follows: regarding $\hypothesis{1}{}$, the independent variable was the Fisher *z*-transformed correlation coefficient of the correlation between effect size and sample size, where the transformation is defined as $z_r=\tanh^{-1}(r)$. The independent variable for $\hypothesis{2}{}$ was the difference $\Delta_{\widehat{\mu}_d}$ between the average effect size estimate of each meta-analysis and the mean parameter of the Gaussian effect size distribution estimated with SPEEC. Lastly, the independent variable for $\hypothesis{4}{}$, was a binary indicator specifying the research synthesis type (classical meta-analysis or RRR), with RRR set as the reference level for regression. The coefficients from the beta regressions for these hypotheses were estimated using Maximum Likelihood estimation with the L-BFGS optimiser [@liu_limited_1989].

Hypothesis $\hypothesis{3}{}$ was formulated to compare the estimated means of the Gaussian effect size distribution to the average effect sizes. This comparison aimed at assessing whether the presence of effects in mean differences $\Delta_{\widehat{\mu}_d}$ deemed large enough to be considered meaningful, according to specified equivalence bounds $\Delta_{EQ}$, can be rejected [@lakens_improving_2020]. For this purpose, an equivalence test using the Two One-Sided Tests (TOST) procedure [@schuirmann_comparison_1987] was conducted as implemented in the *TOSTER* R package [version `r toster_ver`, @lakens_toster_2023]. This involves conducting two one-tailed tests against the upper and lower equivalence bounds, where both must yield significant results to claim statistical equivalence within the equivalence range $\Delta_{EQ}$. The preregistration specified that the *means* for both variables would be compared for the TOST procedure, implying a Student *t*-test for dependent samples. However, upon analysis of the assumptions for the Student *t*-test, it was found that the normality assumption was violated for both distributions. Parametric tests like the Student *t*-test are generally robust to violations of the normality assumption [@boneau_effects_1960; @knief_violating_2021], so we proceeded as preregistered. However, to assess the robustness of the findings, we also conducted sensitivity analyses using the nonparametric Wilcoxon signed-rank test as part of the TOST procedure (see [Appendix D](Appendix D: Sensitivity Analyses of Equivalence Test)). The choice of a dependent sample test was necessitated by the dependence between the pairs of samples originating from the same underlying data. The calculation of effect size measures is beneficial to obtain additional information about the magnitude of equivalence (not preregistered analyses). However, traditional effect size metrics, such as Cohenâ€˜s *d*, prove inadequate within equivalence testing as they do not consider the equivalence range. Hence, the Proportional Distance $PD$, explicitly designed for equivalence testing, was utilised as an effect size metric [see @martinez_gutierrez_effect_2023]. The $PD$ quantifies the proportional distance from the observed effect ($\Delta_{\widehat{\mu}_d}$) to the bound of the equivalence range that is the same sign as the observed effect. 

## Smallest Effect Size of Interest

For all four hypotheses, the smallest effect size of interest (SESOI) was established based on effect sizes that can be reliably detected, considering the constraints imposed by the sample size resources available for this secondary data analysis [@lakens_performing_2014; @lakens_equivalence_2018]. More specifically, three simulation-based ($\hypothesis{1}{}$, $\hypothesis{2}{}$, $\hypothesis{4}{}$) and one analytical ($\hypothesis{3}{}$) sensitivity power analysis were conducted to determine for which effect sizes we have at least 80\% power ($1-\beta=0.8$) to detect, taking into account the predetermined sample size and fixed type I error rate $\alpha = .05$ (details see [*Appendix B*](Appendix B: Power Analyses determining the SESOIs)). We specified the SESOI for $\hypothesis{3}{}$ in raw units and all other SESOIs in odds ratios. The SESOI for the equivalence hypothesis defines the equivalence bounds for the TOST procedure, $\Delta_{EQ} = \{-0.17, 0.17\}$. *Table 1* summarises all four SESOIs of the hypotheses.

```{r}
#| echo: false
#| results: asis

df_sesoi <- read_csv(here("preregistration/data/sesois.csv")) |>
    mutate(hypothesis = paste("$\\hypothesis{", c("i", "ii", "iii", "iv"), "}{}$", sep = "")) |>
    mutate(unit = if_else(unit == "OR", glue::glue("${unit}$"), "raw unit"))

nice_table(
    x = df_sesoi, col_names = c("Hypothesis", "SESOI", "Unit"),
    general_fn = "Except for $\\\\hypothesis{3}{}$ all SESOIs are defined in terms of odds ratios (OR). The SESOI of $\\\\hypothesis{3}{}$ is defined in raw units.",
    caption = "SESOIs of the Four Hypotheses"
) |>
str_replace_all(string = _, pattern = "\\\\begin\\{tablenotes\\}", "\\\\begin\\{tablenotes\\}[flushleft]") |>
cat()


```