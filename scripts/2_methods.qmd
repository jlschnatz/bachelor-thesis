---
bibliography: ../references.bib
---

\newpage

```{r}
#| echo: false
source(here::here("R/00_functions.R"))
pacman::p_load(tidyverse, insight, here, knitr, kableExtra)
```

# Method

## The SPEEC Method

<!-- Finished this section! -->

This section offers a comprehensive in-depth description and explanation of the SPEEC framework to assess the extent of publication bias and estimate effect sizes in the presence of publication bias in meta-analyses. @fig-speec-workflow offers an overview of the sequential steps of the SPEEC method. Currently, SPEEC is being developed as an open-source R package and is accessible on GitHub at <https://github.com/jlschnatz/speec>.


```{r speec_workflow}
#| out-width: 80%
#| echo: false
#| fig-cap-location: top
#| fig-cap: Overview of the SPEEC Approach
#| fig-pos: H
#| label: fig-speec-workflow
#| fig-align: center

include_graphics(here("figures/speec_workflow.png"))

```

```{=latex}
\begingroup
\small
\noindent
\textit{Note.} Test.
\endgroup
```

### Simulation Framework

<!-- Finished! -->

The initial step in the SPEEC method entails defining the marginal distributions of effect size and sample size for the generative publication bias model. This requires additional assumptions regarding the type of study design from which the simulated effect sizes and sample sizes originate. For this study, Cohen´s *d* is adopted as a widely used effect sizes measure for mean differences[@lakens_calculating_2013] as the effect size for simulation. For this reason, we assume that the simulated effect sizes originate from a between-subjects two-sample *t*-test study design. However, it is worth noting that the SPEEC framework remains adaptable to different study designs where alternative effect size measures are typically employed (e.g., correlational effect sizes, effect sizes derived from proportional data). Depending on the effect size, this would require a potential adaption of the marginal distribution for the effect size and how statistical significance for each simulated study based on a different effect sizes is determined for the application of publication bias.  

The marginal distribution for the total sample size $\mathrm{n}$ of a study should be inherently modeled as a discrete distribution. Count data of this nature are commonly modeled using either a Poisson or Negative-Binomial distribution. In various psychological domains, sample size distributions often exhibit considerable variance and skewness [see for example @cafri_meta-meta-analysis_2010; @marszalek_sample_2011; @sassenberg_research_2019; @shen_samples_2011; @szucs_empirical_2017]. Considering this variability and skewness we opted for the Negative-Binomial distribution which can account for variance independently of the mean and thus handle overdispered data effectively. We use the mean-dispersion parametrization where the probability of sucess $p$ and the target number of sucesses $r$ are reparametrized to mean $\mu = \frac{r\cdot(1-p)}{p}$ and dispersion $\phi=r$ to model the study-specific total sample sizes $n_i$.

$$
n_1, \,n_2,\, \ldots, \,n_k \quad \text{where} \quad N\stackrel{\mathrm{i.i.d.}}{\sim} \mathcal{NB}(\phi_n, \mu_n) \quad \text{for } i = 1, \ldots, k
$$ {#eq-nb-dist}

Concerning the marginal distribution of the effect size $\mathrm{d}$, it is reasonable to assume a Gaussian distribution with mean $\mu_d$ and variance $\sigma^2_d$. To account for the increasing precision in estimating the true effect size mean $\mu_d$ as the sample size increases (i.e., the sampling error), that contributes to the characteristic funnel shaped effect size-sample size distribution, we compute the variance of the mean differences $\bar{x}_{i1} - \bar{x}_{i2}$, from which the effect sizes originate in this type of design. Subsequently,  a normalization factor $\gamma_i$ is derived by dividing each individual variance $\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}$ with the overall mean of those variances ensuring that $\bar{\gamma}=1$.

$$
\begin{gathered}
\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}=\sigma_d^2/n_i \\
\gamma_i=\frac{\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}}{\sum_{i=1}^{k}\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}/k} \\
\end{gathered}
$$ {#eq-normalization}

\noindent
With this normalization factor, the total variance of the individual variances of the individual variances is $\text{Var}(\gamma \cdot \sigma^2_d) = \sigma^2_d$. The study-specific effect sizes $d_i$ are subsequently modeled as

$$
d_1, \,d_2, \,\ldots, \,d_k \quad \text{where} \quad D \stackrel{}{\sim} \mathcal{N}(\mu_d, \gamma_i\cdot\sigma_d^2) \quad \text{for} \quad i = 1, \ldots, k.
$$ {#eq-norm-dist}

Using this definition for the marginal distribution of effect size, the SPEEC approach assumes a fixed effects meta-analytical model, where the only source of variation in effect size is explained by measurement error. However, it is worth noting that the publication bias simulation model of the SPEEC method could be further extended to account for effect size heterogeneity. This could involve including an additional heterogeneity parameter $\tau^2$ in the simulation framework to account for additional variability that goes beyond sampling error.

Conditional on the marginal distributions, $k$ individual studies are sampled from the joint distribution of effect size and sample size. The selection of $k$ is user-defined, however, the larger the number of samples $k$, the lower the uncertainty of the joint distribution of effect size and sample size. In general, there is a trade-off between the increased computational cost and the reduced uncertainty of the joint distribution for increasing values of $k$. We opted for $k=10^4$ samples for each simulation iteration for the analyses of the hypotheses.

### Application of Publication Bias

Following the simulation step of sampling $k$ individual studies from the joint distribution of effect size and sample size, the subsequent stage involves the application of publication bias to the random samples. As previously discussed, publication bias is operationalized in terms of the likelihood of a study being published conditional on the statistical significance of its results. Statistical significance in traditional null hypothesis significance testing (NHST) contexts is commonly determined using *p*-values, employing a dichotomous decision rule with conventional type I erro-rates set as cut-off values [@andrade_p_2019; @mcshane_abandon_2019]. Translated to this simulation setting, two-tailed *p*-values for each individual study *i* can be calculated from the corresponding effect size $d_i$ and sample size $n_i$. To implement this, we assume that the individual studies $i$ in the simulation originate form a balanced sample size design. This means that when the total sample size $n_i$ is even, the group sample sizes $n_{1i}$ and $n_{2i}$ are simply defined as $n_i/2$. Otherwise, when the total sample size $n_i$ is odd, the group sample sizes are determined as the ceilinged $\lceil n_i/2 \rceil$ and floored $\lfloor n_i/2 \rfloor$ values, respectively. Subsequently, the *p*-value $p_i$ of each simulated study can be derived from its corresponding *t*-value 

$$
t_i=\Big\lvert \frac{d_i}{\sqrt{1/n_{1i} +1/n_{2i}}} \Big\rvert.
$$ {#eq-t-value}

$$
p_i=2 \cdot P(t_i~|~df_i)
$$ {#eq-p-value}

\noindent
where $P(t_i , \, df_i)$ is the cumulative central *t*-distribution with degrees of freedom $df_i=n_{1i}+n_{2i}-2$. Given each *p*-value $p_i$, publication bias is introduced by assigning each study $i$ a weight

$$
\omega_{\text{PBS}_i}(p_i)= \begin{cases}  
\pbs & \text{for}~~p_i \geq \alpha \\
1 & \text{otherwise}\end{cases}
$$ {#eq-pbs}

\noindent
with the constraint $\pbs \in \mathbb{R} : 0 \leq \pbs \leq 1$. This weight denotes the probability of a study $i$ being selected conditional on the *p*-value and the type I error rate $\alpha$. This definition of the publication bias weight is directly analogue to the step weighting function used in publication bias selection models [see @hedges_modeling_1992; @iyengar_selection_1988]. If a study $i$ is not statistically significant (i.e., $p_i \geq \alpha$), the publication bias parameter $\pbs$ is assigned, else the probability of a study being selected is equal to one, indicating no publication bias. Thus, the publication bias parameter $\pbs$ denotes the probability the selection of a non-significant study relative to a statistiacally significant study. For instance, a publication bias parameter of $\pbs=0.5$ would indicate that simulated studies that are non-significant are half as likely to be selected in comparison to studies that are statistically significant. Importantly, the type I error rate needs to be fixed across all simulated studies and is set to the common significance threshold of $\alpha = 0.05$ for the analyses of the hypotheses. Following the computation of publication bias weight $\omega_{\text{PBS}_i}$ for each study $i$, the likelihood of a study being selected can be expressed as $\mathbb{P}(S_i = 1)= \omega_{\text{PBS}_i}$. Here, $S_i$ is a binary indicator function, signifying whether an individual study $i$ is selected during the publication bias selection process.

$$
S_i = 
\begin{cases}  
  0 & \text{study not selected} \\
  1 & \text{study selected}
\end{cases}
$$ {#eq-id-fun}

\noindent
Using this binary indictor function, the resulting subsets for the selected $(d_i^\prime, \,n_i^\prime)$ and non-selected samples $(d_i^{\prime\prime}, \,n_i^{\prime\prime})$ from the initial $k$ simulated studies can be defined as

$$
\begin{gathered}
(d_i^\prime, n_i^\prime) = (d_i, n_i ~ | ~ S_i = 1) \quad \text{for} \quad i = 1, \dots, k^\prime \quad \text{and} \\
(d_i^{\prime\prime}, n_i^{\prime\prime}) = (d_i, n_i ~ | ~ S_i = 0) \quad \text{for} \quad i = 1, \dots, k^{\prime\prime}.
\end{gathered}
$$ {#eq-subsets}


One challenge of simulating a fixed number of $k$ studies is that, all else being equal, as the severity of publication bias increases (i.e., lower values for $\pbs$), there are fewer remaining studies $k^\prime$ after the selection process compared to the original number of simulations $k$. This would result in a loss of precision in the parameter estimation for decreasing values of $\pbs$. To address this, the process outlined above (from  @eq-nb-dist - @eq-subsets) is repeated a second time with an adjusted number of simulations $k_{\small{\text{adj}}} = \big\lceil {k^2}/{k'} \big\rceil$, ensuring that the number of selected studies $k^\prime_{\small{\text{adj}}}$ is roughly equal across the entire range of $\pbs$. This adjusted number of of simulations $k_{\small{\text{adj}}}$ corresponds to the ceiling of the number of initial simulations $k$ divided by the proportion of "survived" studies $k^\prime/k$.

### Formulation as an Optimization Problem

After the simulation of $k_{\small{\text{adj}}}$ samples from the publication bias model conditional on the marginal distribution parameter of effect size ($\mu_d$, $\sigma^2_d$) and sample size ($\phi_n$, $\mu_n$) and the publication bias parameter $\pbs$, the resulting subset after the application of publication bias After the application of publication bias to the random samples to determine the subset $(d_i^\prime, \,n_i^\prime)$ for $i=1, \dots,i=k_{\small{\text{adj}}}^\prime$ .

Following step...

quantifiy how closely the simulated data from the publication bias model aligns with the empirical meta-analytical data. More specifically, we are interested in quantifying the statistical distance of the bivariate kernel density estimate of between the empirical meta-analytical data and the simulated data of the publication bias model.

Different ways to define statistical distances between probablity distributions (or estimated kernel density), we use Kullback-Leibler divergence [KL-divergence, @kullback_information_1951], origins in information theory.

In this context the KL-divergence has an intuitieve interpretation where the estimated joint density of effect size and sample size of meta-analytical data $\widehat{f}_{e}$ is assumed to be the true data generating distribution and the estimated kernel density from data simulated from the theoretical publication bias model $\widehat{f}_{t}$ is an approximate of $\widehat{f}_{e}$.

To accomplish this, the *KernSmooth* R package [@wand_kernsmooth_2023] was used to estimate the joint kernel density distribution for both empirical data and simulated  using a bivariate standard Gaussian kernel that is evaluated on a linearly-binned square grid [see @wand_fast_1994; @wand_kernel_1994]. The grid size was chosen to be $n_{\text{grid}}=2^7+1$ equidistant grid points in each dimension but is user-definable in the *speec* R package. The bandwidth of the kernel function was determined using the reliable plug-in method proposed by @sheather_reliable_1991, but the *speec* R package also offers other common bandwidth selection methods. The ensure the comparability of the estimated kernel densities between the empirical and simulated data, the bounds of the the square grid, $\mathbf{b}_\mathrm{n} \times \mathbf{b}_\mathrm{d}$, have to be exactly the same and are determined based on the empirical meta-analytical data. To define the bounds, the maximum likelihood values for the parameters of the marginal distribution of effect size ($\hat{\mu}_d,\hat{\sigma}^2_d$) and sample size ($\hat{\phi}_n, \hat{\mu}_n$) are estimated. Using these estimates, the quantiles spanning the inner 99 percentile of the cumulative distribution function are obtained from the quantile functions $Q_d(p ~ | ~ \hat{\mu}_d, ~ \hat{\sigma}^2_d)$ and $Q_d(p ~ | ~ \hat{\phi}_n, ~ \hat{\mu}_n)$, where $p_1=0.005$ and $p_2 = 1-p_1$. Subsequently the bounds for the effect size $\mathbf{b}_\mathrm{d}$ and sample size $\mathbf{b}_\mathrm{n}$ are defined as the minimum and maximum values of these quantiles and the range of the empirical data, respectively.

$$
\begin{gathered}
 \mathbf{b}_\mathrm{d}=\left\{Q_d(p_1 ~ | ~ \hat{\mu}_d, \, \hat{\sigma}_d^2) \land \min(d), \, Q_d(p_2 ~ | ~ \hat{\mu}_d, \, \hat{\sigma}_d^2) \lor \max(d) \right\} \\
\mathbf{b}_\mathrm{n} = \left\{Q_n(p_1 ~ | ~ \hat{\phi}_n, \, \hat{\mu}_n) \land \min(n), \, Q_n(p_2 ~ | ~ \hat{\phi}_n, \, \hat{\mu}_n) \lor \max(n) \right\}
\end{gathered}
$$ {#eq-bounds-kde}

These ensures that the entire range of the empirical data is covered for the kernel density estimation. Finally, the Kullback-Leibler-Divergence is computed from the kernel density estimates of the empirical $\hat{f}_e$ and simulated theoretical $\hat{f}_t$ data.
$$
D_{\text{KL}}(\widehat{f}_e ~ \lVert ~ \widehat{f}_t)=\nsum_{u=1}^{n_{\text{grid}}}\nsum_{v=1}^{n_{\text{grid}}}\widehat{f}_e(u, v)\ln\left(\frac{\widehat{f}_e(u,v)}{\widehat{f}_t(u,v)}\right)
$$ {#eq-kld}

$$
\begin{aligned}
& \min_{\mu_d,  ~ \sigma^2_d,  ~ \mu_n, ~ \phi_n, ~ \omega_{PBS}} \Big\{ D_{\text{KL}}(\widehat{f_e} ~ \| ~ \widehat{f_t}) \Big\}, ~ \text{subject to:} ~ \mu_d, \sigma^2_d, \mu_n, \phi_n, \omega_{PBS} \in \mathbb{R} \\
& -4 \leq \mu_d \leq 4, \quad 0 \leq \sigma^2_d \leq 6, 10 \leq \mu_n \leq 15000, \quad 0.01 \leq \phi_n \leq 1000, 0 \leq \omega_{PBS} \leq 1
\end{aligned}
$$ {eq-optim}

constitute an optimization problem, where the aim is to find values for the four distributional parameters and the publication bias parameter such that the kullback-leibler loss function ... the divergence between the estimated joint kernel density of simulated theoretical data from the estimated joint kernel density of the empirical data

### Algorithmic Parameter Optimization with Differential Evolution

Objective of finding parameter values for which Kullback-Leibler divergence between the estimated joint kernel density of the simulated theoretical data from the empirical data is minimized, use differential evolution DE [see @storn_differential_1997], which is a simple metaheuristic algorithm for global optimization [@feoktistov_differential_2006]. DE is an evolutionary algorithm based on principles such as mutation, cross over and selection, and requires in comparison to other optimization algorithms only few control parameters that are generally straightforward to select to achieve favorable outcomes [@storn_differential_1997]. Importantly, all parameters of the DE algorithm including the control parameters, the stopping criterion and boundary constraints of the differential evolution algorithm were defined globally for the parameter estimation of all meta-analyses.

To utilize DE for optimization within the SPEEC approach, the R package *RcppDE* [@edelbuettel_rcppde_2022] was employed, implementing the classical algorithm *DE/rand/1* [@storn_differential_1997]. The control parameters for DE were chosen based on the recommendations of @storn_differential_1997 with additional adjustments informed by preliminary testing of simulated data from the simulation framework of SPEEC, setting the population size $NP$ to 150, the mutation constant $F$ to 0.9 and the crossover constant $CR$ to 0.1. In the application of the DE algorithm, we adopted a direct termination criteria approach [@jain_termination_2001; @ghoreishi_termination_2017], with the termination condition being the maximum number of generations. Since there are no universally applicable default values for the maximum number of generations, as it is contingent upon the optimization problem at hand [@jain_termination_2001], the choice for $t_{\text{max}}$ was also informed by preliminary testing of simulated data from the simulation framework of SPEEC. These tests suggested that $t_{\text{max}} = 1000$ is a reasonable decision. In determining the boundaries for the parameter search space, a balance was made between avoiding boundaries that are too wide, which could lead to inefficient exploration of the search space, and ensuring that the boundaries are not too narrow to ensure sufficient coverage of the potential parameters. More specifically, the minima and maxima for all distributional parameters were determined using Maximum Likelihood (see Table XXX), and the boundaries were set slightly above those values to ensure good coverage. 

## Secondary Data Description

To examine the confirmatory hypotheses of this study aiming to provide a preliminary assessment about the viability of the SPEEC method, we use secondary data sourced from previous research by @linden_heterogeneity_2021. The dataset can be accessed both from its original source (see https://osf.io/yr3xd/) and through the OSF and GitHub repositories associated with this project (see section XXX). Furthermore, detailed metadata about the dataset and a transparency statement regarding prior knowledge of the data are provided in the preregistration of this study. This comprehensive dataset comprises both "traditional" meta-analyses and publication bias free registered replication reports. The dataset encompasses a total of 207 research syntheses covering various psychological phenomena. Within this dataset, there are 150 meta-analyses, each subset consisting of 50 meta-analyses from different subfields of psychology (social psychology, organizational psychology, and cognitive psychology). Additionally, the dataset includes 57 registered replication reports, which are particularly relevant for investigating hypotheses $\mathcal{H}_3$ and $\mathcal{H}_4$. For each research synthesis, information on the total sample size and effect size of each primary study was compiled.  The meta-analyses were selected via random sampling, adhering to predefined inclusion criteria and prespecified journals from which the data was sampled from. One crucial inclusion criterion  by @linden_heterogeneity_2021 was that effects must be reported as standardized mean differences (Cohen´s *d* or Hedges´ *g*) or as correlations (Pearson´s *r* or Fisher´s *z*). In cases where a meta-analysis or replication study employed a different effect size measure than Cohen’s *d*, the effect sizes were transformed accordingly [@linden_heterogeneity_2021].

## Statistical Analysis

```{r}
#| echo: false
r_ver <- paste(version$major, version$minor, sep = ".")
```

All statistical analyses were performed using R [version `r r_ver`, @r_core_team_r_2023]. Data and analysis scripts are made available members of Goethe University on the Local Instrastructure for Open Science (LIFOS) and the publicly on the [Open Science Framework (OSF)](https://osf.io/87m9k/?view_only=030c8d1c46474270b5886c4dfc491a78).

Regarding the hypotheses, in which the publication bias parameter $\pbs$ was the dependent variable ($\mathcal{H}_1$, $\mathcal{H}_2$, $\mathcal{H}_4$), beta regression models as implemented in the *betareg* package [@zeileis_betareg_2021] was used to analyse the data. This choice is motivated by the restriction of the parameter space for the publication bias to the standard unit interval, whereby non-normality, skewness and heteroscedasticity can anticipated [@cribari-neto_beta_2010; @smithson_better_2006]. Beta regression is recognized for its adaptability in handling such deviations. We used a logit link for the mean parameter $\mu$ and a identity link for the dispersion paramater that was fixed such that the beta regression model can be described as

$$
\begin{gathered}
\omega_{\text{PBS}_i} \sim  \mathcal{B}(\mu_i, \phi) \\
\log{\bigg(\frac{\mu_i}{1 - \mu_i}\bigg)}=x_i^\top\beta.
\end{gathered}
$$ {#eq-beta-reg}

The independent variables for these three hypotheses were as follows: regarding $\mathcal{H}_1$, the independent variable was the Fisher *z*-transformed correlation coefficient of the correlation between effect size and sample size, where the transformation is defined as $z_r=\tanh^{-1}(r)$. The independent variable for $\mathcal{H}_2$ was the difference $\Delta_{\widehat{\mu}_d}$ between the average effect size estimate of each meta-analysis and the estimated mean parameter of the Gaussian effect size distribution from the SPEEC approach. Lastly, the independent variable for $\mathcal{H}_4$ was a binary indicator specifying the research synthesis type (traditional meta-analysis or multisite replications), with multisite replication studies set as the reference level for regression. The coefficients from the beta-regressions for these hypotheses were estimated using Maximum Likelihood estimation with the BFGS optimizer.

Hypothesis $\mathcal{H}_3$ was aimed at comparing the estimated means of the Gaussian effect size distribution to the average effect sizes to assess whether the presence of effects in mean differences $\Delta_{\widehat{\mu}_d}$ deemed large enough to be considered meaningful, according to specified equivalence bounds $\Delta_{EQ}$, can be rejected [@lakens_improving_2020]. For this, we conducted an equivalence test using the Two One-Sided Tests procedure [@schuirmann_comparison_1987] as implemented in the *TOSTER* R package [@lakens_toster_2023]. To perform the TOST procedure, the $TOSTER$ R package [@lakens_toster_2023] was utilized. We employed Welch´s two-sample *t*-tests for dependent samples with corrected degrees of freedom as the Welch's *t*-test generally offers better control of type I error rates when the samples are heteroscedastic, while maintaining robustness compared to Student's *t*-test when test assumptions are satisfied [@delacre_why_2017]. Furthermore, the choice of a dependent samples test was necessitated by the dependence between the pairs of samples originating from the same underlying data. The equivalence bounds $\Delta_{EQ}$ against which the data was tested were defined by the smallest effect size of interest.

## Smallest Effect Size of Interest

For all four hypotheses, we will establish the smallest effect size of interest (SESOI) based on effect sizes that that can reliably detected, considering the constraints imposed by the sample size resources available for this secondary data analysis [@lakens_performing_2014; @lakens_equivalence_2018]. More specifically, we conducted three simulation-based ($\mathcal{H}_1$, $\mathcal{H}_3$, $\mathcal{H}_4$) and one analytical ($\mathcal{H}_2$) sensitivity power analysis to determine which effect sizes we have at least 80 percent power ($1-\beta=0.8$) to detect, taking into account the constraints of the sample size and a fixed type I error rate $\alpha = .05$ (details see Appendix XXX). We specified the SESOI for $\mathcal{H}_3$ in raw units and all other SESOIs in odds ratios. The SESOI for the equivalence hypothesis will define the equivalence bounds for the TOST procedure ($\Delta_{EQ} = (-0.17, 0.17)$). Table XXX summarises all four SESOIs of the hypotheses.



```{r}
#| echo: false
#| results: asis

df_sesoi <- read_csv(here("preregistration/data/sesois.csv")) |>
    mutate(hypothesis = paste0("$", gsub("H", "\\\\mathcal{H}_", hypothesis), "$")) |>
    mutate(unit = if_else(unit == "OR", glue::glue("${unit}$"), "raw unit"))

nice_table(
    x = df_sesoi, col_names = c("Hypothesis", "SESOI", "Unit"),
    general_fn = "Except for $\\\\mathcal{H}_3$ all SESOIs are defined in terms of odds ratios (OR). The SESOI of $\\\\mathcal{H}_3$ is defined in raw units.",
    caption = "Smallest Effect Sizes of Interest of the Hypotheses"
)

```