---
bibliography: ../references.bib
---

\newpage

```{r}
#| echo: false
source(here::here("R/00_functions.R"))
pacman::p_load(tidyverse, insight, here)
```

# Method

## The `SPEEC` Approach

### Overview

-   Simulation-based approach to estimate publication bias severity and correct potentially biased (inflated) effect sizes under present publication bias based on the joint distribution of effect size and sample size
-   Simulation of theoretical data > joint distribution of effect size and sample size under marginal distributional assumptions > Application of publication bias > empirical kernel density estimation > comparison of empirical and simulated data > loss function
-   Implementation as an open source R package (alpha version) that is already available on GitHub: <https://github.com/jlschnatz/speec>
-   General steps
    -   Simulate samples of joint distribution of effect size and sample size from marginal distributional assumptions
    -   Application of publication bias
    -   KDE for Simulated and Empirical Data
    -   Compare Distributions using KL Divergence
    -   Parameter Optimization via Simulated Annealing

(1) Simulation of random samples from joint probability distribution of effect size and sample size
(2) Application of publication bias
(3) Kernel Density Estimation of drawn theoretical samples and the empirical empirical samples
(4) Compuation of the divergence between the estimated probability of empirical against theoretical data
(5) Algorithmic optimization of bias and distributional parameters via simulated annealing
Simulation of $k$ random samples from the joint distribution of effect size and sample size, subsequent application of publication bias resulting in a subset $(d_i^\prime, n_i^\prime)$ of the inital random samples. 

```{=latex}
\begin{figure}[H]
\caption{Overview of the SPEEC Approach\label{fig:speec}}
```

```{r speec-fig}
#| fig-align: center
#| out-width: 80%
#| echo: false

knitr::include_graphics(here("figures/speec_workflow.png"))

```

```{=latex}
\begingroup
\footnotesize
\textit{Note.} Test.
\endgroup
\end{figure}
```

### Simulation Framework

The marginal distribution for the total sample size $\mathrm{n}$ should be inherently modeled as a discrete distribution. Count data of this nature are commonly modeled using either a Poisson or Negative-Binomial distribution. In various psychological domains, sample size distributions often exhibit considerable variance and skewness [see for example @cafri_meta-meta-analysis_2010; @marszalek_sample_2011; @sassenberg_research_2019; @shen_samples_2011; @szucs_empirical_2017]. Considering this variability and skewness we opted for the Negative-Binomial distribution which can account for variance independently of the mean and thus handle overdispered data effectively. We use the mean-dispersion parametrization where the probability of sucess $p$ and the target number of sucesses $r$ are reparametrized to mean $\mu = r(1-p)/p$ and dispersion $\phi=r$ to model the study-specific total sample sizes $n_i$.

$$
n_1, \,n_2,\, \ldots, \,n_k \quad \text{where} \quad N\stackrel{\mathrm{i.i.d.}}{\sim} \mathcal{NB}(\phi_n, \mu_n) \quad \text{for } i = 1, \ldots, k
$$ {#eq-nb-dist}

Concerning the marginal distribution of the effect size $d$, we assume a normal distribution with mean $\mu_d$ and variance $\sigma^2_d$, where the effect size itself is assumed to originate from a common two-sample independent *t*-test design. To address the increasing precision in estimating the true effect size mean $\mu_d$ as sample size increases, contributing to the characteristic funnel shape of the effect size-sample size distribution, we compute the variance of the mean differences $\bar{x}_{i1} - \bar{x}_{i2}$, from which the effect sizes originate in this type of design. Subsequently, we derive a normalization factor $\gamma_i$ by dividing each individual variance $\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}$ with overall mean of those variances ensuring that $\bar{\gamma}=1$.

$$
\begin{gathered}
\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}=\sigma_d^2/n_i \\
\gamma_i=\frac{\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}}{\sum_{i=1}^{k}\sigma^2_{\bar{x}_{i1} - \bar{x}_{i2}}/k} \\
\end{gathered}
$$ {#eq-normalization}

With this normalization factor, the total variance of the individual variances of the individual variances is $\text{Var}(\gamma \cdot \sigma^2_d) = \sigma^2_d$. The study-specific effect sizes $d_i$ are subsequently modeled as

$$
d_1, \,d_2, \,\ldots, \,d_k \quad \text{where} \quad D \stackrel{}{\sim} \mathcal{N}(\mu_d, \gamma_i\cdot\sigma_d^2) \quad \text{for} \quad i = 1, \ldots, k 
$$ {#eq-norm-dist}

### Application of Publication Bias

Following the simulation step of sampling $k$ individual studies from the joint distribution of effect size and sample size conditional on the parameters, the subsequent step entails applying publication bias to these samples. As mentioned in the introduction, we operationalize publication bias in terms of the likelihood of a study being published conditional on the statistical significance of its results. Translated to this simulation setting we can calculate the two-tailed *p*-value of each individual study $i$ from the random samples of effect size $d_i$ and sample size $n_i$. We presume that individual studies $i$ originate from a balanced sample size design, where the group sample sizes $n_{1i}$ and $n_{2i}$ are defined as $n_i/2$ when the total sample size is even. If the total sample size is odd, the group sample sizes are determined as the ceilinged $\lceil n_i/2 \rceil$ and and floored $\lfloor n_i/2 \rfloor$ values, respectively. To calculate the *p*-value $p_i$ of each simulated study, derived from *t*-value $t_i$

$$
t_i=\Big\lvert \frac{d_i}{\sqrt{1/n_{1i} +1/n_{2i}}} \Big\rvert 
$$ {#eq-t-value}

$$
p_i=2 \cdot P(t_i~|~df_i)
$$ {#eq-p-value}

where $P(t_i , \, df_i)$ is the cumulative central *t*-distribution with degrees of freedom $df_i=n_{1i}+n_{2i}-2$. Given each *p*-value $p_i$, publication bias is introduced by assigning each study $i$ a weight

$$
\omega_{\text{PBS}_i}(p_i)= \begin{cases}  
\omega_{\text{PBS}} & \text{for}~~p_i \geq \alpha \\
1 & \text{otherwise}\end{cases}
$$ {#eq-pbs}

given $\omega_{\text{PBS}} \in \mathbb{R} : 0 \leq \omega_{\text{PBS}} \leq 1$. This weight denotes the probability of a study $i$ being selected conditional on the *p*-value and the type I error rate $\alpha$. If a study $i$ was not statistically significant ($p_i \geq \alpha$), a publication bias weight $\omega_{\text{PBS}}$ is assigned, else the probability of a study being selected is 1, indicating no publication bias. Thus, the publication bias parameter $\omega_{\text{PBS}}$ denotes the relative probability of a nonsignificant study being selected relative relative to a significant study being selected.We assume a fixed type I error rate for all simulated studies at the common threshold of $\alpha = .05$.

Following the computation of publication bias weight $\omega_{\text{PBS}_i}$ for each study $i$, the likelihood of a study being selected can be expressed as $\mathbb{P}(S_i = 1)= \omega_{\text{PBS}_i}$. Here, $S_i$ serves as a binary indicator function, signifying whether study $i$ is selected during the publication bias process.

$$
S_i = 
\begin{cases}  
  0 & \text{study not selected} \\
  1 & \text{study selected}
\end{cases}
$$ {#eq-id-fun}

Subsequently the two resulting subsets $(d_i^\prime, \,n_i^\prime)$ and $(d_i^{\prime\prime}, \,n_i^{\prime\prime})$ from the initial random sample can be defined as

$$
\begin{gathered}
(d_i^\prime, n_i^\prime) = (d_i, n_i ~ | ~ S_i = 1) \quad \text{for} \quad i = 1, \dots, k^\prime \quad \text{and} \\
(d_i^\prime\prime, n_i^\prime\prime) = (d_i, n_i ~ | ~ S_i = 0) \quad \text{for} \quad i = 1, \dots, k^{\prime\prime}
\end{gathered}
$$ {#eq-subsets}

One challenge lies in the fact that, ceteris paribus, as publication bias becomes more severe (i.e., the lower values $\omega_{\text{PBS}}$), the lower $k^\prime$ will be in comparison to the original number of simulations $k$ leading to a precision loss for the estimation of the parameters. The migitate this, the process outlined above (@eq-nb-dist - @eq-subsets) repeated a second time with an adjusted number of simulations $k_{\text{adj}} = \big\lceil {k^2}/{k'} \big\rceil$, ensuring that the number of selected studies $k^\prime_{\text{adj}}$ is roughly equal across the entire range of $\omega_{\text{PBS}}$.

### Formulation as an Optimization Problem

After the selection of the subset $(d_i^\prime, \,n_i^\prime)$ from the simulated theoretical random samples conditional on the publication bias parameter $\omega_{\text{PBS}}$ and the parameters of the marginal distributions for the effect size and sample size, the following step involves a statistical comparison between the this subset and the empirical meta-analytical data by means of the of the Kullback-Leibler divergence [@kullback_information_1951]. This assessment aims to quantify how closely the distributions of the theoretical samples align with the empirical data.

In the subsequent phase, the statistical dissimilarity between the empirical meta-analytical data and the selected subset $(d_i^\prime, \,n_i^\prime)$ of the simulated theoretical samples is evaluated by means of the Kullback-Leibler divergence [@kullback_information_1951]. This assessment aims to quantify how closely the distributions of the theoretical samples align with the empirical data. To achieve this, the joint kernel density is estimated for both theoretical simulated and empirical data is estimated using a bivariate standard Gaussian kernel that is evaluated on a square grid, with a grid size of $n_{\text{grid}}=2^7+1$ equidistant grid points in each dimension. The bounds of the square grid are determined based on the empirical meta-analytical data. To the define these bounds the maximum likelihood estimates for the parameters of the marginal distribution of effect size ($\hat{\mu}_d,\hat{\sigma}^2_d$) and sample size ($\hat{\phi}_n, \hat{\mu}_n$) are computed. Then, utilizing these estimates, the quantiles derived from inner 99th percentile ($p_1 = .005, p_2=.995$) of the cumulative distribution are computed. Subsequently, the bounds are defined as the absolute minimum and maximum of these quantiles and then range of the empirical meta-analytical data, respectively.

$[Q_n(p_1 \, | \, \hat{\phi}_n, \hat{\mu}_n), \, Q_n(p_2 \, | \, \hat{\phi}_n, \hat{\mu}_n)]$

$[Q_d(p_1 \, | \, \hat{\mu}_d, \hat{\sigma}^2_d), \, Q_d(p_2 \, | \, \hat{\mu}_d, \hat{\sigma}^2_d)]$

Finally, KL-divergence is calculated from the kernel density estimates for both empirical and simulated data, providing a measure of their statistical distance.

$$
D_{\text{KL}}(\widehat{f_e} ~ \lVert ~ \widehat{f_t})=\nsum_{u=1}^{g}\nsum_{v=1}^{g}\widehat{f_e}(u, v)\ln\left(\frac{\widehat{f_e}(u,v)}{\widehat{f_t}(u,v)}\right)
$$ {#eq-kld}

$$
\begin{aligned}
& \min_{\mu_d, \sigma^2_d, \mu_n, \phi_n, \omega_{PBS}} \Big\{ D_{\text{KL}}(\widehat{f_e} ~ \| ~ \widehat{f_t}) \Big\}, ~ \text{subject to:} ~ \mu_d, \sigma^2_d, \mu_n, \phi_n, \omega_{PBS} \in \mathbb{R} \\
& -4 \leq \mu_d \leq 4, \quad 0 \leq \sigma^2_d \leq 6, 10 \leq \mu_n \leq 15000, \quad 0.01 \leq \phi_n \leq 1000, 0 \leq \omega_{PBS} \leq 1
\end{aligned}
$$

constitute an optimization problem, where the aim is to find values for the four distributional parameters and the publication bias parameter such that the kullback-leibler loss function ... the divergence between the estimated joint kernel density of simulated theoretical data from the estimated joint kernel density of the empirical data

### Algorithmic Parameter Optimization with Differential Evolution

Objective of finding parameter values for which Kullback-Leibler divergence between the estimated joint kernel density of the simulated theoretical data from the empirical data is minimized, use differential evolution DE [see @storn_differential_1997], which is a simple metaheuristic algorithm for global optimization [@feoktistov_differential_2006]. DE is an evolutionary algorithm based on principles such as mutation, cross over and selection, and requires in comparison to other optimization algorithms only few control parameters that are generally straightforward to select to achieve favorable outcomes [@storn_differential_1997]. Importantly, all parameters of the DE algorithm including the control parameters, the stopping criterion and boundary constraints of the differential evolution algorithm were defined globally for the parameter estimation of all meta-analyses.

To utilize DE for optimization within the SPEEC approach, the R package *RcppDE* [@edelbuettel_rcppde_2022] was employed, implementing the classical algorithm *DE/rand/1* [@storn_differential_1997]. The control parameters for DE were chosen based on the recommendations of @storn_differential_1997 with additional adjustments informed by preliminary testing of simulated data from the simulation framework of SPEEC, setting the population size $NP$ to 150, the mutation constant $F$ to 0.9 and the crossover constant $CR$ to 0.1. In the application of the DE algorithm, we adopted a direct termination criteria approach [@jain_termination_2001; @ghoreishi_termination_2017], with the termination condition being the maximum number of generations. Since there are no universally applicable default values for the maximum number of generations, as it is contingent upon the optimization problem at hand [@jain_termination_2001], the choice for $t_{\text{max}}$ was also informed by preliminary testing of simulated data from the simulation framework of SPEEC. These tests suggested that $t_{\text{max}} = 1000$ is a reasonable decision. In determining the boundaries for the parameter search space, a balance was made between avoiding boundaries that are too wide, which could lead to inefficient exploration of the search space, and ensuring that the boundaries are not too narrow to ensure sufficient coverage of the potential parameters. More specifically, the minima and maxima for all distributional parameters were determined using Maximum Likelihood (see Table XXX), and the boundaries were set slightly above those values to ensure good coverage. 

## Secondary Data Description

The data used for the assement of the predictions of the four hypotheses stem from extensive meta-analytical dataset from previous work by @linden_heterogeneity_2021.

The dataset contains a total of 207 research synthesis on 207 psychological phenomena. Subdivided into 150 "traditional" meta-analysis which contain 3 times 50 meta-analyses from different subfields of psychology (social psychology, organizational psychology and cognitive psychology).

Additionaly 57 large-scale replication studies and registered reports, which are important for the investigation of $\mathcal{H}_3$ and $\mathcal{H}_4$.

-   To investigate predictions of the hypotheses we use secondary data by Linden & Hönekopp (2021)
-   Extensive meta-analytical database, containing both traditional meta-analysis as well as multisite replication studies and registered reports (which are absent of publication bias)
-   The subset of traditional meta-analysis consists of 150 total meta-analyses
-   The subset of traditional meta-analysis from different subfields of psychology (Social psychology, organizational psychology and cognitive psychology; 50 each)
-   additionally 57 multisite replication studies and registered reports
-   for each meta-analysis information about the total sample size and effect size of each primary study was collected
-   Random sampling to determine the meta-analysis for the study (within the inclusion criteria)
-   One important inclusions criteria of Linden & Hönekopp (2021), effects had to be reported as standardized mean differences (Cohen´s d or Hedges´g) or as correlations (Pearson´s r or Fisher´s z)
-   In instances where a meta-analysis or replication study used a different effect size measure than Cohen’s d, the effect sizes were transformed accordingly

## Statistical Analysis

```{r}
#| echo: false
r_ver <- paste(version$major, version$minor, sep = ".")
```

All statistical analyses were performed using R [version `r r_ver`, @r_core_team_r_2023]. Data and analysis scripts are made available members of Goethe University on the Local Instrastructure for Open Science (LIFOS) and the publicly on the [Open Science Framework (OSF)](https://osf.io/87m9k/?view_only=030c8d1c46474270b5886c4dfc491a78).

Regarding the hypotheses, in which the publication bias parameter $\omega_{\text{PBS}}$ was the dependent variable ($\mathcal{H}_1$, $\mathcal{H}_2$, $\mathcal{H}_4$), beta regression models as implemented in the *betareg* package [@zeileis_betareg_2021] was used to analyse the data. This choice is motivated by the restriction of the parameter space for the publication bias to the standard unit interval, whereby non-normality, skewness and heteroscedasticity can anticipated [@cribari-neto_beta_2010; @smithson_better_2006]. Beta regression is recognized for its adaptability in handling such deviations. We used a logit link for the mean parameter $\mu$ and a identity link for the dispersion paramater that was fixed such that the beta regression model can be described as

$$
\begin{gathered}
\omega_{\text{PBS}_i} \sim  \mathcal{B}(\mu_i, \phi) \\
\log{\bigg(\frac{\mu_i}{1 - \mu_i}\bigg)}=x_i^\top\beta.
\end{gathered}
$$ {#eq-beta-reg}

The independent variables for these three hypotheses were as follows: regarding $\mathcal{H}_1$, the independent variable was the Fisher *z*-transformed correlation coefficient of the correlation between effect size and sample size, where the transformation is defined as $z_r=\tanh^{-1}(r)$. The independent variable for $\mathcal{H}_2$ was the difference $\Delta_{\widehat{\mu}_d}$ between the average effect size estimate of each meta-analysis and the estimated mean parameter of the Gaussian effect size distribution from the SPEEC approach. Lastly, the independent variable for $\mathcal{H}_4$ was a binary indicator specifying the research synthesis type (traditional meta-analysis or multisite replications), with multisite replication studies set as the reference level for regression. The coefficients from the beta-regressions for these hypotheses were estimated using Maximum Likelihood estimation with the BFGS optimizer.

Hypothesis $\mathcal{H}_3$ was aimed at comparing the estimated means of the Gaussian effect size distribution to the average effect sizes to assess whether the presence of effects in mean differences $\Delta_{\widehat{\mu}_d}$ deemed large enough to be considered meaningful, according to specified equivalence bounds $\Delta_{EQ}$, can be rejected [@lakens_improving_2020]. For this, we conducted an equivalence test using the Two One-Sided Tests procedure [@schuirmann_comparison_1987] as implemented in the *TOSTER* R package [@lakens_toster_2023]. To perform the TOST procedure, the $TOSTER$ R package [@lakens_toster_2023] was utilized. We employed Welch´s two-sample *t*-tests for dependent samples with corrected degrees of freedom as the Welch's *t*-test generally offers better control of type I error rates when the samples are heteroscedastic, while maintaining robustness compared to Student's *t*-test when test assumptions are satisfied [@delacre_why_2017]. Furthermore, the choice of a dependent samples test was necessitated by the dependence between the pairs of samples originating from the same underlying data. The equivalence bounds $\Delta_{EQ}$ against which the data was tested were defined by the smallest effect size of interest.

## Smallest Effect Size of Interest

For all four hypotheses, we will establish the smallest effect size of interest (SESOI) based on effect sizes that that can reliably detected, considering the constraints imposed by the sample size resources available for this secondary data analysis [@lakens_performing_2014; @lakens_equivalence_2018]. More specifically, we conducted three simulation-based ($\mathcal{H}_1$, $\mathcal{H}_3$, $\mathcal{H}_4$) and one analytical ($\mathcal{H}_2$) sensitivity power analysis to determine which effect sizes we have at least 80 percent power ($1-\beta=0.8$) to detect, taking into account the constraints of the sample size and a fixed type I error rate $\alpha = .05$ (details see Appendix XXX). We specified the SESOI for $\mathcal{H}_3$ in raw units and all other SESOIs in odds ratios. The SESOI for the equivalence hypothesis will define the equivalence bounds for the TOST procedure ($\Delta_{EQ} = (-0.17, 0.17)$). Table XXX summarises all four SESOIs of the hypotheses.

```{r}
#| echo: false
#| results: asis
df_sesoi <- read.csv(here("preregistration/data/sesois.csv")) |>
  mutate(hypothesis = paste0("$", gsub("H", "\\\\mathcal{H}_", hypothesis), "$")) |>
  mutate(unit = if_else(unit == "OR", glue::glue("${unit}$"), "raw unit")) 

nice_table(
  x = df_sesoi,
  caption = "Smallest Effect Sizes of Interest of the Hypotheses",
  col_names = c("Hypothesis", "SESOI", "Unit"),
  general_fn = "$OR$ = Odds ratio"
)
```
