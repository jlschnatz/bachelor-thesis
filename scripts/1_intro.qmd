---
bibliography: ../bibliographyreferences.bib
---

\newpage

# Introduction

<!--# Epistemological Intro -->

<!--# Topics: Epistemological background, Definition publication bias historical account, consequences (in light of the replication crisis) -->

<!--# Question? More into causes of publication bias? is this missing?-->

Science is commonly conceived as a cumulative enterprise [@cooper_research_2019] with the overarching goal of attaining robust knowledge about the world [@kitcher_advancement_1993]. Within this landscape, researcher often study the same phenomen, driven by the idea that generalizing and synthesizing findings from individual studies contributes to advancement of knowledge. However, this premise hinges on the underlying assumption, that the available scientific literature is representative for all conducted research [@song_dissemination_2010]. Contrary to this, researchers have pointed out for over half a century that results of published studies differ systematically from unpublished studie [@smart_importance_1964; @sterling_publication_1959; @bozarth_signifying_1972; @bakan_test_1966]. This discrepancy arises as the publication of a study often hinges on the direction or strength of its findings [@dickersin_existence_1990; \
@dickersin_publication_1993] and is collectively known as *publication bias*. Especially in a publishing culture that prioritizes novelty and positive results [@nosek_scientific_2012], many statistically nonsignificant studies end up in the "file-drawer" and never get published [@rosenthal_file_1979].

The ramifications of publication bias are severe, culminating in inflated meta-analytical effect sizes [@stanley_detecting_2021; @franco_publication_2014], heightened false-positive rate [@kicinski_how_2014; @munafo_how_2010], thereby increasing the risk of erroneous conclusions that may jeopardize the validity of research [@begg_publication_1994]. These ramifications become especially relevant in the light of recent large-scale replication projects providing evidence for non-replicability of many psychological findings [@open_science_collaboration_estimating_2015; @klein_many_2018; @camerer_evaluating_2018; @klein_investigating_2014; @ebersole_many_2016; @ebersole_many_2020]. This underscores why publication bias identified as a major threat to replicable science [@munafo_manifesto_2017] and thus a considered as a significant contributor to the replication crisis [@renkewitz_how_2019]. Given the myriad of issues associated with publication bias and its widespread impact, there has been considerable attention directed towards investigating methodologies to detect publication bias.

<!--# How can we measure publication bias? High-level summary -->

In this regard, there has been a great deal of research on publication bias detection techniques with numerous statistical methods developed over the past 50 years [@marks-anglin_historical_2020]. These statistical techniques can generally be classified into methodologies that operate with *p*-values and methodologies that are based on the relationship between effect size and sample size [@vevea_publication_2019]. While both categories encompass highly sophisticated statistical techniques (CITATION?), a straightforward and frequently described method, that has been associated with publication bias, involves examining the correlation between effect size and sample size. Additionally, this method encapsulates the central ideas of other approaches, such as Begg's rank correlation [@begg_operating_1994], Egger's regression [@egger_bias_1997], and its proposed variants [for an overview see @song_dissemination_2010], all rooted in the relationship between effect size and sample size.

## Reasoning of the n-ES correlation

The central tenets of the correlation of effect size and sample size as an indicator of publication bias originate from the concepts of the funnel plot and its assemmetry under the influence of publication bias that was introduced by @light_summing_1984. When multiple studies investigate of common underlying effect, the empirical effect sizes (for example Cohen´s *d* or Fisher-z transformed *r*) follow a normal distribution and fluctuate around the true effect size. Due to sampling error, the lower the sample sizes of individual studies, the less precision they exhibit to estimate the true effect size (i.e., larger standard error), leading to a larger variation around the true effect size. In the absence of publication bias this will result in a symmetric funnel shaped distribution [@light_summing_1984]. However, when the publishing of studies is contingent on their statistical significance, the funnel plot will be assymetric. As the statistical significance of *p*-values is jointly determined by the sample size (i.e., standard error of the test statistic) and effect size (i.e., test statistic), larger effect sizes attain statistical significance with smaller sample sizes, while smaller effect sizes necessitate larger sample sizes to be significant. Consequently, the negative correlation between effect size and sample size emerges because the threshold for the smallest effect sizes that is statistically significant decreases with increasing sample size [@linden_publication_2024]. The correlation between effect size and sample size has been described and attributed to publication bias extensively in various research including psychology [@kuhberger_publication_2014; @fritz_comprehensive_2013; @levine_sample_2009], evolutionary biology and ecology [@palmer_detecting_1999; @moller_how_2001; @jennions_publication_2002; @jennions_relationships_2002], political science [@gerber_testing_2001] and educational research [@slavin_effective_2008; @slavin_relationship_2009] Its prevalence across these disciplines highlights its role as a widely recognized and applied tool for the detection of publication bias.<!--# Challenges of the n-ES correlation -->

## Methodological Concerns

Despite the significant attention and prevalent use of the effect size-sample size correlation in various research fields for detecting publication bias, coupled with its frequent acknowledgment as a valid indicator of such bias, there exist persisting methodological concerns. As I will argue in the next section, these concerns have only been partially discussed and adressed in the existing literature and may compromise the validity of the interpretation of the correlation as an indicator of publication bias. To illustrate the inherent challenges of the effect size-sample size correlation as an indicator of publication bias, we simulated a set 10000 primary studies\[\^1\] on the same effect underlying effect and varied different parameters that contribute to its limitations (see figure 1). This includes the true effect size $\delta: \{0,~0.4\}$, the extent of publication bias $\omega_{PBS}:\{0,~1\}$ or how much less likely studies with non-significant *p*-values are compared to studies with significant results (in this extreme case either non-significant studies are not published at all, or there are no differences between non-significant and significant studies), the signedness of the effect size ($d$ and $\lvert ~ d ~ \rvert$) and the type of hypothesis (directional $\mathcal{H_1}:\theta>0$ and non-directional $\mathcal{H_1}: \theta \neq0$)

<!--# Use of unsignes effect sizes -->

Firstly, it is common practice to use unsigned effect sizes to estimate the n-es correlation [e.g., @slavin_relationship_2009; @levine_sample_2009; @weinerova_published_2022; @kuhberger_publication_2014]. Whilst this is very common, it has only recently been acknowledged that the use of unsigned effect sizes can lead to a statistical artefact resulting in a small negative correlation, even in the absence of publication bias [@linden_publication_2024]. As depicted in figure 1 (leftmost column compared to second leftmost column), the artificial correlation in absence of publication bias is most severe when the true effect is close to zero, as this condition leads to the most sign changes. Especially when considering that effect sizes in psychology are typically smaller than common benchmarks [@weinerova_published_2022; @lovakov_empirically_2021], and thus it is likely that the true effect sizes of psychological phenomen are often small, this excaberates the problem of the statistical artifact.

<!--# Nil null hypothesis -->

If a negative correlation can emerge even in the absence of publication bias, this raises questions about the appropriate null hypotheses to test against, specifically, what correlation we would expect if publication bias is absent [@linden_publication_2024]. There has been a long tradition in null hypothesis testing to use the nil null hypothesis [@cohen_earth_1994], which states that a population parameter is exactly zero. This is also very common in studies that have used the n-es correlation together with unsigned effect sizes [@kuhberger_publication_2014; @slavin_relationship_2009; @levine_sample_2009; @weinerova_published_2022] and underscores a lack of thorough consideration for the potential falseness of this hypothesis in such cases. The determination of an appropriate null hypothesis for testing in these scenarios, however, remains uncertain.

<!--# Why not signed effect sizes? -->

Utilizing *signed* effect sizes may seem like a straightfoward solution to the aforementioned problems, however, it introduces its own set of challenges. Especially, when researcher make non-directional hypothesis and where the true effect size is close to zero, the distribution of the signed effect sizes and sample size will be symmetrically hollowed out under the influence of publication bias. This symmetry (see Figure 1 H) will result in the correlation being zero, leading to a false negative - a failure to detect publication bias when it is present.

<!--# Conceptual challenges (linearity assumption, doesn´t capture what is aims to capture -->

Apart from these more statistical challenges, there is also a more conceptual challenge.

-\> n-es correlation somewhat misses the point of publication bias

-   Fails to capture the point of publication bias -\> depending on statistical significance -\> effect size and sample size correlation only indirectly captures the censorship process of non-significant studies

-   as figure shows, the non-linear relationship → critical test statistic value under which p \< alpha nonlinear -\>

-   @harrer_doing_2021

-   **Questionable linearity assumption**

    -   Pearson correlation assumes that under publication bias → linear relationship between effect size and sample size → the higher the effect size the lower the required sample size for the effect to be significant and vice versa may given the (false) impression that this assumptions holds

    -   But publication bias operates under statistical significance (which is most dominantly → if p-value smaller than alpha threshold; CITATION) → as figure shows, the non-linear relationship → critical test statistic value under which p \< alpha nonlinear

    -   Spearman correlation loosens the assumption of a linear effect in that the relationship has to be only strictly monotic → but still: this is not how publication bias operates

## Limitations

-   Poor performance under effect size heterogeneity
-   Oftentimes only indirectly capture publication bias (small-study effects)

## The Present Study

Research questions: - How does publication bias influence the joint probability distribution of effect size and sample size‚? - How can the magnitude of publication bias in meta-analyses be estimated and effect sizes under publication bias be corrected from the joint distribution of sample size and effect size


The present study introduces $\mathrm{SPEEC}$ ($\mathbfsfup{S}$imulation-based $\mathbfsfup{P}$ublication bias $\mathbfsfup{E}$stimation and $\mathbfsfup{E}$ffect size $\mathbfsfup{C}$orrection), a novel simulation-based framework to assess the extent of publication bias in meta-analyses and estimate and correct effect sizes under the presence of publication bias. 

utilizing/leveraging the joint distribution effect size and sample size, and how publication bias influence the joint distribution

Two-fold objective of study: first main goal, detailed description and introduction of SPEEC, second goal: proof of concept / feasability study to preliminary assess introduced method using empirical meta-analytical data.

For this, we use existing secondary meta-analytical data that has been collected by @linden_heterogeneity_2019. Includes both traditional meta-analyses as well as registered replication reports.

set of predictions/hypotheses that should hold true if the approach works in principle,

Thesis is strucured the following way, short introduction of the SPEEC method, then explanation of the hypotheses that are tested to assess the SPEEC approach (proof of concept), then detailed explanation of the SPEEC method (as it is and possible extensions), results of the hypotheses

---

Short primer of SPEEC:

The central idea of the SPEEC method is the explicit formulation of a generative model of publication bias that incorporates assumptions about the marginal distributions of effect size and sample size as well as how publication bias impacts the their joint distribution. 

The extent of publication bias is modeled by a publication bias parameter $\pbs$ that captures the probability of a statistically non-significant study being selected (i.e., published relative) relative to a significant study being selected.

From this generative publication bias model, simulation of theoretical data 

More specifically, distributional parameters -> marginal distribution of effect size and sample size + publication bias parameter, gibt das ausmaß von publication bias an -> relative likelihood of statistically significant studies 

Publication bias parameter


- Central idea: explicitely model generative process of publication bias -> with specific assumption about the marginal distributions of effect size and sample size and how publication bias operates
- Generative model of publication bias -> simulate theoretical data from generative model (this generative model has distributional parameters that reflect the marginal distribution of sample size and effect size as well as publication bias parameter)
- Compare simulared data from generative model to empirical meta-analytical data -> determine the divergence of the estimated bivariate kernel density of the theoretical data from the empirical data (KL-divergence) -> this serves as a loss function -> formulation as an optimization problem 
- Find parameters for which the KL-divergence is minimized


Simulation framework: simulation of effect size-sample size data that is sampled from theoretical model conditional on marginal distributional assumptions of effect size (Gaussian distribution) and sample size (Negative_Binomial distribution). Introduction of publication bias parameter, which can be defined as the relative likelihood of a individual statistically non-significant study being published relative to a statistically significant study. Application of publication bias on the simulated joint distribution samples (effect size sample size). Comparison between empirial data and simulated samples from theoretical model in terms of the divergence between their respetice kernel density estimates. Using this framework -> formulation as an optimization problem -> parameter optimization regarding the marginal distributional parameters and the publication bias parameter to optimize the objective function.

The central idea of the SPEEC method involves

simulation of theoretical model that integrates assumptions about the marginal distribution of effect size and sample size and how publication bias influences the joint distribution

simulation of assumption how publication bias operates

simulation of random samples from theoretical

-   Simulation-based approach to estimate publication bias severity and correct potentially biased (inflated) effect sizes under present publication bias based on the joint distribution of effect size and sample size
-   Simulation of theoretical data > joint distribution of effect size and sample size under marginal distributional assumptions > Application of publication bias > empirical kernel density estimation > comparison of empirical and simulated data > loss function

- Simulation of random samples from theoretical publication bias model which integrates assumptions about the marginal distribution of effect size and sample size and how publication bias influences the joint distribution
- Comparison between random samples for theoretical model to real data → compute loss function as the statistical divergence between the estimated joint density distribution
- Iterative algorithmic optimization (differential evolution) of distributional parameters and publication bias parameter to minimize the loss function

## Confirmatory Hypotheses
   
To assess the SPEEC method, a set of four theoretical predictions are derived, that constitute the hypotheses of this study. These hypotheses serve as benchmarks for assessing the viability of the proposed method and are therefore expected to hold true if the approach works in principle. If the predictions fail to be corroborated by the empirical meta-analytical data, this would raise concerns about the viability of the SPEEC method and necessitate a further review of its implementation.

<!-- Hypothesis I (still open) -->

Firstly, we conducted a direct comparison between the correlation of effect size and sample size, serving as an indicator of publication bias, and the publication bias parameter $\pbs$ estimated within the SPEEC method. It can be expected that the estimated publication bias parameter $\epbs$ is positively associated with the Fisher *z*-transformed Spearman correlation coefficients of the association between unsigned effect size and sample size in each meta-analysis. In other words, when the proposed method estimates high publication bias (i.e., low probabilities for $\epbs$ ) it is expected that the correlation coefficients for each meta-analysis to be more negative and conversely. In statistical terms, this implies that the regression coefficient $\beta_{z_{r_s}}$ is expected to be greater than zero.

$$
\begin{gathered}
\hypothesis{i}{0}: \beta_{z_{r_s}} \leq 0 \\
\hypothesis{i}{1}: \beta_{z_{r_s}} > 0
\end{gathered}
$$ {#eq-h1}

<!-- Hypothesis II (finished!) -->

In cases where substantial publication bias is present within the scientific literature of a particular research phenomenon, and the true effect size is precisely zero ($\delta=0$), the distribution of effect size and sample size exhibits increased symmetric sparsity around zero in areas where individual studies would not be statistically significant for a given effect size and sample size [@light_summing_1984]. This is explained by the fact that only studies with either large positive or large negative effects will be statistically significant and consequently have a higher likelihood of being published in the presence of publication bias. Because of this symmetry for a true effect size of zero, the average effect size $\widehat{\delta}$ should not be biased since negative and positive effects should, in theory, mutually cancel each other out. Consequently, the difference $\Delta_{\mu_d}$ between the average effect size $\widehat{\delta}$ and the estimated mean parameter $\mu_d$ of the effect size distribution from the SPEEC approach should remain invariant independent of the magnitude of publication bias. However, when the true effect size exceeds zero ($\delta>0$), publication bias leads to an overestimation of the true effect (i.e. $\widehat{\delta}>\delta$), and conversely, overestimation in the opposite direction (i.e., $\widehat{\delta} < \delta$) when $\delta<0$. If the estimated mean parameter $\widehat{\mu}_d$ of the Gaussian effect size distribution obtained from the *SPEEC* approach is a more accurate estimate of the true effect size $\delta$ in the presence of publication bias compared to the mean effect size $\widehat{\delta}$, it follows from the prior reasoning that a curvilinear, inverted U-shaped pattern can be expected between the difference $\delta_{\mu_d}$ of these two parameters and the publication bias parameter $\pbs$. In other words, when the mean difference $\Delta_{\mu_d}$ is approaching zero, publication bias severity is expected to decrease (indicated by larger values for $\pbs$). Conversely, when the difference $\Delta_{\mu_d}$ diverges from zero in both negative and positive directions, publication bias severity is expected to increase (i.e. lower values for $\pbs$). In statistical terms, for the second hypothesis of this study $\hypothesis{i}{}$, the quadratic regression term $\beta_{\Delta_{\mu_d}}$ is expected to be smaller than zero.

$$
\begin{gathered}
\hypothesis{i}{0}: \quad \beta_{\Delta_{\mu_d}} \geq 0 \\ 
\hypothesis{i}{1}: \quad \beta_{\Delta_{\mu_d}} < 0
\end{gathered}
$$ {#eq-h2}

<!-- Hypothesis III -->

Registered reports are an alternative two-stage publishing model, where study protocols are submitted, peer-reviewed and in-princple accepted prior to data collection [@chambers_past_2022; @nosek_registered_2014]. In-princple accepted studies are then published regardless of the outcome of the study, because the decision to publish was made before the results of the studies are known, which eliminates publication bias [@simons_introduction_2014; @chambers_past_2022]. Consequently, the effect sizes within these multisite replication projects and registered replication reports that are publishied within this framework cannot be biased by publication bias. Following this reasoning, in the third hypothesis of the present study $\hypothesis{iii}{}$, it can expected that the average effect size of a registered replication report $\widehat{\delta}$ to be equivalent to the mean parameter of the Gaussian effect size distribution $\mu_d$ that is estimated using the SPEEC approach within specified equivalence bounds $\Delta_{EQ} = \{\Delta_L, ~ \Delta_U\}$. The equivalence bounds are defined by the smallest effect size of interest for this study [@lakens_equivalence_2018] and are set to $\Delta_{EQ}=\{-0.18, 0.18\}$ (see section [Smallest Effect Size of Interest] for the rationale of this decision).

$$
\begin{gathered}
\Delta_{\mu_d}=\widehat{\delta}-\widehat{\mu}_d \\
\hypothesis{iii}{01}: \Delta_{\mu_d} \leq \Delta_{L} \quad \cap \quad \hypothesis{iii}{02}: \Delta_{\mu_d} \geq \Delta_{U} \\
\hypothesis{iii}{1}: \Delta_L >\Delta_{\mu_d} > \Delta_U
\end{gathered}
$$ {#eq-h3}

<!-- Hypothesis IV -->

In theory, one could posit that the publication bias parameter $\pbs$ in registered replication reports should precicely equal one, as individual replication studies are almost always predetermined in the registration of the study and included in the final report independent of their statistical outcomes. However, due to the inherent upper limits of one for the publication bias parameter $\pbs$, testing this point prediction would not be sensible within a null hypothesis testing framework. Instead, a relative comparison between traditional meta-analyses and registered replication reports allows for making testable predictions. More specifically, it can be expected that the publication bias parameter $\pbs$ is greater for multisite replication studies in comparison to traditional meta-analysis. This indicates that relative to individual statistically non-significant primary studies within traditional meta-analyses, statistically non-significant individual replication studies should have a greater likelihood of being of being included in the the final registered replication report (and thus being published). In statistical terms, when the regressor is a binary indicator of the type of research synthesis with the reference level being the publication biased absent multisite replication studies ($MR$) and the outcome is the estimated publication bias parameter $\pbs$, the regression coefficient $\beta_{MR}$ can be expected to be greater than zero.

$$
\begin{gathered}
\hypothesis{iv}{0}: \quad \beta_{MR} \leq 0 \\
\hypothesis{iv}{1}: \quad \beta_{MR} > 0
\end{gathered}
$$ {#eq-h4}