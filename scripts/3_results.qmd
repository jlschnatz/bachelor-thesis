---
bibliography: ../bibliography/tidy_references.bib
---

```{r}
#| message: false
#| echo: false

knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
pacman::p_load(insight, glue, tidyverse, here, parameters, broom, betareg, performance, marginaleffects, datawizard)
source(here("R/functions.R"))
```

# Confirmatory Results Assessing SPEEC

```{r dispersion-w-wpbs}

m_phi <- read_rds(here("data/src/model_dispersion.rds")) |>
    tidy(conf.int = TRUE) |>
    mutate(ci = format_ci(conf.low, conf.high)) |>
    filter(component == "precision") |>
    mutate(across(c(estimate, std.error, statistic), ~format_value(.x, digits = 2))) |>
    mutate(p.value = format_p(p.value, name = "$p$")) |>
    select(b = estimate, se = std.error, z = statistic, ci, p = p.value)
  
string_phi <- glue("$\\widehat{\\phi}$ = {{m_phi$b}}, {{m_phi$ci}}, $SE$ = {{m_phi$se}}, $z$ = {{m_phi$z}}, {{m_phi$p}}", .open = "{{", .close = "}}")

```

As an initial step, the assumptions made to determine the Smallest Effect Sizes of Interest (SESOI) for the four hypotheses were assessed. In the simulations-based sensitivity power analyses aimed to determine the SESOIs (see the Appendix for detailed explanations), three dispersion parameter conditions $\phi:\{10, 20, 30\}$ for the distribution of the publication bias parameter $\pbs$ were simulated. Employing an intercept-only beta regression model with the complete dataset, the estimated dispersion parameter was `r string_phi`. This finding contradicts our initial assumptions regarding the dispersion parameter's magnitude, rendering the interpretation of SESOIs for our hypotheses untenable. Consequently, it is appropriate to refrain from interpreting SESOIs that were determined from the simulation-based sensitivity power analyses in the subsequent analyses.

<!-- Hypotheses I -->

```{r}
#| message: false
#| echo: false

# Load beta-regression model
mod_h1 <- read_rds(here("data/src/model_h1.rds"))

std_mod_h1 <- datawizard::standardise(mod_h1, verbose = FALSE)

# APA-string (OR, CI, SE, z, p)
b <- coef(mod_h1)
se <- sqrt(diag(vcov(mod_h1)))
z <- b / se
q_crit <- qnorm(0.05) # because one-sided
p <- pnorm(z, lower.tail = FALSE)
lb <- b + se * q_crit 
f_b <- format_value(exp(b[2]))
f_ci <- format_ci(exp(lb[2]), Inf, ci_string = "$CI$")
f_se <- format_value(se[2])
f_z <- format_value(z)[2]
f_p <- str_replace(format_p(p, name = "$p$"), "0\\.", "\\.")[2]
string_h1 <- glue("$OR$ = {f_b}, {f_ci}, $SE$ = {f_se}, $z$ = {f_z}, {f_p}")

# Calculate standardized parameters (OR), and SD of predictor
std_or <- exp(coef(std_mod_h1))["z_rs"]
f_std_or <- format_value(std_or, digits = 2)
data_meta <- read_csv(here("data/meta/processed/data_lindenhonekopp_proc.csv"))
data_cor <- summarise(data_meta, z_rs = atanh(cor(n, d, method = "spearman")), .by = id_meta)
sd_zrs <- format_value(sd(data_cor$z_rs), digits = 2)


# Calculate R2 of model
r2_h1 <- r2_fcn(mod_h1, "w_pbs")
f_r2_h1 <- format_value(r2_fcn(mod_h1, "w_pbs"), digits = 2)

# Calculate AME (1-sd increases) -> probability scale
h1_ame_sd <- format_percent(avg_comparisons(mod_h1, variables = list(z_rs = "sd"))$estimate)

r2_percent_h1 <- str_replace(format_percent(r2_fcn(mod_h1, "w_pbs"), digits = 0), "\\%", "\\\\%")

```

Regarding hypothesis $\mathcal{H}_1$, panel A of @fig-hypotheses-multipanel depicts the relationship between estimated publication bias parameter $\epbs$ and the Fisher *z*-transformed Spearman correlation coefficients $z_{r_s}$ of the correlation of effect size and sample size in each meta-analysis. The observed slope was positive in the direction of the hypothesis and statistically significant `r string_h1`. Lower values for $z_{r_s}$ were significantly associated with lower publication bias parameter values $\epbs$. Additionally, to enhance the interpretability of the regression slope, we refitted the model with standardized values of $z_{r_s}$ and computed the average marginal effects using the *marginaleffects* package [@arel-bundock_marginaleffects_2024]. On average, for every standard deviation increase in the Fisher *z*-transformed correlation coefficient, $SD(z_{r_s})=$ `r sd_zrs`, the model only predicted an increase of `r h1_ame_sd` in the publication bias parameter $\widehat{\omega}_{\text{PBS}}$. In line with this, the general explanative power of the model as determined by the  $\text{pseudo-}R^2$ [@ferrari_beta_2004] was low, $\text{pseudo-}R^2=$ `r f_r2_h1`. Thus, only `r r2_percent_h1` of the variance in $\pbs$ could be explained by the variance of $z_{r_s}$.



<!-- Multi-Panel Figure for Hypotheses -->

```{r hypothesis_multipanel}
#| out-width: 100%
#| echo: false
#| fig-cap-location: top
#| fig-cap: Visual Summary of the Results from the Four Hypotheses
#| fig-pos: H
#| label: fig-hypotheses-multipanel
#| fig-align: center

knitr::include_graphics(here("figures/hypotheses_multipanel.png"))

```

```{=latex}
\begingroup
\scriptsize
\vspace{-5pt}
\noindent\textit{Note.} ...
\endgroup
```

<!-- Hypotheses II -->

```{r}
# Load beta-regression model
mod_h2 <- read_rds(here("data/src/model_h2.rds"))
std_mod_h2 <- standardise(mod_h2, verbose = FALSE)

# APA-string (OR, CI, SE, z, p)
b <- coef(mod_h2)
se <- sqrt(diag(vcov(mod_h2)))
z <- b / se
q_crit <- qnorm(0.95) # because one-sided
p <- pnorm(z, lower.tail = TRUE)
ub <- b + se * q_crit 
f_b <- format_value(exp(b[2]))
f_ci <- format_ci(0, exp(ub[2]), ci_string = "$CI$")
f_se <- format_value(se[2])
f_z <- format_value(z)[2]
f_p <- str_replace(format_p(p, name = "$p$"), "0\\.", "\\.")[2]
string_h2 <- glue("$OR$ = {f_b}, {f_ci}, $SE$ = {f_se}, $z$ = {f_z}, {f_p}")

# standardized OR
std_or <- exp(coef(std_mod_h2))[2]
f_std_or <- format_value(std_or, digits = 3)

# SD of Delta mu_d - mean_d
data_parameters <- read_csv(here("data/optim/processed/data_optim_merged.csv"))
data_delta <-  data_meta |>
    group_by(id_meta) |>
    summarise(mean_d = mean(d)) |>
    inner_join(data_parameters, join_by(id_meta)) |>
    select(id_meta, mean_d, mu_d) |>
    mutate(delta = mu_d - mean_d) 
sd_delta <- sd(data_delta$delta)
f_sd_delta <- format_value(sd_delta)

# R2 of model
r2_h2 <- r2_fcn(mod_h2, "w_pbs")
f_r2_h2 <- format_value(r2_fcn(mod_h2, "w_pbs"), digits = 2)
h2_ame_sd <- format_percent(avg_comparisons(mod_h2, variables = list(Delta = "sd"))$estimate)

```

Concerning $\hypothesis{ii}{}$, panel B of @fig-hypotheses-multipanel depicts the relationship between the estimated publication bias parameter as a function of the difference between the average effect size $\widehat{\delta}$ and the estimated mean parameter of the Gaussian effect size distribution $\widehat{\mu}_d$. The corresponding estimated quadratic slope was negative as indicated by the predicted concave inverse u-shaped line and statistically significant at an $\alpha$-level of $5\%$, `r string_h2`. Again, we calculated the average marginal effect for improved interpretability. On average, for every standard deviation increase in $\Delta_{\widehat{\mu}_d}$, $SD(\Delta_{\widehat{\mu}_d})=$ `r f_sd_delta`, the model only predicted a decrease of `r h2_ame_sd` in the publication bias parameter $\widehat{\omega}_{\text{PBS}}$. The overall explained variation of $\pbs$ by $\Delta_{\widehat{\mu}_d}$ was low, $\text{pseudo-}R^2=$ `r f_r2_h2`.

<!--# Hypotheses III -->

```{r}
#| results: asis

tost_model <- read_rds(here::here("data/src/model_h3.rds"))
tidy_tost <- tost_model |>
    pluck("TOST") |>
    as.data.frame() |>
    rownames_to_column("type") |>
    mutate(across(t:SE, ~round(.x, 2)))

tidy_effsize <- tost_model |>
    pluck("effsize") |>
    rownames_to_column("type") |>
    mutate(across(estimate:upper.ci, ~round(.x, 2)))

# Mean Difference
string_md <- glue("$MD$ = {tidy_effsize[1, 'estimate']}, {format_ci(tidy_effsize[1, 'lower.ci'], tidy_effsize[1, 'upper.ci'], ci = 0.9)}")

# t-test
tt <- subset(tidy_tost, type == "t-test")
string_tt <- glue("$t$({tt$df}) = {tt$t}, $SE$ = {tt$SE}, {format_p(tt$p.value, digits = 3, name = '$p$')}")

# TOST (only test with lower t-values necessary)
tost <- tidy_tost |>
    filter(type != "t-test") |>
    filter(abs(t) == min(abs(t)))
string_tost <- glue("$t$({tost$df}) = {tost$t}, $SE$ = {tost$SE}, {format_p(tost$p.value, digits = 3, name = '$p$')}")


data_pd <- read_rds(here("data/src/es_pd_h3.rds"))
fmt_pd <- with(data_pd, glue("$PD$ = {format_value(pd)}, {format_ci(lb, ub, ci_string = 'bootstrapped $CI_{BCa}$', digits = 3)}"))
fmt_pd_inv <- format_value(1/abs(data_pd$pd), digits = 1)
fmt_pd_pct <- format_percent(abs(data_pd$pd))

```

Regarding hypothesis $\hypothesis{iii}{}$, panel C of @fig-hypotheses-multipanel illustrates the mean difference $\Delta_{\widehat{\mu}_d}$ between the estimated mean parameter of the Gaussian effect size distribution $\widehat{\mu}_d$ and the average effect size $\widehat{\delta}$, along with its corresponding 90% confidence interval. Additionally, the null $t$-distributions of the Two One-Sided Tests (TOST) against the equivalence bounds $\Delta_{EQ}=\{-0.17, 0.17\}$ are illustrated. We only report the results of the *t*-test with the lower *t*-value in the main results, as both tests must be significant to reject the null hypothesis [@lakens_equivalence_2017]. Both one-sided paired *t*-tests were statistically significant, `r string_tost`. This is also indicated by 90% confidence interval falling inside the equivalence range in panel C of @fig-hypotheses-multipanel. We additionally conducted an exploratory null hypothesis significance test to test the point hypothesis that the true mean difference of $\Delta_{\mu_d}$ is exactly zero (not preregistered). The mean difference significantly deviated from zero `r string_md`, `r string_tt`. Again, this is also illustrated in the @fig-hypotheses-multipanel, as the 90% confidence interval does not contain zero. Regarding the additional non-preregistered analyses on the magnitude of equivalence, the proportional distance was `r fmt_pd`. This indicates that the observed mean difference $\Delta_{\widehat{\mu}_d}$ is considerably distant from the lower equivalence bound (i.e., `r fmt_pd_pct` of the distance away from 0 to the lower bound). Put differently, the observed mean difference could have been `r fmt_pd_inv` times larger to reach the lower equivalence bound.

<!--# Hypotheses IV -->

```{r}

mod_h4 <- read_rds(here("data/src/model_h4.rds"))

# Descriptives
descr_h4 <- summarise(mod_h4$model, w_pbs = format_value(mean(w_pbs), digits = 2), .by = type_synthesis)
m_ma <- filter(descr_h4, type_synthesis == "Meta-Analyses")$w_pbs
m_mr <- filter(descr_h4, type_synthesis == "Multisite Replications")$w_pbs

# APA-string (OR, CI, SE, z, p)
b <- coef(mod_h4)
se <- sqrt(diag(vcov(mod_h4)))
z <- b / se
q_crit <- qnorm(0.05) # because one-sided
p <- pnorm(z, lower.tail = FALSE)
lb <- b + se * q_crit 
f_b <- format_value(exp(b[2]))
f_ci <- format_ci(exp(lb[2]), Inf)
f_se <- format_value(se[2])
f_z <- format_value(z)[2]
f_p <- str_replace(format_p(p, name = "$p$"), "0\\.", "\\.")[2]
string_h4 <- glue("$OR$ = {f_b}, {f_ci}, $SE$ = {f_se}, $z$ = {f_z}, {f_p}")

# Calculate R2 of model
r2_h1 <- format_value(r2_fcn(mod_h4, "w_pbs"), digits = 3)

h4_ame <- format_percent(avg_comparisons(mod_h4)$estimate)
h4_ame <- format_percent(plogis(coef(mod_h4)[1] + 1*coef(mod_h4)[2]) - plogis(coef(mod_h4)[1])) # same thing!

```

Finally, regarding hypothesis $\hypothesis{iv}{}$, panel D of @fig-hypotheses-multipanel depicts a comparison of the distributions of the estimated publication bias parameter $\epbs$ between the traditional meta-analyses and the registered replication reports. The figure illustrates that there are high-density regions in the distribution of $\epbs$ close to one for both traditional meta-analyses and registered replication reports. Moreover, the estimated publication bias parameter values below this high-density region are more uniformly distributed for the traditional meta-analyses than the registered replication reports. In the distribution of $\epbs$ for registered replication reports, there are notable outliers with predicted values for $\epbs<0.5$. Already descriptively, contrary to our expectation that the estimated publication bias parameters for registered replication reports (RRRs) would be greater (i.e., lower publication bias) than for traditional meta-analyses (MA), the mean of the estimated publication bias values $\pbs$ of the regular meta-analysis subset is greater than the mean of the multisite replication subset ($M_{\text{MA}}=$ `r m_ma`; $M_{\text{RRR}}=$ `r m_mr`). In line with this, the slope of the beta regression was non-significant, `r string_h4`. Once more, we computed the average marginal effect to examine how the estimated publication bias parameter $\epbs$ changes with the discrete shift from the reference level (RRRs) to traditional meta-analysis. The model predicted a change of `r h4_ame` in $\epbs$ in the opposing direction of the hypothesis.

# Intermediate Discussion of the Confirmatory Results

In the present study we assessed the introduced SPEEC method in a proof of concept using secondary empirical meta-analytical data. We derived four hypotheses, which should be corroborated by the empirical data, if the method works in principle. 

Regarding the results of the hypotheses, it was found that the empirical data was more extreme under the null hypotheses than the prespecified type I error rate of $\alpha = .05$ for $\hypothesis{i}{}$, $\hypothesis{ii}{}$ and $\hypothesis{iv}{}$, leading us to reject the null hypotheses for these predictions from a statistical viewpoint. However, it is important to evaluate these results not only in their statistical significance but also in terms of the practical significance by means of the magnitude of the observed effects [@shaver_what_1993; @lecroy_understanding_2007]. In this regard, we found that for $\hypothesis{i}{}$ and $\hypothesis{ii}{}$, both the explained variance and the average marginal effects were low. This indicates both the effect size-sample size correlation as an alternative indicator of publication bias and the difference $\Delta_{\mu_d}$ between the average effect size and the estimated effect size mean parameter of the SPEEC method only a weak magnitude of effect on the publication bias parameter $\pbs$. Regarding $\hypothesis{iii}{}$, the significance for both the equivalence test and the null hypothesis significance test indicated that although the point null hypothesis ($\Delta{\mu_d}=0$) can be rejected, the equivalence test indicated the difference was too small to be considered meaningful according to the equivalence range. This means that the estimated mean parameter of the effect size $\mu_d$ from the SPEEC method and the average effect size can be considered equivalent within the equivalent range for the subset of RRRs. In fact, the overall magnitude of the effect for the equivalence test can be considered large in terms of the proportional distance $PD$ results. Most importantly, however, we failed to reject the null hypothesis for $\hypothesis{iv}{}$. That is, no evidence was found that the publication bias parameter for RRRs would be greater in comparison to traditional meta-analyses, or in other words, that the probability for selection of non-significant studies in comparison to significant studies greater for RRRs in comparison to traditional meta-analyses. Upon closer examination of the distribution of $\epbs$ for the RRR subgroup, it was observed that while most predictions for $\pbs$ were close to one, there were outliers where the predictions for $\epbs$ approached zero. This contradicts our expectations because, as argued in the hypothesis, it is known for a fact that publication bias in RRRs is absent. 

Overall, the absence of evidence regarding $\hypothesis{iv}{}$ and the weak magnitude of the of the effect found for $\hypothesis{i}{}$ and $\hypothesis{ii}{}$ point to potential problems within the SPEEC method itself or within the parameter optimization process using differential evolution. These findings underscore the need for additional exploratory analyses aimed at diagnosing and addressing potential problems within the parameter estimation in SPEEC.


# Diagnostic Evaluation of Parameter Estimation in SPEEC

<!-- Question: How can we diagnostically assess SPEEC? -->

As this study relies on empirical data to assess the proposed SPEEC approach, the true values for the distributional parameters and the publication bias parameter are unknown. However, as discussed previously, publication bias is inherently absent by design in registered replication reports. Thus, the four distributional parameters ($\mu_d$, $\sigma^2_d$, $\mu_n$, $\phi_n$) within the SPEEC method cannot be biased due to publication bias (especially the mean and variance of the effect size distribution). Leveraging this fact, we can use the subset of the registered replication reports to diagnose the parameter estimation within SPEEC. More specifically, we can derive Maximum Likelihood estimates for the distributional parameters to compare them with the corresponding values estimated by the SPEEC approach, anticipating approximate equivalence between the two approaches. This part of the analysis was not preregistered and conceived after the confirmatory analyses were conducted. Based on this comparative approach between ML and SPEEC, we formulated multiple diagnostic questions to assess the parameter estimation:

1.  To what degree do the estimated distributional parameters differ between SPEEC and MLE?
2.  How are the discrepancies in one parameter associated with those in the other distributional parameters across SPEEC and MLE? Specifically, does a consistency exist in the discrepancies between these parameters?
3.  Is the discrepancy between SPEEC and MLE in the distributional parameters associated with the publication bias parameter $\pbs$?

\noindent
Additionally, we were interested in whether the discrepancy between SPEEC and ML of the distributional parameters and the publication bias parameter $\pbs$ were associated with potentially other relevant factors that may increase the uncertainty of the parameter estimation.

4.  Does the discrepancy between SPEEC and MLE estimates of the distributional parameters correlate with the sample size of the RRRs (*k*)?
5.  Is the discrepancy between SPEEC and MLE in the distributional parameters and the publication bias parameter $\pbs$ associated with effect size heterogeneity?

The Maximum Likelihood estimates for the distributional parameters were obtained using the Nelder-Mead optimizer [@nelder_simplex_1965]. Additionally, the mean and median discrepancy between SPEEC and MLE were calculated to descriptively to assess the average difference between the two estimation methods. To estimate the extent to which the true effect sizes vary within an RRR, we used the standard deviation $\tau$ of the effect sizes to measure between-study heterogeneity. Heterogeneity was estimated using the DerSimonian-Laird estimator [@dersimonian_meta-analysis_1986] in the *metafor* package [@viechtbauer_metafor_2024].

```{r diagnostics}
#| out-width: 100%
#| fig-align: center
#| layout-align: center
#| fig-pos: H
#| fig-cap: Scatter Plot comparing the estimated Distributional Parameters via SPEEC and Maximum Likelihood
#| fig-cap-location: top
#| label: fig-diagnostics

knitr::include_graphics(here("figures/method_comparison.png"))

```

```{=latex}
\begingroup
\scriptsize
\noindent
\textit{Note.} ...
\endgroup

```
```{r}
#| echo: false

discr_ml_speec <- as.data.frame(read_rds(here("data/src/method_comparison_descr.rds")))
rownames(discr_ml_speec) <- discr_ml_speec$parameter

```

Regarding the first question, @fig-diagnostics provides a visual summary of the analysis comparing the distributional parameters estimated using the SPEEC method against those estimated using Maximum Likelihood Estimation (MLE). The diagonal line signifies perfect alignment between MLE and SPEEC. Values below the diagonal indicate higher values for MLE than SPEEC, while values above the diagonal indicate the opposite. Panel A of @fig-diagnostics reiterates the findings of the analysis of $\hypothesis{iii}{}$, suggesting a small discrepancy between the two methods in estimating the mean of the Gaussian effect size distribution $\mu_d$, `r str_discr(discr_ml_speec, "delta_mu_d")`. According to the equivalence test of $\hypothesis{iii}{}$, this discrepancy can be deemed negligible. However, the other panels indicate contrasting outcomes. In Panel B, a systematic discrepancy in the estimation of the variance parameter of the effect size distribution $\sigma^2_d$ between SPEEC and MLE can be observed, `r str_discr(discr_ml_speec, "delta_sigma2_d")`. Descriptively, this suggests that, on average, the variance was greater in the SPEEC approach compared to MLE. Furthermore, this discrepancy increases in nonlinear trend and displays substantial heteroscedasticity with rising variance estimates from the MLE approach. More concretely, the greater the estimated variance from MLE, the greater the deviation of SPEEC from MLE, and the larger the variability in the estimation of SPEEC. Similarly, Panel C also illustrates a systematic overestimation of the mean parameter $\mu_n$ of the Negative-Binomial sample size distribution by SPEEC in comparison to MLE (`r str_discr(discr_ml_speec, "delta_mu_n")`), which again increases in a heteroscedastic exponential-appearing trend. Lastly, Panel D shows that SPEEC generally underestimated the dispersion parameter $\phi_n$ of the sample size distribution in comparison to the ML estimate (`r str_discr(discr_ml_speec, "delta_phi_n")`) and also furthermore indicates a systematic relationship in the discrepancy between the two approaches. Lastly, Panel D illustrates that SPEEC tends to underestimate the dispersion parameter $\phi_n$ of the sample size distribution compared to the ML estimate (`r str_discr(discr_ml_speec, "delta_phi_n")`) and also furthermore indicates a systematic relationship in the discrepancy between the two approaches. As the estimated dispersion from ML decreases, the disparity between ML and SPEEC becomes progressively greater.

\begingroup
\singlespacing

```{r tab-ml-speec}
#| results: asis

table_ml_speec <- read_tex(here("tables/table_diagnostic_cormat.tex"))
cat(table_ml_speec, sep = "\n")

```

\endgroup

To address the remaining diagnostic questions, we conducted a pairwise correlational analysis using the Pearson correlation coefficient between the absolute differences of the parameter estimates derived from the two estimation methods, the publication bias parameter $\epbs$, the total number of primary replication studies $k$ within each registered replication report and the estimated between-study heterogeneity $\widehat{\tau}$. Concerning the multiple testing performed, the question of adjusting *p* values in exploratory studies for multiple testing is an ongoing debate [@rubin_p_2017]. For reasons of transparency, both unadjusted and adjusted *p* values are therefore reported following the method of @benjamini_controlling_1995.

Regarding the parameters of the Gaussian effect size distribution ($\mu_d$, $\sigma^2_d$), a strong positive correlation was observed between the absolute difference in the mean parameter estimates and the variance parameter estimates obtained from ML and SPEEC. This indicates that as the absolute differences between SPEEC and ML increased for the mean parameter $\mu_d$, the absolute discrepancy also increased for the variance parameter $\sigma^2_d$ of the effect size distribution. Furthermore, strong negative correlations were found between the publication bias parameter $\pbs$ and the discrepancy between ML and SPEEC estimates of the mean and variance parameters of the effect size distribution. More specifically, an increase in the absolute discrepancy between both estimation methods increased for both the mean ($\lvert\Delta_{\mu_d}\rvert$) and variance ($\lvert\Delta_{\sigma^2_d}\rvert$), was associated with a decrease in the publication bias parameter $\pbs$, signifying more severe predicted publication bias.  Notably, the total number of primary replications $k$ was not significantly associated with the divergence of ML and SPEEC of any distributional parameter or the publication bias parameter $\pbs$. Moreover, regarding the estimated between-study heterogeneity parameter $\widehat{\tau}$, strong positive correlations were observed for divergences between ML and SPEEC in both the effect size mean and variance parameter. Thus, an increase in the effect size heterogeneity of the RRRs was associated with an increase in the divergence between ML and SPEEC for both parameters of the effect size distribution. Lastly, the estimated between-study heterogeneity parameter $\widehat{\tau}$ was also negatively associated with the estimated publication bias parameter $\epbs$, such that an increase in heterogeneity predicted a decrease in the publication bias parameter (i.e., more severe publication bias). Notably, this correlation was only significant for the unadjusted *p*-value.
