---
bibliography: ../references.bib
---

\newpage

```{r}
#| message: false
#| echo: false

knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
pacman::p_load(insight, glue, tidyverse, here, parameters, broom, betareg, performance, marginaleffects, datawizard)
source(here("R/00_functions.R"))
```

# Results

## Variable Dispersion of $\pbs$

```{r dispersion-w-wpbs}

m_phi <- read_rds(here("data/src/model_dispersion.rds")) |>
    tidy(conf.int = TRUE) |>
    mutate(ci = format_ci(conf.low, conf.high)) |>
    filter(component == "precision") |>
    mutate(across(c(estimate, std.error, statistic), ~format_value(.x, digits = 2))) |>
    mutate(p.value = format_p(p.value, name = "$p$")) |>
    select(b = estimate, se = std.error, z = statistic, ci, p = p.value)
  
string_phi <- glue("$\\widehat{\\phi}$ = {{m_phi$b}}, {{m_phi$ci}}, $SE$ = {{m_phi$se}}, $z$ = {{m_phi$z}}, {{m_phi$p}}", .open = "{{", .close = "}}")

```

As an initial step, the assumptions made to determine the Smallest Effect Sizes of Interest (SESOI) for the four hypotheses were assessed. In the simulations-based sensitivity power analyses aimed at ascertaining the SESOIs (refer to the Appendix for details), three dispersion parameter conditions $\phi:\{10, 20, 30\}$ for the distribution of the publication bias parameter $\pbs$ were simulated. Employing an intercept-only beta regression model with the complete dataset, the estimated dispersion parameter was `r string_phi`. This finding contradicts our initial assumptions regarding the dispersion parameter's magnitude, rendering the interpretation of SESOIs for our hypotheses untenable. Consequently, it is appropriate to refrain from interpreting SESOIs in the subsequent analyses.

<!-- Multi-Panel Figure for Hypotheses -->

```{=latex}
\begin{figure}[H]
\caption{Visual Summary of Results from Four Hypotheses
\label{fig:hypotheses}}
```
```{r hypotheses}
#| out-width: 90%
#| fig-align: center
knitr::include_graphics(here("figures/combine_hypotheses.png"))
```

```{=latex}
\begingroup
\scriptsize
\textit{Note.} \textbf{A}. Estimated publication bias parameter vs. Fisher z-transformed correlation coefficients. Fitted line: regression coefficients with 95% CI. \textbf{B}. Estimated publication bias parameter vs. difference between mean parameter and average effect size. \textbf{C}. Mean difference between mean parameter and effect size, with 90% CI, compared to null t-distribution for equivalence bounds. \textbf{D}. Comparison of estimated publication bias parameter distribution between normal meta-analyses and multisite replications. Point range: marginal predicted values with 95% CI from regression.
\endgroup
\end{figure}
```
## Confirmatory Results of the Predictions from the Hypotheses

<!-- Hypotheses I -->

```{r}
#| message: false

# Load beta-regression model
mod_h1 <- read_rds(here("data/src/model_betareg_h1.rds"))

std_mod_h1 <- datawizard::standardise(mod_h1)

# APA-string (OR, CI, SE, z, p)
b <- coef(mod_h1)
se <- sqrt(diag(vcov(mod_h1)))
z <- b / se
q_crit <- qnorm(0.05) # because one-sided
p <- pnorm(z, lower.tail = FALSE)
lb <- b + se * q_crit 
f_b <- format_value(exp(b[2]))
f_ci <- format_ci(exp(lb[2]), Inf, ci_string = "$CI$")
f_se <- format_value(se[2])
f_z <- format_value(z)[2]
f_p <- str_replace(format_p(p, name = "$p$"), "0\\.", "\\.")[2]
string_h1 <- glue("$OR$ = {f_b}, {f_ci}, $SE$ = {f_se}, $z$ = {f_z}, {f_p}")

# Calculate standardized parameters (OR), and SD of predictor
std_or <- exp(coef(std_mod_h1))["z_rs"]
f_std_or <- format_value(std_or, digits = 2)
data_meta <- read_csv(here("data/meta/processed/data_lindenhonekopp_proc.csv"))
data_cor <- summarise(data_meta, z_rs = atanh(cor(n, d, method = "spearman")), .by = id_meta)
sd_zrs <- format_value(sd(data_cor$z_rs), digits = 2)


# Calculate R2 of model
r2_h1 <- format_value(r2_fcn(mod_h1, "w_pbs"), digits = 3)

# Calculate AME (1-sd increases) -> probability scale
h1_ame_sd <- format_percent(avg_comparisons(mod_h1, variables = list(z_rs = "sd"))$estimate)

```

Regarding hypothesis $\mathcal{H}_1$, panel A of figure 2 depicts the relationship between estimated publication bias parameter $\widehat{\omega}_{\text{PBS}}$ and the Fisher *z*-transformed Spearman correlation coefficients $z_{r_S}$ of the effect size sample size association in each meta-analysis. The observed slope was marginally positive, the sign of the coefficient was in the direction of the hypothesis, however statistically non-significant `r string_h1`. This indicates, that lower values of $z_{r_S}$ were not statistally significantly associated with lower publication bias parameter values $\widehat{\omega}_{\text{PBS}}$. Additionally, to enhance the interpretability of the regression slope, we refitted the model with standardized values of $z_{r_S}$ and computed the average marginal effects [@arel-bundock_marginaleffects_2024]. On average, for every standard deviation increase in the Fisher *z*-transformed correlation coefficient $z_{r_S}$ ($SD(z_{r_S})=$ `r sd_zrs`), the model only predicted an increase of `r h1_ame_sd` in the publication bias parameter $\widehat{\omega}_{\text{PBS}}$. In line with this, the general explanative power of the model as determined by the pseudo $R^2$ [@ferrari_beta_2004] was low, $R^2=$ `r r2_h1`.

<!-- Hypotheses II -->

```{r}
# Load beta-regression model
mod_h2 <- read_rds(here("data/src/model_betareg_h2.rds"))
std_mod_h2 <- standardise(mod_h2)

# APA-string (OR, CI, SE, z, p)
b <- coef(mod_h2)
se <- sqrt(diag(vcov(mod_h2)))
z <- b / se
q_crit <- qnorm(0.95) # because one-sided
p <- pnorm(z, lower.tail = TRUE)
ub <- b + se * q_crit 
f_b <- format_value(exp(b[2]))
f_ci <- format_ci(0, exp(ub[2]), ci_string = "$CI$")
f_se <- format_value(se[2])
f_z <- format_value(z)[2]
f_p <- str_replace(format_p(p, name = "$p$"), "0\\.", "\\.")[2]
string_h2 <- glue("$OR$ = {f_b}, {f_ci}, $SE$ = {f_se}, $z$ = {f_z}, {f_p}")

# standardized OR
std_or <- exp(coef(std_mod_h2))[2]
f_std_or <- format_value(std_or, digits = 3)

# SD of Delta mu_d - mean_d
data_parameters <- read_csv(here("data/optim/processed/data_optim_merged.csv"))
data_delta <-  data_meta |>
    group_by(id_meta) |>
    summarise(mean_d = mean(d)) |>
    inner_join(data_parameters, join_by(id_meta)) |>
    select(id_meta, mean_d, mu_d) |>
    mutate(delta = mu_d - mean_d) 
sd_delta <- sd(data_delta$delta)
f_sd_delta <- format_value(sd_delta)

# R2 of model
r2_ferrari_cribari <- format_value(r2_fcn(mod_h2, "w_pbs"), digits = 3)

h2_ame_sd <- format_percent(avg_comparisons(mod_h2, variables = list(Delta = "sd"))$estimate)

```

Concerning $\mathcal{H}_2$, panel B of figure 1 depicts the relationship between estimated publication bias parameter as a function of the difference between the average effect size $\widehat{\delta}$ and the estimated mean parameter of the Gaussian effect size distribution $\widehat{\mu}_d$. The corresponding estimated quadratic slope was negative as indicated by the predicted concave inverse u-shaped line and statistically significant at an $\alpha$-level of 5\\%, `r string_h2`. We again calculated the average marginal effect for improved interpretability. On average, for every standard deviation increase in $\Delta_{\widehat{\mu}_d,\widehat{\delta}}$ ($SD(\Delta_{\widehat{\mu}_d,\widehat{\delta}})=$ `r f_sd_delta`), the model only predicted an increase of `r h2_ame_sd` in the publication bias parameter $\widehat{\omega}_{\text{PBS}}$. The overall explained variation of $\pbs$ by $\Delta_{\widehat{\mu}_d,\widehat{\delta}}$ was low, $R^2_{\text{pseudo}}=$ `r r2_ferrari_cribari`.

<!--# Hypotheses III -->

```{r}
#| results: asis

tost_model <- read_rds(here::here("data/src/model_tost_h3.rds"))
tidy_tost <- tost_model |>
    pluck("TOST") |>
    as.data.frame() |>
    rownames_to_column("type") |>
    mutate(across(t:SE, ~round(.x, 2)))

tidy_effsize <- tost_model |>
    pluck("effsize") |>
    rownames_to_column("type") |>
    mutate(across(estimate:upper.ci, ~round(.x, 2)))

# Mean Difference
string_md <- glue("$M$ = {tidy_effsize[1, 'estimate']}, {format_ci(tidy_effsize[1, 'lower.ci'], tidy_effsize[1, 'upper.ci'], ci = 0.9)}")

# Hedge´s g_rm
string_smd <- glue("Hedge´s $g_{rm}$ = {{tidy_effsize[2, 'estimate']}}, {{format_ci(tidy_effsize[2, 'lower.ci'], tidy_effsize[2, 'upper.ci'], ci = 0.9)}}", .open = "{{", .close = "}}")

# t-test
tt <- subset(tidy_tost, type == "t-test")
string_tt <- glue("$t$({tt$df}) = {tt$t}, $SE$ = {tt$SE}, {format_p(tt$p.value, digits = 3, name = '$p$')}")

# TOST (only test with lower t-values necessary)
tost <- tidy_tost |>
    filter(type != "t-test") |>
    filter(abs(t) == min(abs(t)))
string_tost <- glue("$t$({tost$df}) = {tost$t}, $SE$ = {tost$SE}, {format_p(tost$p.value, digits = 3, name = '$p$')}")

```

In relation to hypothesis $\mathcal{H}_3$, panel C of Figure 2 illustrate the mean difference $\Delta_{\widehat{\mu}_d, \widehat{\delta}}$ between the estimated mean parameter of the Gaussian effect size distribution $\widehat{\mu}_d$ and the average effect size $\widehat{\delta}$, along with its corresponding confidence interval. Additionally, the null $t$-distributions of the Two One-Sided Tests (TOST) against the equivalence bounds $\Delta_{EQ}=(-0.17, 0.17)$ are illustrated. We only report the results of the t-test with the lower *t*-value in the main results as both tests must be significant to reject the null hypothesis [@lakens_equivalence_2017]. Both one-sided paired t-tests were statistically significant, `r string_tost`. This is also indicated by 90% confidence interval lying within the equivalence range in panel C of Figure 2. We additionally conducted an exploratory null hypothesis significance test to test the point hypothesis that the true mean difference of $\Delta_{\widehat{\mu}_d, \widehat{\delta}}$ is exactly zero. The mean difference significantly deviated from zero `r string_md`, `r string_tt`. This indicates that, despite the significant null hypothesis significance test, the difference was too small to be considered meaningful according to the equivalence range $\Delta_{EQ}=(-0.17, 0.17)$ of the equivalence test.

<!--# Hypotheses IV -->

```{r}

mod_h4 <- read_rds(here("data/src/model_betareg_h4.rds"))

# Descriptives
descr_h4 <- summarise(mod_h4$model, w_pbs = format_value(mean(w_pbs), digits = 2), .by = type_synthesis)
m_ma <- filter(descr_h4, type_synthesis == "Meta-Analyses")$w_pbs
m_mr <- filter(descr_h4, type_synthesis == "Multisite Replications")$w_pbs

# APA-string (OR, CI, SE, z, p)
b <- coef(mod_h4)
se <- sqrt(diag(vcov(mod_h4)))
z <- b / se
q_crit <- qnorm(0.05) # because one-sided
p <- pnorm(z, lower.tail = FALSE)
lb <- b + se * q_crit 
f_b <- format_value(exp(b[2]))
f_ci <- format_ci(exp(lb[2]), Inf)
f_se <- format_value(se[2])
f_z <- format_value(z)[2]
f_p <- str_replace(format_p(p, name = "$p$"), "0\\.", "\\.")[2]
string_h4 <- glue("$OR$ = {f_b}, {f_ci}, $SE$ = {f_se}, $z$ = {f_z}, $p$ {f_p}")

# Calculate R2 of model
r2_h1 <- format_value(r2_fcn(mod_h4, "w_pbs"), digits = 3)

h4_ame <- format_percent(avg_comparisons(mod_h4)$estimate)
h4_ame <-format_percent(plogis(coef(mod_h4)[1] + 1*coef(mod_h4)[2]) - plogis(coef(mod_h4)[1])) # same thing!

```

Finally, regarding hypothesis $\mathcal{H}_4$, panel D of Figure 2 illustrates the comparison between the estimated publication bias parameters for typical meta-analysis in comparison to multisite replication studies / registered reports. Already descriptively, contrary to our expectation that the estimated puhblication bias parameters for multisite replication studies (MR) would be greater (i.e., lower publication bias) than for regular meta-analysis (MA), the mean of the estimated publication bias values $\pbs$ of the regular meta-analysis subset is greater than the mean of the multisite replication subset ($M_{\text{MA}}=$ `r m_ma`; $M_{\text{MR}}=$ `r m_mr`). In line with this, slope of the beta regression was non-significant, `r string_h4`, as also indicated by the overlappping confidence interval of the predicted marginal means in panel D. Once more, we computed the average marginal effect to examine how the estimated publication bias parameter $\widehat{\omega}_{\text{PBS}}$ changes with the discrete shift from the reference level (multisite replication studies / registered reports) to typical meta-analysis, as predicted by the regression model, reveiling a change of `r h4_ame` in the opposing direction of the hypothesis.

## Diagnostic Evaluation of Parameter Estimation in SPEEC

```{r}
discr_ml_speec <- as.data.frame(read_rds(here("data/src/data_descr_discrepancy_ml_speec.rds")))
rownames(discr_ml_speec) <- discr_ml_speec$parameter

str_discr <- function(x, parameter) {
    x$parameter <- dplyr::case_match(x$parameter,
    "delta_mu_d" ~ "({\\Delta_{\\mu_d}})",
    "delta_sigma2_d" ~ "({\\Delta_{\\sigma^2_d}})",
    "delta_phi_n" ~ "({\\Delta_{\\phi_n}})",
    "delta_mu_n" ~ "({\\Delta_{\\mu_n}})"
    )
    subs <- x[parameter, ]
    glue::glue('$M{subs$parameter}$ = {subs$mean}, $Mdn{subs$parameter}$ = {subs$median}')
}

```

As this study relies on empirical data to preliminarily assess the proposed SPEEC approach, the true values for the distributional parameters and the publication bias parameter are unknown. However, as discussed previously, publication bias is inherently absent by design in multisite replication studies and registered reports. Thus, the four distributional parameters ($\mu_d$, $\sigma^2_d$, $\mu_n$, $\phi_n$) within the SPEEC approach cannot be biased due to publication bias (especially the mean and variance of the effect size distribution). Leveraging this fact, we can use a subset of the data encompassing the multisite replication studies and registered reports for a diagnostic evaluation of the parameter estimation within the SPEEC approach. More specifically, we can derive Maximum Likelihood estimates for the distributional parameters to compare them with the corresponding values estimated by the SPEEC approach, anticipating approximate equivalence between the two approaches. This part was of the analysis was not preregistered and conceived after the confirmatory analyses were conducted. Based on this comparative approach between ML and SPEEC, we formulated five diagnostic questions to assess parameter estimation:

1.  To what degree do the estimated distributional parameters differ between SPEEC and MLE?
2.  How are the discrepancies in one parameter associated with those in the other distributional parameters across SPEEC and MLE? Specifically, does a consistency exist in the discrepancies between these parameters?
3.  Does the discrepancy between SPEEC and MLE estimates of the distributional parameters correlate with the sample size of the multisite replication studies?
4.  Is the discrepancy between SPEEC and MLE in the distributional parameters associated with sample size of multisite replication studies *k*?
5.  Is the discrepancy between SPEEC and MLE in the distributional parameters associated with the publication bias parameter $\pbs$?

The Maximum Likelihood estimates for the distributional parameters were obtained using the Nelder-Mead optimization algorithm [@nelder_simplex_1965]. Additionally, the mean and median discrepancy between SPEEC and MLE were calculated to descriptively to assess the average difference between the two estimation methods.

```{=latex}
\begin{figure}[H]
\caption{Scatter Plot comparing the estimated Distributional Parameters via SPEEC and Maximum Likelihood\label{fig:ml-speec-comparison}}
```
```{r ml-speec-cor}
#| out-width: 100%
#| fig-align: center
#| layout-align: center
knitr::include_graphics(here("figures/ml_speec_comparison.png"))
```

```{=latex}
\begingroup
\scriptsize
\textit{Note.} \textbf{A1}. Comparison of estimated mean parameter $\mu_d$ from Gaussian effect size distribution. \textbf{A2}. Comparison of estimated variance parameter $\sigma_d^2$ of Gaussian effect size distribution. Axes and colorbar are log (base 10) transformed. \textbf{B1}. Comparison of mean parameter $\mu_n$ of Negative-Binomial sample size distribution \textbf{B2}. Comparison of dispersion parameter $\phi_n$ of Negative-Binomial sample size distribution. Axes and colorbar are log (base 10) transformed.
\endgroup
\end{figure}
```

Regarding question one, Figure 3 provides a visual summary of the analysis comparing the distributional parameters estimated by the SPEEC method against those estimated by Maximum Likelihood Estimation (MLE). The diagonal line signifies perfect alignment between MLE and SPEEC estimates. Values below the diagonal indicate higher values for MLE compared to SPEEC, while values above the diagonal indicate the opposite. 

Panel A of figure XX reiterates the findings of the analysis of $\mathcal{H}_3$, suggesting a small discrepancy between the two methods in estimating the mean of the Gaussian effect size distribution, `r str_discr(discr_ml_speec, "delta_mu_d")`. This discrepancy can be deemed practically negligible according to the equivalence test of $\mathcal{H}_3$. However, the other panels indicate contrasting outcomes.Panel B reveils a systematic discepancy in the estimation of the variance parameter of the Gaussien effect size distribution between SPEEC and MLE, `r str_discr(discr_ml_speec, "delta_sigma2_d")`. Descriptively, this suggests that on average, the variance was estimated to be greater in the SPEEC approach compared to MLE. Furthermore, this discrepancy increases in exponential trend and displays substantial heteroscedasticity with rising variance estimates from the MLE approach. Similarly, Panel C also illustrates a systematic overestimation of the mean parameter of the Negative-Binomial sample size distribution by SPEEC in comparison to MLE (`r str_discr(discr_ml_speec, "delta_mu_n")`), which again increases in an exponential trend with higher mean parameter estimates from MLE. Lastly, Panel D shows that the SPEEC approach generally understimates dispersion parameter of the sample size distribution in comparison to the ML estimate (`r str_discr(discr_ml_speec, "delta_phi_n")`) and also furthermore indicates a systematic relationship in the discrepancy between the two approaches.

```{r tab-ml-speec}
#| results: asis

table_ml_speec <- read_tex(here("tables/ml_speec_diff_cor.tex"))
cat(table_ml_speec, sep = "\n")

```

To address the remaining diagnostic questions, we conducted a pairwise correlational analysis between the absolute differences of the parameter estimates derived from the two estimation methods, alongside the publication bias parameter $\widehat{\omega}_{\text{PBS}}$ and the total number of primary replication studies $k$ within each multisite replication project or registered report. We used the Pearson correlation coefficient and corrected the obtained *p*-values for multiple comparisons to control the false discovery rate [@benjamini_controlling_1995].

Regarding the parameters of the Gaussian effect size distribution, a strong positive correlation was observed between the absolute difference in the mean parameter estimates and the variance parameter estimates obtained from ML and SPEEC. This indicates that as the absolute discrepancy between SPEEC and ML increased for the mean parameter $\mu_d$, the absolute discrepancy also increased for the variance parameter $\sigma^2_d$ of the effect size distribution. Furthermore, strong negative correlations were found between the publication bias parameter $\pbs$ and the discrepancy between ML and SPEEC estimates of the mean and variance parameters of the effect size distribution. More specifically, as the absolute  discrepancy between both estimation methods increased for both the mean ($\lvert\Delta_{\mu_d}\rvert$) and variance ($\lvert\Delta_{\sigma^2_d}\rvert$), the publication bias parameter $\pbs$ decreased, signifying more severe predicted publication bias. Notably, the total number of of primary replications $k$ was not significantly associated to the divergence of ML and SPEEC of any distributional parameter or the publication bias parameter $\pbs$.