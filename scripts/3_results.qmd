---
bibliography: ../bibliography/tidy_references.bib
---

```{r}
#| message: false
#| echo: false

knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
pacman::p_load(insight, glue, tidyverse, here, parameters, broom, betareg, performance, marginaleffects, datawizard)
source(here("R/functions.R"))
```

# Confirmatory Results Assessing SPEEC

```{r dispersion-w-wpbs}

m_phi <- read_rds(here("data/src/model_dispersion.rds")) |>
    tidy(conf.int = TRUE) |>
    mutate(ci = format_ci(conf.low, conf.high)) |>
    filter(component == "precision") |>
    mutate(across(c(estimate, std.error, statistic), ~format_value(.x, digits = 2))) |>
    mutate(p.value = format_p(p.value, name = "$p$")) |>
    select(b = estimate, se = std.error, z = statistic, ci, p = p.value)
  
string_phi <- glue("$\\widehat{\\phi}$ = {{m_phi$b}}, {{m_phi$ci}}, $SE$ = {{m_phi$se}}, $z$ = {{m_phi$z}}, {{m_phi$p}}", .open = "{{", .close = "}}")

```

As an initial step, the assumptions made to determine the Smallest Effect Sizes of Interest (SESOI) for the four hypotheses were assessed. In the simulation-based sensitivity power analyses aimed at determining the SESOIs (see [Appendix B](Appendix B: Power Analyses determining the SESOIs)), three dispersion parameter conditions $\phi:\{10, 20, 30\}$ for the distribution of the publication bias parameter $\pbs$ were simulated. Employing an intercept-only beta regression model with the complete dataset, the estimated dispersion parameter was `r string_phi`. This much lower estimated dispersion in comparison to the simulated conditions indicates the variability in $\pbs$ was much higher that expected. In general, the lower the dispersion parameter $\phi$ for a fixed mean $\mu$, the higher the variance [@ferrari_beta_2004], as $\mathbb{V}[\pbs]=\mu \cdot (1 - \mu) / (1 + \phi)$. This finding contradicts our initial assumptions regarding the dispersion parameter's magnitude, rendering the interpretation of SESOIs for our hypotheses untenable. Consequently, it is appropriate to refrain from interpreting SESOIs that were determined from the simulation-based sensitivity power analyses in the subsequent analyses.

<!-- Hypotheses I -->

```{r}
#| message: false
#| echo: false

# Load beta-regression model
mod_h1 <- read_rds(here("data/src/model_h1.rds"))

std_mod_h1 <- datawizard::standardise(mod_h1, verbose = FALSE)

# APA-string (OR, CI, SE, z, p)
b <- coef(mod_h1)
se <- sqrt(diag(vcov(mod_h1)))
z <- b / se
q_crit <- qnorm(0.05) # because one-sided
p <- pnorm(z, lower.tail = FALSE)
lb <- b + se * q_crit 
f_b <- format_value(exp(b[2]))
f_ci <- format_ci(exp(lb[2]), Inf, ci_string = "$CI$")
f_se <- format_value(se[2])
f_z <- format_value(z)[2]
f_p <- str_replace(format_p(p, name = "$p$"), "0\\.", "\\.")[2]
string_h1 <- glue("$OR$ = {f_b}, {f_ci}, $SE$ = {f_se}, $z$ = {f_z}, {f_p}")

# Calculate standardized parameters (OR), and SD of predictor
std_or <- exp(coef(std_mod_h1))["z_rs"]
f_std_or <- format_value(std_or, digits = 2)
data_meta <- read_csv(here("data/meta/processed/data_lindenhonekopp_proc.csv"))
data_cor <- summarise(data_meta, z_rs = atanh(cor(n, d, method = "spearman")), .by = id_meta)
sd_zrs <- format_value(sd(data_cor$z_rs), digits = 2)

# Calculate R2 of model
r2_h1 <- r2_fcn(mod_h1, "w_pbs")
f_r2_h1 <- format_value(r2_fcn(mod_h1, "w_pbs"), digits = 2)

# Calculate AME (1-sd increases) -> probability scale
h1_ame_sd <- format_percent(avg_comparisons(mod_h1, variables = list(z_rs = "sd"))$estimate)

r2_percent_h1 <- str_replace(format_percent(r2_fcn(mod_h1, "w_pbs"), digits = 0), "\\%", "\\\\%")

marginaleffects_ver <- as.character(packageVersion("marginaleffects"))
```

Regarding hypothesis $\hypothesis{1}{}$, panel A of $\autoref{fig:hypotheses}$ depicts the relationship between the estimated publication bias parameter $\epbs$ and the Fisher *z*-transformed Spearman correlation coefficients $z_{r_s}$ of the correlation of effect size and sample size in each meta-analysis. The observed slope was positive in the direction of the hypothesis and statistically significant `r string_h1`. Lower values for $z_{r_s}$ were significantly associated with lower publication bias parameter values $\epbs$. Additionally, to enhance the interpretability of the regression slope, we refitted the model with standardised values of $z_{r_s}$ and computed the average marginal effects using the *marginaleffects* package [version `r marginaleffects_ver`, @arel-bundock_marginaleffects_2024]. On average, for every standard deviation increase in the Fisher *z*-transformed correlation coefficient, $SD(z_{r_s})=$ `r sd_zrs`, the model only predicted an increase of `r h1_ame_sd` in the publication bias parameter $\widehat{\omega}_{\text{PBS}}$. In line with this, the general explanative power of the model as determined by the  $\text{pseudo-}R^2$ [@ferrari_beta_2004] was low, $\text{pseudo-}R^2=$ `r f_r2_h1`. Thus, only `r r2_percent_h1` of the variance in $\pbs$ could be explained by the variance of $z_{r_s}$.



<!-- Hypotheses II -->

```{r}
# Load beta-regression model
mod_h2 <- read_rds(here("data/src/model_h2.rds"))
std_mod_h2 <- standardise(mod_h2, verbose = FALSE)

# APA-string (OR, CI, SE, z, p)
b <- coef(mod_h2)
se <- sqrt(diag(vcov(mod_h2)))
z <- b / se
q_crit <- qnorm(0.95) # because one-sided
p <- pnorm(z, lower.tail = TRUE)
ub <- b + se * q_crit 
f_b <- format_value(exp(b[2]))
f_ci <- format_ci(0, exp(ub[2]), ci_string = "$CI$")
f_se <- format_value(se[2])
f_z <- format_value(z)[2]
f_p <- str_replace(format_p(p, name = "$p$"), "0\\.", "\\.")[2]
string_h2 <- glue("$OR$ = {f_b}, {f_ci}, $SE$ = {f_se}, $z$ = {f_z}, {f_p}")

# standardized OR
std_or <- exp(coef(std_mod_h2))[2]
f_std_or <- format_value(std_or, digits = 3)

# SD of Delta mu_d - mean_d
data_parameters <- read_csv(here("data/optim/processed/data_optim_merged.csv"))
data_delta <-  data_meta |>
    group_by(id_meta) |>
    summarise(mean_d = mean(d)) |>
    inner_join(data_parameters, join_by(id_meta)) |>
    select(id_meta, mean_d, mu_d) |>
    mutate(delta = mu_d - mean_d) 
sd_delta <- sd(data_delta$delta)
f_sd_delta <- format_value(sd_delta)

# R2 of model
r2_h2 <- r2_fcn(mod_h2, "w_pbs")
f_r2_h2 <- format_value(r2_fcn(mod_h2, "w_pbs"), digits = 2)
h2_ame_sd <- format_percent(avg_comparisons(mod_h2, variables = list(Delta = "sd"))$estimate)

```

Concerning $\hypothesis{2}{}$, panel B of  depicts the relationship between the estimated publication bias parameter as a function of the difference between the average effect size $\widehat{\delta}$ and the estimated mean parameter of the Gaussian effect size distribution $\widehat{\mu}_d$. The corresponding estimated quadratic slope was negative as indicated by the predicted concave inverse u-shaped line and statistically significant at an $\alpha$-level of $5\%$, `r string_h2`. Again, we calculated the average marginal effect for improved interpretability. On average, for every standard deviation increase in $\Delta_{\widehat{\mu}_d}$, $SD(\Delta_{\widehat{\mu}_d})=$ `r f_sd_delta`, the model only predicted a decrease of `r h2_ame_sd` in the publication bias parameter $\widehat{\omega}_{\text{PBS}}$. The overall explained variation of $\pbs$ by $\Delta_{\widehat{\mu}_d}$ was low, $\text{pseudo-}R^2=$ `r f_r2_h2`.


<!-- Multi-Panel Figure for Hypotheses -->

```{=latex}
\begin{figure}[h]
\caption{Visual Summary of the Confirmatory Results from the Four Hypotheses}
\label{fig:hypotheses}
```
```{r hypotheses}
#| out-width: 100%
#| fig-align: center
knitr::include_graphics(here("figures/hypotheses_multipanel.png"))
```

```{=latex}
\begingroup
\scriptsize
\vspace{-5pt}
\setstretch{0.7}
\noindent\textit{Note.} $\textbf{A}$. Scatter plot of the Fisher $z$-transformed correlation coefficients of the effect size sample size correlation predicting estimated publication bias parameters $\pbs$. Fitted line and ribbon represents the predicted values and 95\% confidence interval from the beta regression model. $n=150$. $\textbf{B}$. Estimated publication bias parameter $\pbs$ predicted by the difference $\Delta_{\mu_d}$ between mean parameter of the effect size distribution of SPEEC $\mu_d$ and average effect size $\widehat{\delta}$ Fitted line and ribbon represents the predicted values and 95\% confidence interval from the beta regression model. $n = 207$. $\textbf{C}$. Mean difference between the between mean parameter of the effect size distribution of SPEEC $\mu_d$ and average effect size $\widehat{\delta}$ with 90\% CI, compared to null $t$-distribution for the equivalence bounds. $n=57$. $\textbf{D}$. Comparison of estimated publication bias parameter $\pbs$ distributions between classical meta-analyses and registered replication reports. The point interval represents the predicted values and 95\% confidence interval from the beta regression model. $n=207$.
\endgroup
\end{figure}
```

<!--# Hypotheses III -->

```{r}
#| results: asis

tost_model <- read_rds(here::here("data/src/model_h3.rds"))
tidy_tost <- tost_model |>
    pluck("TOST") |>
    as.data.frame() |>
    rownames_to_column("type") |>
    mutate(across(t:SE, ~round(.x, 2)))

tidy_effsize <- tost_model |>
    pluck("effsize") |>
    rownames_to_column("type") |>
    mutate(across(estimate:upper.ci, ~round(.x, 2)))

# Mean Difference
string_md <- glue("$MD$ = {tidy_effsize[1, 'estimate']}, {format_ci(tidy_effsize[1, 'lower.ci'], tidy_effsize[1, 'upper.ci'], ci = 0.9)}")

# t-test
tt <- subset(tidy_tost, type == "t-test")
string_tt <- glue("$t$({tt$df}) = {tt$t}, $SE$ = {tt$SE}, {format_p(tt$p.value, digits = 3, name = '$p$')}")

# TOST (only test with lower t-values necessary)
tost <- tidy_tost |>
    filter(type != "t-test") |>
    filter(abs(t) == min(abs(t)))
string_tost <- glue("$t$({tost$df}) = {tost$t}, $SE$ = {tost$SE}, {format_p(tost$p.value, digits = 3, name = '$p$')}")


data_pd <- read_rds(here("data/src/es_pd_h3.rds"))
fmt_pd <- with(data_pd, glue("$PD$ = {format_value(pd)}, {format_ci(lb, ub, ci_string = 'bootstrapped $CI_{BCa}$', digits = 3)}"))
fmt_pd_inv <- format_value(1/abs(data_pd$pd), digits = 1)
fmt_pd_pct <- format_percent(abs(data_pd$pd))

```

Regarding hypothesis $\hypothesis{3}{}$, panel C of $\autoref{fig:hypotheses}$ illustrates the mean difference $\Delta_{\widehat{\mu}_d}$ between the estimated mean parameter of the Gaussian effect size distribution $\widehat{\mu}_d$ and the average effect size $\widehat{\delta}$, along with its corresponding 90% confidence interval. Additionally, the null $t$-distributions of the TOSTs against the equivalence bounds $\Delta_{EQ}=\{-0.17, 0.17\}$ are illustrated. We only report the results of the *t*-test with the lower *t*-value in the main results, as both tests must be significant to reject the null hypothesis [@lakens_equivalence_2017]. Both one-sided paired *t*-tests were statistically significant, `r string_tost`. This is also indicated by 90% confidence interval falling inside the equivalence range in panel C of $\autoref{fig:hypotheses}$. We additionally conducted an exploratory null hypothesis significance test to test the point hypothesis that the true mean difference of $\Delta_{\mu_d}$ is exactly zero (not preregistered). The mean difference significantly deviated from zero `r string_md`, `r string_tt`. Again, this is also illustrated in the $\autoref{fig:hypotheses}$, as the 90% confidence interval does not contain zero. Regarding the additional non-preregistered analyses on the magnitude of equivalence, the proportional distance was `r fmt_pd`. This indicates that the observed mean difference $\Delta_{\widehat{\mu}_d}$ is considerably distant from the lower equivalence bound (i.e., `r fmt_pd_pct` of the distance away from 0 to the lower bound). Put differently, the observed mean difference could have been `r fmt_pd_inv` times larger to reach the lower equivalence bound.

<!--# Hypotheses IV -->

```{r}

mod_h4 <- read_rds(here("data/src/model_h4.rds"))

# Descriptives
descr_h4 <- summarise(mod_h4$model, w_pbs = format_value(mean(w_pbs), digits = 2), .by = type_synthesis)
m_ma <- filter(descr_h4, type_synthesis == "Meta-Analyses")$w_pbs
m_mr <- filter(descr_h4, type_synthesis == "Multisite Replications")$w_pbs

# APA-string (OR, CI, SE, z, p)
b <- coef(mod_h4)
se <- sqrt(diag(vcov(mod_h4)))
z <- b / se
q_crit <- qnorm(0.05) # because one-sided
p <- pnorm(z, lower.tail = FALSE)
lb <- b + se * q_crit 
f_b <- format_value(exp(b[2]))
f_ci <- format_ci(exp(lb[2]), Inf)
f_se <- format_value(se[2])
f_z <- format_value(z)[2]
f_p <- str_replace(format_p(p, name = "$p$"), "0\\.", "\\.")[2]
string_h4 <- glue("$OR$ = {f_b}, {f_ci}, $SE$ = {f_se}, $z$ = {f_z}, {f_p}")

# Calculate R2 of model
r2_h1 <- format_value(r2_fcn(mod_h4, "w_pbs"), digits = 3)

h4_ame <- format_percent(avg_comparisons(mod_h4)$estimate)
h4_ame <- format_percent(plogis(coef(mod_h4)[1] + 1*coef(mod_h4)[2]) - plogis(coef(mod_h4)[1])) # same thing!

```

Finally, regarding hypothesis $\hypothesis{4}{}$, panel D of $\autoref{fig:hypotheses}$ depicts a comparison of the distributions of the estimated publication bias parameter $\epbs$ between the classical meta-analyses and the RRRs. The figure illustrates that there are high-density regions in the distribution of $\epbs$ close to one for both classical meta-analyses and RRRs. Moreover, the estimated publication bias parameter values below this high-density region are more uniformly distributed for the classical meta-analyses than the RRRs. In the distribution of $\epbs$ for RRRs, there are notable outliers with predicted values for $\epbs<0.5$. Already descriptively, contrary to our expectation that the estimated publication bias parameters for RRRs would be greater (i.e., lower publication bias) than for classical meta-analyses (MA), the mean of the estimated publication bias values $\pbs$ of the regular meta-analysis subset is greater than the mean of the RRR subset ($M_{\text{MA}}=$ `r m_ma`; $M_{\text{RRR}}=$ `r m_mr`). In line with this, the slope of the beta regression was non-significant, `r string_h4`. Once more, we computed the average marginal effect to examine how the estimated publication bias parameter $\epbs$ changes with the discrete shift from the reference level (RRR) to classical meta-analysis. The model predicted a change of `r h4_ame` in $\epbs$ in the opposite direction of the hypothesis.

# Intermediate Discussion of the Confirmatory Results

In the present study we assessed the introduced SPEEC method in a proof of concept using secondary empirical meta-analytical data. We derived four hypotheses, which should be corroborated by the empirical data, if the method works in principle. 

Regarding the results of the hypotheses, it was found that the empirical data was more extreme under the null hypotheses than the prespecified type I error rate of $\alpha = .05$ for $\hypothesis{1}{}$, $\hypothesis{2}{}$ and $\hypothesis{4}{}$, leading us to reject the null hypotheses for these predictions from a statistical viewpoint. However, it is important to evaluate these results not only in terms of their statistical significance but also in terms of their practical significance by means of the magnitude of the observed effects [@shaver_what_1993; @lecroy_understanding_2007]. In this regard, we found that for $\hypothesis{1}{}$ and $\hypothesis{2}{}$, both the explained variance and the average marginal effects were low. This indicates both the effect size-sample size correlation as an alternative indicator of publication bias and the difference $\Delta_{\mu_d}$ between the average effect size and the estimated effect size mean parameter of the SPEEC method only a weak magnitude of effect on the publication bias parameter $\pbs$. Regarding $\hypothesis{3}{}$, the significance for both the equivalence test and the null hypothesis significance test indicated that although the point null hypothesis ($\Delta{\mu_d}=0$) can be rejected, the equivalence test indicated the difference was too small to be considered meaningful according to the equivalence range. This means that the estimated mean parameter of the effect size $\mu_d$ from the SPEEC method and the average effect size can be considered equivalent within the equivalence range for the subset of RRRs. In fact, the overall magnitude of the effect for the equivalence test can be considered large in terms of the proportional distance $PD$ results. Most importantly, however, we failed to reject the null hypothesis for $\hypothesis{4}{}$. That is, no evidence was found that the publication bias parameter for RRRs would be greater in comparison to classical meta-analyses, or in other words, that the probability for selection of non-significant studies in comparison to significant studies would be greater for RRRs in comparison to classical meta-analyses. Upon closer examination of the distribution of $\epbs$ for the RRR subgroup, it was observed that while most predictions for $\pbs$ were close to one, there were outliers where the predictions for $\epbs$ approached zero. This contradicts our expectations because, as argued in the hypothesis, because puublication bias should be absent in RRRs.

Overall, the absence of evidence regarding $\hypothesis{4}{}$ and the weak magnitude of the effect found for $\hypothesis{1}{}$ and $\hypothesis{2}{}$ point to potential problems within the SPEEC method itself or within the parameter optimization process using differential evolution. These findings underscore the need for additional exploratory analyses aimed at diagnosing and addressing potential problems within the parameter estimation in SPEEC.


# Diagnostic Evaluation of Parameter Estimation in SPEEC

<!-- Question: How can we diagnostically assess SPEEC? -->

As this study relies on empirical data to assess the proposed SPEEC approach, the true values for the distributional parameters and the publication bias parameter are unknown. However, as discussed previously, publication bias is inherently absent by design in RRRs. Thus, the four distributional parameters ($\mu_d$, $\sigma^2_d$, $\mu_n$, $\phi_n$) within the SPEEC method cannot be biased due to publication bias (especially the mean and variance of the effect size distribution). Leveraging this fact, we can use the subset of the RRRs to diagnose the parameter estimation within SPEEC. More specifically, we can use maximum likelihood estimation (MLE) to estimate the distributional parameters and subsequently compare them with the corresponding values estimated by the SPEEC method, anticipating approximate equivalence between the two approaches. This part of the analysis was not preregistered and conceived after the confirmatory analyses were conducted. Based on this comparative approach between MLE and SPEEC, we formulated multiple diagnostic questions to assess the parameter estimation:

1.  To what degree do the estimated distributional parameters differ between SPEEC and MLE?
2.  How are the discrepancies in one parameter associated with those in the other distributional parameters across SPEEC and MLE? Specifically, does a consistency exist in the discrepancies between these parameters?
3.  Is there a discrepancy between SPEEC and MLE in the distributional parameters associated with the publication bias parameter $\pbs$?

\noindent
Additionally, we were interested in whether the discrepancy between SPEEC and ML of the distributional parameters and the publication bias parameter $\pbs$ are associated with potentially other relevant factors that may increase the uncertainty of the parameter estimation. Accordingly, we derived two additional questions:

4.  Does the discrepancy between SPEEC and MLE estimates of the distributional parameters correlate with the sample size of the RRRs (*k*)?
5.  Is the discrepancy between SPEEC and MLE in the distributional parameters and the publication bias parameter $\pbs$ associated with effect size heterogeneity?

The estimates of the MLE for the distributional parameters were obtained using the Nelder-Mead optimizer [@nelder_simplex_1965]. Additionally, the mean and median discrepancy between SPEEC and MLE were calculated to descriptively assess the average difference between the two estimation methods. To estimate the extent to which the true effect sizes vary within an RRR, we used the standard deviation $\tau$ of the effect sizes to measure between-study heterogeneity. Heterogeneity was estimated using the DerSimonian-Laird estimator [@dersimonian_meta-analysis_1986] in the *metafor* package [version `r as.character(packageVersion("metafor"))`, @viechtbauer_metafor_2024].


```{=latex}
\begin{figure}[h]
\caption{Divergence Between the Estimated Distributional Parameters of SPEEC and MLE}
\label{fig:diagnostics}
```
```{r diagnostics}
#| out-width: 100%
#| fig-align: center
knitr::include_graphics(here("figures/method_comparison.png"))
```

```{=latex}
\begingroup
\scriptsize
\vspace{-5pt}
\setstretch{0.7}
\noindent\textit{Note.} Comparison of the four estimated distributional parameters between SPEEC and Maximum Likelihood Estimation (MLE). Diagonal line represents perfect match between both methods. Axes of B and D are $\log_{10}$-scaled. Colorbar indicates the absolute divergences between MLE and SPEEC for each parameter.
\endgroup
\end{figure}
```



```{r}
#| echo: false

discr_ml_speec <- as.data.frame(read_rds(here("data/src/method_comparison_descr.rds")))
rownames(discr_ml_speec) <- discr_ml_speec$parameter

```

Regarding the first question, $\autoref{fig:diagnostics}$ provides a visual summary of the analysis comparing the distributional parameters estimated using the SPEEC method against those estimated using Maximum Likelihood Estimation (MLE). The diagonal line signifies perfect alignment between MLE and SPEEC. Values below the diagonal indicate higher values for MLE than SPEEC, while values above the diagonal indicate the opposite. Panel A of  $\autoref{fig:diagnostics}$ reiterates the findings of the analysis of $\hypothesis{3}{}$, suggesting a small discrepancy between the two methods in estimating the mean of the Gaussian effect size distribution $\mu_d$, `r str_discr(discr_ml_speec, "delta_mu_d")`. According to the equivalence test of $\hypothesis{3}{}$, this discrepancy can be deemed negligible. However, the other panels indicate contrasting outcomes. In Panel B, a systematic discrepancy in the estimation of the variance parameter of the effect size distribution $\sigma^2_d$ between SPEEC and MLE can be observed, `r str_discr(discr_ml_speec, "delta_sigma2_d")`. Descriptively, this suggests that, on average, the variance was greater in the SPEEC approach compared to MLE. Furthermore, this discrepancy increases in a nonlinear trend and displays substantial heteroscedasticity with rising variance estimates from the MLE approach. The greater the estimated variance from MLE, the greater the deviation of SPEEC from MLE, and the larger the variability in the estimation of SPEEC. Similarly, Panel C also illustrates a systematic overestimation of the mean parameter $\mu_n$ of the Negative-Binomial sample size distribution by SPEEC in comparison to MLE (`r str_discr(discr_ml_speec, "delta_mu_n")`), which again increases in a heteroscedastic exponential-appearing trend. Lastly, Panel D shows that SPEEC generally underestimated the dispersion parameter $\phi_n$ of the sample size distribution in comparison to the MLE estimate (`r str_discr(discr_ml_speec, "delta_phi_n")`) and furthermore indicates a systematic relationship in the discrepancy between the two approaches. Lastly, Panel D illustrates that SPEEC tends to underestimate the dispersion parameter $\phi_n$ of the sample size distribution compared to the MLE (`r str_discr(discr_ml_speec, "delta_phi_n")`) and also furthermore indicates a systematic relationship in the discrepancy between the two approaches.

To address the remaining diagnostic questions, we conducted a pairwise correlational analysis using the Pearson correlation coefficient between the absolute differences of the parameter estimates derived from the two estimation methods, the publication bias parameter $\epbs$, the number of replication studies $k$ within each RRR and the between-study heterogeneity $\widehat{\tau}$. Concerning the multiple testing performed, the question of adjusting *p*-values in exploratory studies for multiple testing is an ongoing debate [@rubin_p_2017]. For reasons of transparency, both unadjusted and adjusted *p*-values are therefore reported following the method of Benjamini and Hochberg [-@benjamini_controlling_1995]. 

*Table 2* summarises the results of the correlational analysis. Regarding the parameters of the Gaussian effect size distribution ($\mu_d$, $\sigma^2_d$), a strong positive correlation was observed between the absolute difference in the mean parameter estimates and the variance parameter estimates obtained from MLE and SPEEC. This indicates that as the absolute differences between SPEEC and MLE increased for the mean parameter $\mu_d$, the absolute discrepancy also increased for the variance parameter $\sigma^2_d$ of the effect size distribution. Furthermore, strong negative correlations were found between the publication bias parameter $\pbs$ and the discrepancy between MLE and SPEEC estimates of the mean and variance parameters of the effect size distribution. More specifically, an increase in the absolute discrepancy between both estimation methods increased for both the mean ($\lvert\Delta_{\mu_d}\rvert$) and variance ($\lvert\Delta_{\sigma^2_d}\rvert$) was associated with a decrease in the publication bias parameter $\pbs$, signifying more severe predicted publication bias.  Notably, the total number of primary replications $k$ was not significantly associated with the divergence of MLE and SPEEC of any distributional parameter or the publication bias parameter $\pbs$. Moreover, regarding the estimated between-study heterogeneity parameter $\widehat{\tau}$, strong positive correlations were observed for divergences between MLE and SPEEC in both the effect size mean and variance parameter. Thus, an increase in the effect size heterogeneity of the RRRs was associated with an increase in the divergence between ML and SPEEC for both parameters of the effect size distribution. Lastly, the estimated between-study heterogeneity parameter $\widehat{\tau}$ was also negatively associated with the estimated publication bias parameter $\epbs$, such that an increase in heterogeneity predicted a decrease in the publication bias parameter (i.e., more severe publication bias). Notably, this correlation was only significant for the unadjusted *p*-value.

\begingroup
\renewcommand{\arraystretch}{1.45}
   

```{r tab-ml-speec}
#| results: asis

table_ml_speec <- read_tex(here("tables/table_diagnostic_cormat.tex"))
cat(table_ml_speec, sep = "\n")

```

\endgroup