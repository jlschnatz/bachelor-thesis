@software{allaire_quarto_2024,
	title = {Quarto},
	url = {https://doi.org/10.5281/zenodo.5960048},
	author = {Allaire, J. J. and Teague, Charles and Scheidegger, Carlos and Xie, Yihui and Dervieux, Christophe},
	date = {2024},
}
@article{andrade_p_2019,
	title = {The P Value and Statistical Significance: Misunderstandings, Explanations, Challenges, and Alternatives},
	volume = {41},
	issn = {0253-7176},
	url = {https://doi.org/10.4103/IJPSYM.IJPSYM_193_19},
	doi = {10.4103/IJPSYM.IJPSYM_193_19},
	shorttitle = {The P Value and Statistical Significance},
	pages = {210--215},
	number = {3},
	journaltitle = {Indian Journal of Psychological Medicine},
	author = {Andrade, Chittaranjan},
	urldate = {2024-05-02},
	date = {2019-05-01},
	note = {Publisher: {SAGE} Publications India},
	file = {Full Text PDF:/Users/luca/Zotero/storage/489J894T/Andrade - 2019 - The P Value and Statistical Significance Misunder.pdf:application/pdf},
}
@software{arel-bundock_marginaleffects_2024,
	title = {marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests},
	url = {https://marginaleffects.com/},
	version = {0.18.0.9},
	author = {Arel-Bundock, Vincent},
	date = {2024},
}
@article{bakan_test_1966,
	title = {The test of significance in psychological research},
	volume = {66},
	issn = {0033-2909},
	doi = {10.1037/h0020412},
	pages = {423--437},
	number = {6},
	journaltitle = {Psychological Bulletin},
	shortjournal = {Psychol Bull},
	author = {Bakan, D.},
	date = {1966-12},
	pmid = {5974619},
	keywords = {Humans, Psychometrics, Psychological Tests},
}
@article{begg_operating_1994,
	title = {Operating Characteristics of a Rank Correlation Test for Publication Bias},
	volume = {50},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2533446},
	doi = {10.2307/2533446},
	abstract = {An adjusted rank correlation test is proposed as a technique for identifying publication bias in a meta-analysis, and its operating characteristics are evaluated via simulations. The test statistic is a direct statistical analogue of the popular "funnel-graph." The number of component studies in the meta-analysis, the nature of the selection mechanism, the range of variances of the effect size estimates, and the true underlying effect size are all observed to be influential in determining the power of the test. The test is fairly powerful for large meta-analyses with 75 component studies, but has only moderate power for meta-analyses with 25 component studies. However, in many of the configurations in which there is low power, there is also relatively little bias in the summary effect size estimate. Nonetheless, the test must be interpreted with caution in small meta-analyses. In particular, bias cannot be ruled out if the test is not significant. The proposed technique has potential utility as an exploratory tool for meta-analysts, as a formal procedure to complement the funnel-graph.},
	pages = {1088--1101},
	number = {4},
	journaltitle = {Biometrics},
	author = {Begg, Colin B. and Mazumdar, Madhuchhanda},
	urldate = {2024-02-04},
	date = {1994},
	note = {Publisher: [Wiley, International Biometric Society]},
	keywords = {notion},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/QPAWGDD9/Begg und Mazumdar - 1994 - Operating Characteristics of a Rank Correlation Te.pdf:application/pdf},
}
@incollection{begg_publication_1994,
	location = {New York, {NY}, {US}},
	title = {Publication bias},
	isbn = {978-0-87154-226-7},
	abstract = {publication bias presents possibly the greatest methodologic threat to validity of a meta-analysis / it can be caused by the biased and selective reporting of the results of a given study, or, more seriously, by the selective decision to publish the results of the study in the first place / undetected publication bias is especially serious owing to the fact that the meta-analysis may not only lead to a spurious conclusion, but the aggregation of data may give the impression, with standard statistical methodology, that the conclusions are very precise  methods for identifying publication bias / methods for correcting publication bias ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {399--409},
	booktitle = {The handbook of research synthesis},
	publisher = {Russell Sage Foundation},
	author = {Begg, Colin B.},
	date = {1994},
	keywords = {Statistical Validity, Meta Analysis, Scientific Communication, Data Collection},
	file = {Begg - 1994 - Publication bias.pdf:/Users/luca/Zotero/storage/ANRARK4F/The Handbook of research synthesis -- Cooper, Harris M\; Hedges, Larry V -- 1994 -- New York_ Russell Sage Foundation -- 9780871542267 -- e5ec7912251b20962cc27abb619a2e25 -- Anna’s Archive.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/T8LJUQX6/1993-99100-024.html:text/html;The Handbook of research synthesis -- Cooper, Harris M\; Hedges, Larry V -- 1994 -- New York_ Russell Sage Foundation -- 9780871542267 -- e5ec7912251b20962cc27abb619a2e25 -- Anna’s Archive.pdf:/Users/luca/Zotero/storage/KVRTTN2R/The Handbook of research synthesis -- Cooper, Harris M\; Hedges, Larry V -- 1994 -- New York_ Russell Sage Foundation -- 9780871542267 -- e5ec7912251b20962cc27abb619a2e25 -- Anna’s Archive.pdf:application/pdf;The Handbook of research synthesis -- Cooper, Harris M\; Hedges, Larry V -- 1994 -- New York_ Russell Sage Foundation -- 9780871542267 -- e5ec7912251b20962cc27abb619a2e25 -- Anna’s Archive.pdf:/Users/luca/Zotero/storage/8HTFWE9Y/The Handbook of research synthesis -- Cooper, Harris M\; Hedges, Larry V -- 1994 -- New York_ Russell Sage Foundation -- 9780871542267 -- e5ec7912251b20962cc27abb619a2e25 -- Anna’s Archive.pdf:application/pdf},
}
@article{benjamini_controlling_1995,
	title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
	volume = {57},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346101},
	shorttitle = {Controlling the False Discovery Rate},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate ({FWER}). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the {FWER} when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the {FWER} is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	pages = {289--300},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	urldate = {2024-04-07},
	date = {1995},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/MMWCXJI8/Benjamini und Hochberg - 1995 - Controlling the False Discovery Rate A Practical .pdf:application/pdf},
}
@article{bozarth_signifying_1972,
	title = {Signifying significant significance},
	volume = {27},
	issn = {1935-990X},
	doi = {10.1037/h0038034},
	abstract = {Examined publication practices in counseling psychology by reviewing 3 counseling psychology journals from 1967 to 1970 to determine the frequencies of articles reporting rejection at the traditional alpha levels. Of 1,046 articles examined, 86\% used tests of statistical significance. Findings suggest that "traditional" significant results were related to publication. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {774--775},
	number = {8},
	journaltitle = {American Psychologist},
	author = {Bozarth, Jerold D. and Roberts, Ralph R.},
	date = {1972},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Scientific Communication, Statistical Significance, Counseling Psychology},
	file = {Bozarth und Roberts - 1972 - Signifying significant significance.pdf:/Users/luca/Zotero/storage/LVQWFZTL/Bozarth und Roberts - 1972 - Signifying significant significance.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/XVQVX24Z/1990-56998-001.html:text/html},
}
@article{cafri_meta-meta-analysis_2010,
	title = {A Meta-Meta-Analysis: Empirical Review of Statistical Power, Type I Error Rates, Effect Sizes, and Model Selection of Meta-Analyses Published in Psychology},
	volume = {45},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171003680187},
	doi = {10.1080/00273171003680187},
	shorttitle = {A Meta-Meta-Analysis},
	abstract = {This article uses meta-analyses published in Psychological Bulletin from 1995 to 2005 to describe meta-analyses in psychology, including examination of statistical power, Type I errors resulting from multiple comparisons, and model choice. Retrospective power estimates indicated that univariate categorical and continuous moderators, individual moderators in multivariate analyses, and tests of residual variability within individual levels of categorical moderators had the lowest and most concerning levels of power. Using methods of calculating power prospectively for significance tests in meta-analysis, we illustrate how power varies as a function of the number of effect sizes, the average sample size per effect size, effect size magnitude, and level of heterogeneity of effect sizes. In most meta-analyses many significance tests were conducted, resulting in a sizable estimated probability of a Type I error, particularly for tests of means within levels of a moderator, univariate categorical moderators, and residual variability within individual levels of a moderator. Across all surveyed studies, the median effect size and the median difference between two levels of study level moderators were smaller than Cohen's (1988) conventions for a medium effect size for a correlation or difference between two correlations. The median Birge's (1932) ratio was larger than the convention of medium heterogeneity proposed by Hedges and Pigott (2001) and indicates that the typical meta-analysis shows variability in underlying effects well beyond that expected by sampling error alone. Fixed-effects models were used with greater frequency than random-effects models; however, random-effects models were used with increased frequency over time. Results related to model selection of this study are carefully compared with those from Schmidt, Oh, and Hayes (2009), who independently designed and produced a study similar to the one reported here. Recommendations for conducting future meta-analyses in light of the findings are provided.},
	pages = {239--270},
	number = {2},
	journaltitle = {Multivariate Behavioral Research},
	author = {Cafri, Guy and Kromrey, Jeffrey D. and Brannick, Michael T.},
	urldate = {2024-01-23},
	date = {2010-03-31},
	pmid = {26760285},
	note = {Number: 2
Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171003680187},
}
@article{camerer_evaluating_2018,
	title = {Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015},
	volume = {2},
	rights = {2018 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0399-z},
	doi = {10.1038/s41562-018-0399-z},
	abstract = {Being able to replicate scientific findings is crucial for scientific progress1–15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516–36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
	pages = {637--644},
	number = {9},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
	urldate = {2024-02-17},
	date = {2018-09},
	langid = {english},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Psychology, Economics},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/FWDQE589/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf:application/pdf},
}
@article{carter_correcting_2019,
	title = {Correcting for Bias in Psychology: A Comparison of Meta-Analytic Methods},
	volume = {2},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245919847196},
	doi = {10.1177/2515245919847196},
	shorttitle = {Correcting for Bias in Psychology},
	abstract = {Publication bias and questionable research practices in primary research can lead to badly overestimated effects in meta-analysis. Methodologists have proposed a variety of statistical approaches to correct for such overestimation. However, it is not clear which methods work best for data typically seen in psychology. Here, we present a comprehensive simulation study in which we examined how some of the most promising meta-analytic methods perform on data that might realistically be produced by research in psychology. We simulated several levels of questionable research practices, publication bias, and heterogeneity, and used study sample sizes empirically derived from the literature. Our results clearly indicated that no single meta-analytic method consistently outperformed all the others. Therefore, we recommend that meta-analysts in psychology focus on sensitivity analyses—that is, report on a variety of methods, consider the conditions under which these methods fail (as indicated by simulation studies such as ours), and then report how conclusions might change depending on which conditions are most plausible. Moreover, given the dependence of meta-analytic methods on untestable assumptions, we strongly recommend that researchers in psychology continue their efforts to improve the primary literature and conduct large-scale, preregistered replications. We provide detailed results and simulation code at https://osf.io/rf3ys and interactive figures at http://www.shinyapps.org/apps/{metaExplorer}/.},
	pages = {115--144},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Carter, Evan C. and Schönbrodt, Felix D. and Gervais, Will M. and Hilgard, Joseph},
	urldate = {2023-11-29},
	date = {2019-06-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {notion},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/PHNKNNRT/Carter et al. - 2019 - Correcting for Bias in Psychology A Comparison of.pdf:application/pdf},
}
@article{cha_comprehensive_2007,
	title = {Comprehensive Survey on Distance/Similarity Measures Between Probability Density Functions},
	volume = {1},
	abstract = {Distance or similarity measures are essential to solve many pattern recognition problems such as classification, clustering, and retrieval problems. Various distance/similarity measures that are applicable to compare two probability density functions, pdf in short, are reviewed and categorized in both syntactic and semantic relationships. A correlation coefficient and a hierarchical clustering technique are adopted to reveal similarities among numerous distance/similarity measures.},
	pages = {300--307},
	number = {4},
	journaltitle = {International Journal of Mathematical Models and Methods in Applied Sciences},
	shortjournal = {International Journal of Mathematical models and Methods in Applied Sciences},
	author = {Cha, Sung-Hyuk},
	date = {2007-01-01},
}
@article{chambers_past_2022,
	title = {The past, present and future of Registered Reports},
	volume = {6},
	rights = {2021 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01193-7},
	doi = {10.1038/s41562-021-01193-7},
	abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
	pages = {29--42},
	number = {1},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Chambers, Christopher D. and Tzavella, Loukia},
	urldate = {2024-04-15},
	date = {2022-01},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Culture, Publishing},
	file = {Full Text PDF:/Users/luca/Zotero/storage/WWV7HBP5/Chambers und Tzavella - 2022 - The past, present and future of Registered Reports.pdf:application/pdf},
}
@article{cohen_earth_1994,
	title = {The earth is round (p{	extless}.05)},
	volume = {49},
	issn = {1935-990X},
	doi = {10.1037/0003-066X.49.12.997},
	abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {997--1003},
	number = {12},
	journaltitle = {American Psychologist},
	author = {Cohen, Jacob},
	date = {1994},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Null Hypothesis Testing},
	file = {Cohen - 1994 - The earth is round (p  .05).pdf:/Users/luca/Zotero/storage/X7ITI2PG/Cohen - 1994 - The earth is round (p  .05).pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/AZZ8XQXM/1995-12080-001.html:text/html},
}
@incollection{cooper_research_2019,
	edition = {3},
	title = {Research synthesis as a scientific process},
	isbn = {978-0-87154-163-5},
	url = {http://www.scopus.com/inward/record.url?scp=84902712953&partnerID=8YFLogxK},
	pages = {3--16},
	booktitle = {The handbook of research synthesis and meta-analysis},
	publisher = {Russell Sage Foundation},
	author = {Cooper, Harris and Hedges, Larry Vernon and Valentine, Jeffrey C.},
	editor = {Cooper, Harris and Hedges, Larry Vernon and Valentine, Jeffrey C.},
	urldate = {2024-02-17},
	date = {2019},
	file = {Cooper et al. - 2019 - Research synthesis as a scientific process.pdf:/Users/luca/Zotero/storage/XKCIHWGG/Cooper et al. - 2019 - Research synthesis as a scientific process.pdf:application/pdf},
}
@article{cribari-neto_beta_2010,
	title = {Beta Regression in R},
	volume = {34},
	rights = {Copyright (c) 2009 Francisco Cribari-Neto, Achim Zeileis},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v034.i02},
	doi = {10.18637/jss.v034.i02},
	abstract = {The class of beta regression models is commonly used by practitioners to model variables that assume values in the standard unit interval (0, 1). It is based on the assumption that the dependent variable is beta-distributed and that its mean is related to a set of regressors through a linear predictor with unknown coefficients and a link function. The model also includes a precision parameter which may be constant or depend on a (potentially different) set of regressors through a link function as well. This approach naturally incorporates features such as heteroskedasticity or skewness which are commonly observed in data taking values in the standard unit interval, such as rates or proportions. This paper describes the betareg package which provides the class of beta regressions in the R system for statistical computing. The underlying theory is briefly outlined, the implementation discussed and illustrated in various replication exercises.},
	pages = {1--24},
	journaltitle = {Journal of Statistical Software},
	author = {Cribari-Neto, Francisco and Zeileis, Achim},
	urldate = {2024-03-04},
	date = {2010-04-05},
	langid = {english},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/PG276BDN/Cribari-Neto und Zeileis - 2010 - Beta Regression in R.pdf:application/pdf},
}
@article{delacre_why_2017,
	title = {Why psychologists should by default use Welch's t-test instead of student's t-test},
	volume = {30},
	issn = {2397-8570},
	url = {http://www.scopus.com/inward/record.url?scp=85019141079&partnerID=8YFLogxK},
	doi = {10.5334/irsp.82},
	abstract = {When comparing two independent groups, psychology researchers commonly use Student's t-Tests. Assumptions of normality and homogeneity of variance underlie this test. More often than not, when these conditions are not met, Student's t-Test can be severely biased and lead to invalid statistical inferences. Moreover, we argue that the assumption of equal variances will seldom hold in psychological research, and choosing between Student's t-Test and Welch's t-Test based on the outcomes of a test of the equality of variances often fails to provide an appropriate answer. We show that the Welch's t-Test provides a better control of Type 1 error rates when the assumption of homogeneity of variance is not met, and it loses little robustness compared to Student's t-Test when the assumptions are met. We argue that Welch's t-Test should be used as a default strategy.},
	pages = {92--101},
	number = {1},
	journaltitle = {International Review of Social Psychology},
	author = {Delacre, M. and Lakens, D. and Leys, C.},
	urldate = {2024-04-24},
	date = {2017-04-05},
	keywords = {statistical power, homogeneity of variance, Homogeneity of variance, Homoscedasticity, Levene's test, Statistical power, Student's t-test, Student's t-Test, type 1 error, Type 1 error, type 2 error, Type 2 error, Welch's t-test, Welch's t-Test},
	file = {Volltext:/Users/luca/Zotero/storage/ZJM9U9MD/Delacre et al. - 2017 - Why psychologists should by default use Welch's t-.pdf:application/pdf},
}
@article{dickersin_existence_1990,
	title = {The existence of publication bias and risk factors for its occurrence},
	volume = {263},
	issn = {0098-7484},
	abstract = {Publication bias is the tendency on the parts of investigators, reviewers, and editors to submit or accept manuscripts for publication based on the direction or strength of the study findings. Much of what has been learned about publication bias comes from the social sciences, less from the field of medicine. In medicine, three studies have provided direct evidence for this bias. Prevention of publication bias is important both from the scientific perspective (complete dissemination of knowledge) and from the perspective of those who combine results from a number of similar studies (meta-analysis). If treatment decisions are based on the published literature, then the literature must include all available data that is of acceptable quality. Currently, obtaining information regarding all studies undertaken in a given field is difficult, even impossible. Registration of clinical trials, and perhaps other types of studies, is the direction in which the scientific community should move.},
	pages = {1385--1389},
	number = {10},
	journaltitle = {{JAMA}},
	shortjournal = {{JAMA}},
	author = {Dickersin, K.},
	date = {1990-03-09},
	pmid = {2406472},
	keywords = {Publishing, American Medical Association, History, 19th Century, Periodicals as Topic, Research Support as Topic, Risk Factors, United Kingdom, United States},
	file = {Dickersin - 1990 - The existence of publication bias and risk factors.pdf:/Users/luca/Zotero/storage/92D8H2I7/Dickersin - 1990 - The existence of publication bias and risk factors.pdf:application/pdf},
}
@article{dickersin_publication_1993,
	title = {Publication Bias: The Problem That Won't Go Away},
	volume = {703},
	issn = {1749-6632},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-6632.1993.tb26343.x},
	doi = {10.1111/j.1749-6632.1993.tb26343.x},
	shorttitle = {Publication Bias},
	pages = {135--148},
	number = {1},
	journaltitle = {Annals of the New York Academy of Sciences},
	author = {Dickersin, Kay and Min, Yuan-I},
	urldate = {2024-02-11},
	date = {1993},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-6632.1993.tb26343.x},
	file = {Dickersin und Min - 1993 - Publication Bias The Problem That Won't Go Away.pdf:/Users/luca/Zotero/storage/YVLCV6ZZ/Dickersin und Min - 1993 - Publication Bias The Problem That Won't Go Away.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/7JB6X5X8/j.1749-6632.1993.tb26343.html:text/html},
}
@article{ebersole_many_2016,
	title = {Many Labs 3: Evaluating participant pool quality across the academic semester via replication},
	volume = {67},
	issn = {0022-1031},
	url = {https://www.sciencedirect.com/science/article/pii/S0022103115300123},
	doi = {10.1016/j.jesp.2015.10.012},
	series = {Special Issue: Confirmatory},
	shorttitle = {Many Labs 3},
	abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences—conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
	pages = {68--82},
	journaltitle = {Journal of Experimental Social Psychology},
	shortjournal = {Journal of Experimental Social Psychology},
	author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and Joy-Gaba, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and van Allen, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
	urldate = {2024-02-17},
	date = {2016-11-01},
	keywords = {Social psychology, Replication, Cognitive psychology, Individual differences, Participant pool, Sampling effects, Situational effects},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/PLI7JT4Z/Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf:application/pdf;ScienceDirect Snapshot:/Users/luca/Zotero/storage/L8W73Y5X/S0022103115300123.html:text/html},
}
@article{ebersole_many_2020,
	title = {Many Labs 5: Testing Pre-Data-Collection Peer Review as an Intervention to Increase Replicability},
	volume = {3},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245920958687},
	doi = {10.1177/2515245920958687},
	shorttitle = {Many Labs 5},
	abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology ({RP}:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect (p {	extless} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these {RP}:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the {RP}:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3–9; median total sample = 1,279.5, range = 276–3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the {RP}:P protocols (Δr = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the {RP}:P protocols (r = .04) and the original {RP}:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00–.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19–.50).},
	pages = {309--331},
	number = {3},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Ebersole, Charles R. and Mathur, Maya B. and Baranski, Erica and Bart-Plange, Diane-Jo and Buttrick, Nicholas R. and Chartier, Christopher R. and Corker, Katherine S. and Corley, Martin and Hartshorne, Joshua K. and {IJzerman}, Hans and Lazarević, Ljiljana B. and Rabagliati, Hugh and Ropovik, Ivan and Aczel, Balazs and Aeschbach, Lena F. and Andrighetto, Luca and Arnal, Jack D. and Arrow, Holly and Babincak, Peter and Bakos, Bence E. and Baník, Gabriel and Baskin, Ernest and Belopavlović, Radomir and Bernstein, Michael H. and Białek, Michał and Bloxsom, Nicholas G. and Bodroža, Bojana and Bonfiglio, Diane B. V. and Boucher, Leanne and Brühlmann, Florian and Brumbaugh, Claudia C. and Casini, Erica and Chen, Yiling and Chiorri, Carlo and Chopik, William J. and Christ, Oliver and Ciunci, Antonia M. and Claypool, Heather M. and Coary, Sean and Čolić, Marija V. and Collins, W. Matthew and Curran, Paul G. and Day, Chris R. and Dering, Benjamin and Dreber, Anna and Edlund, John E. and Falcão, Filipe and Fedor, Anna and Feinberg, Lily and Ferguson, Ian R. and Ford, Máire and Frank, Michael C. and Fryberger, Emily and Garinther, Alexander and Gawryluk, Katarzyna and Ashbaugh, Kayla and Giacomantonio, Mauro and Giessner, Steffen R. and Grahe, Jon E. and Guadagno, Rosanna E. and Hałasa, Ewa and Hancock, Peter J. B. and Hilliard, Rias A. and Hüffmeier, Joachim and Hughes, Sean and Idzikowska, Katarzyna and Inzlicht, Michael and Jern, Alan and Jiménez-Leal, William and Johannesson, Magnus and Joy-Gaba, Jennifer A. and Kauff, Mathias and Kellier, Danielle J. and Kessinger, Grecia and Kidwell, Mallory C. and Kimbrough, Amanda M. and King, Josiah P. J. and Kolb, Vanessa S. and Kołodziej, Sabina and Kovacs, Marton and Krasuska, Karolina and Kraus, Sue and Krueger, Lacy E. and Kuchno, Katarzyna and Lage, Caio Ambrosio and Langford, Eleanor V. and Levitan, Carmel A. and de Lima, Tiago Jessé Souza and Lin, Hause and Lins, Samuel and Loy, Jia E. and Manfredi, Dylan and Markiewicz, Łukasz and Menon, Madhavi and Mercier, Brett and Metzger, Mitchell and Meyet, Venus and Millen, Ailsa E. and Miller, Jeremy K. and Montealegre, Andres and Moore, Don A. and Muda, Rafał and Nave, Gideon and Nichols, Austin Lee and Novak, Sarah A. and Nunnally, Christian and Orlić, Ana and Palinkas, Anna and Panno, Angelo and Parks, Kimberly P. and Pedović, Ivana and Pękala, Emilian and Penner, Matthew R. and Pessers, Sebastiaan and Petrović, Boban and Pfeiffer, Thomas and Pieńkosz, Damian and Preti, Emanuele and Purić, Danka and Ramos, Tiago and Ravid, Jonathan and Razza, Timothy S. and Rentzsch, Katrin and Richetin, Juliette and Rife, Sean C. and Rosa, Anna Dalla and Rudy, Kaylis Hase and Salamon, Janos and Saunders, Blair and Sawicki, Przemysław and Schmidt, Kathleen and Schuepfer, Kurt and Schultze, Thomas and Schulz-Hardt, Stefan and Schütz, Astrid and Shabazian, Ani N. and Shubella, Rachel L. and Siegel, Adam and Silva, Rúben and Sioma, Barbara and Skorb, Lauren and de Souza, Luana Elayne Cunha and Steegen, Sara and Stein, L. A. R. and Sternglanz, R. Weylin and Stojilović, Darko and Storage, Daniel and Sullivan, Gavin Brent and Szaszi, Barnabas and Szecsi, Peter and Szöke, Orsolya and Szuts, Attila and Thomae, Manuela and Tidwell, Natasha D. and Tocco, Carly and Torka, Ann-Kathrin and Tuerlinckx, Francis and Vanpaemel, Wolf and Vaughn, Leigh Ann and Vianello, Michelangelo and Viganola, Domenico and Vlachou, Maria and Walker, Ryan J. and Weissgerber, Sophia C. and Wichman, Aaron L. and Wiggins, Bradford J. and Wolf, Daniel and Wood, Michael J. and Zealley, David and Žeželj, Iris and Zrubka, Mark and Nosek, Brian A.},
	urldate = {2024-02-17},
	date = {2020-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/8QYMJ9XV/Ebersole et al. - 2020 - Many Labs 5 Testing Pre-Data-Collection Peer Revi.pdf:application/pdf},
}
@software{edelbuettel_rcppde_2022,
	title = {{RcppDE}: Global Optimization by Differential Evolution in C++},
	rights = {{GPL}-2 {	extbar} {GPL}-3 [expanded from: {GPL} (≥ 2)]},
	url = {https://cran.r-project.org/web/packages/RcppDE/index.html},
	shorttitle = {{RcppDE}},
	abstract = {An efficient C++ based implementation of the '{DEoptim}' function which performs global optimization by differential evolution. Its creation was motivated by trying to see if the old approximation "easier, shorter, faster: pick any two" could in fact be extended to achieving all three goals while moving the code from plain old C to modern C++. The initial version did in fact do so, but a good part of the gain was due to an implicit code review which eliminated a few inefficiencies which have since been eliminated in '{DEoptim}'.},
	version = {0.1.7},
	author = {Edelbuettel, Dirk},
	urldate = {2024-03-25},
	date = {2022-12-20},
	keywords = {Optimization},
}
@article{egger_bias_1997,
	title = {Bias in meta-analysis detected by a simple, graphical test},
	volume = {315},
	issn = {0959-8138},
	doi = {10.1136/bmj.315.7109.629},
	abstract = {{OBJECTIVE}: Funnel plots (plots of effect estimates against sample size) may be useful to detect bias in meta-analyses that were later contradicted by large trials. We examined whether a simple test of asymmetry of funnel plots predicts discordance of results when meta-analyses are compared to large trials, and we assessed the prevalence of bias in published meta-analyses.
{DESIGN}: Medline search to identify pairs consisting of a meta-analysis and a single large trial (concordance of results was assumed if effects were in the same direction and the meta-analytic estimate was within 30\% of the trial); analysis of funnel plots from 37 meta-analyses identified from a hand search of four leading general medicine journals 1993-6 and 38 meta-analyses from the second 1996 issue of the Cochrane Database of Systematic Reviews.
{MAIN} {OUTCOME} {MEASURE}: Degree of funnel plot asymmetry as measured by the intercept from regression of standard normal deviates against precision.
{RESULTS}: In the eight pairs of meta-analysis and large trial that were identified (five from cardiovascular medicine, one from diabetic medicine, one from geriatric medicine, one from perinatal medicine) there were four concordant and four discordant pairs. In all cases discordance was due to meta-analyses showing larger effects. Funnel plot asymmetry was present in three out of four discordant pairs but in none of concordant pairs. In 14 (38\%) journal meta-analyses and 5 (13\%) Cochrane reviews, funnel plot asymmetry indicated that there was bias.
{CONCLUSIONS}: A simple analysis of funnel plots provides a useful test for the likely presence of bias in meta-analyses, but as the capacity to detect bias will be limited when meta-analyses are based on a limited number of small trials the results from such analyses should be treated with considerable caution.},
	pages = {629--634},
	number = {7109},
	journaltitle = {{BMJ} (Clinical research ed.)},
	shortjournal = {{BMJ}},
	author = {Egger, M. and Davey Smith, G. and Schneider, M. and Minder, C.},
	date = {1997-09-13},
	pmid = {9310563},
	pmcid = {PMC2127453},
	keywords = {Statistics as Topic, Meta-Analysis as Topic, Bias, Randomized Controlled Trials as Topic, Regression Analysis, Treatment Outcome},
	file = {Volltext:/Users/luca/Zotero/storage/MQ3PBZZJ/Egger et al. - 1997 - Bias in meta-analysis detected by a simple, graphi.pdf:application/pdf},
}
@misc{etz_technical_2018,
	title = {Technical Notes on Kullback-Leibler Divergence},
	url = {https://osf.io/5vhzu},
	doi = {10.31234/osf.io/5vhzu},
	abstract = {I provide some technical notes regarding the Kullback-Leibler divergence. Derivations of the Kullback-Leibler divergence are provided for Bernoulli, Geometric, Poisson, Exponential, and Normal distributions.},
	publisher = {{OSF}},
	author = {Etz, Alexander},
	urldate = {2024-05-03},
	date = {2018-09-06},
	langid = {english},
	keywords = {{KL} Divergence, Kullback-Leibler Divergence, Likelihood, Likelihood Ratio},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/HCS447V4/Etz - 2018 - Technical Notes on Kullback-Leibler Divergence.pdf:application/pdf},
}
@book{feoktistov_differential_2006,
	title = {Differential Evolution – In Search of Solutions},
	volume = {5},
	url = {https://doi.org/10.1007/978-0-387-36896-2},
	series = {Springer Optimization and Its Applications},
	number = {5},
	publisher = {Springer},
	author = {Feoktistov, Vitaliy},
	date = {2006-01-01},
	file = {(Springer Optimization and Its Applications) Vitaliy Feoktistov - Differential Evolution. In Search of Solutions-Springer (2010).pdf:/Users/luca/Zotero/storage/W7EUZ68U/(Springer Optimization and Its Applications) Vitaliy Feoktistov - Differential Evolution. In Search of Solutions-Springer (2010).pdf:application/pdf;Volltext:/Users/luca/Zotero/storage/4UBFD6CM/(Springer Optimization and Its Applications) Vitaliy Feoktistov - Differential Evolution. In Search of Solutions-Springer (2010).pdf:application/pdf},
}
@article{ferrari_beta_2004,
	title = {Beta Regression for Modelling Rates and Proportions},
	volume = {31},
	issn = {0266-4763},
	url = {https://doi.org/10.1080/0266476042000214501},
	doi = {10.1080/0266476042000214501},
	abstract = {This paper proposes a regression model where the response is beta distributed using a parameterization of the beta law that is indexed by mean and dispersion parameters. The proposed model is useful for situations where the variable of interest is continuous and restricted to the interval (0, 1) and is related to other variables through a regression structure. The regression parameters of the beta regression model are interpretable in terms of the mean of the response and, when the logit link is used, of an odds ratio, unlike the parameters of a linear regression that employs a transformed response. Estimation is performed by maximum likelihood. We provide closed-form expressions for the score function, for Fisher's information matrix and its inverse. Hypothesis testing is performed using approximations obtained from the asymptotic normality of the maximum likelihood estimator. Some diagnostic measures are introduced. Finally, practical applications that employ real data are presented and discussed.},
	pages = {799--815},
	number = {7},
	journaltitle = {Journal of Applied Statistics},
	author = {Ferrari, Silvia and Cribari-Neto, Francisco},
	urldate = {2024-04-07},
	date = {2004-08-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0266476042000214501},
	keywords = {Beta Distribution, Leverage, Maximum Likelihood Estimation, Proportions, Residuals},
}
@article{franco_publication_2014,
	title = {Publication bias in the social sciences: Unlocking the file drawer},
	volume = {345},
	url = {https://www.science.org/doi/full/10.1126/science.1255484},
	doi = {10.1126/science.1255484},
	shorttitle = {Publication bias in the social sciences},
	abstract = {We studied publication bias in the social sciences by analyzing a known population of conducted studies—221 in total—in which there is a full accounting of what is published and unpublished. We leveraged Time-sharing Experiments in the Social Sciences ({TESS}), a National Science Foundation–sponsored program in which researchers propose survey-based experiments to be run on representative samples of American adults. Because {TESS} proposals undergo rigorous peer review, the studies in the sample all exceed a substantial quality threshold. Strong results are 40 percentage points more likely to be published than are null results and 60 percentage points more likely to be written up. We provide direct evidence of publication bias and identify the stage of research production at which publication bias occurs: Authors do not write up and submit null findings.},
	pages = {1502--1505},
	number = {6203},
	journaltitle = {Science},
	author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
	urldate = {2024-01-02},
	date = {2014-09-19},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {notion},
	file = {Full Text PDF:/Users/luca/Zotero/storage/QUN44Q6N/Franco et al. - 2014 - Publication bias in the social sciences Unlocking.pdf:application/pdf},
}
@article{fritz_comprehensive_2013,
	title = {A comprehensive review of reporting practices in psychological journals: Are effect sizes really enough?},
	volume = {23},
	issn = {1461-7447},
	doi = {10.1177/0959354312436870},
	shorttitle = {A comprehensive review of reporting practices in psychological journals},
	abstract = {Over-reliance on significance testing has been heavily criticized in psychology. Therefore the American Psychological Association recommended supplementing the p value with additional elements such as effect sizes, confidence intervals, and considering statistical power seriously. This article elaborates the conclusions that can be drawn when these measures accompany the p value. An analysis of over 30 summary papers (including over 6,000 articles) reveals that, if at all, only effect sizes are reported in addition to p’s (38\%). Only every 10th article provides a confidence interval and statistical power is reported in only 3\% of articles. An increase in reporting frequency of the supplements to p’s over time owing to stricter guidelines was found for effect sizes only. Given these practices, research faces a serious problem in the context of dichotomous statistical decision making: since significant results have a higher probability of being published (publication bias), effect sizes reported in articles may be seriously overestimated. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {98--122},
	number = {1},
	journaltitle = {Theory \& Psychology},
	author = {Fritz, Astrid and Scherndl, Thomas and Kühberger, Anton},
	date = {2013},
	note = {Place: {US}
Publisher: Sage Publications},
	keywords = {Decision Making, Statistical Analysis, Statistical Power, Statistical Probability},
	file = {Fritz et al. - 2013 - A comprehensive review of reporting practices in p.pdf:/Users/luca/Zotero/storage/LASPP6PT/Fritz et al. - 2013 - A comprehensive review of reporting practices in p.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/Z5QB55W7/2013-03308-006.html:text/html},
}
@article{gerber_testing_2001,
	title = {Testing for Publication Bias in Political Science},
	volume = {9},
	issn = {1047-1987},
	url = {https://www.jstor.org/stable/25791658},
	abstract = {If the publication decisions of journals are a function of the statistical significance of research findings, the published literature may suffer from "publication bias." This paper describes a method for detecting publication bias. We point out that to achieve statistical significance, the effect size must be larger in small samples. If publications tend to be biased against statistically insignificant results, we should observe that the effect size diminishes as sample sizes increase. This proposition is tested and confirmed using the experimental literature on voter mobilization.},
	pages = {385--392},
	number = {4},
	journaltitle = {Political Analysis},
	author = {Gerber, Alan S. and Green, Donald P. and Nickerson, David},
	urldate = {2023-12-01},
	date = {2001},
	note = {Publisher: [Oxford University Press, Society for Political Methodology]},
	keywords = {notion},
	file = {Gerber et al. - 2001 - Testing for Publication Bias in Political Science.pdf:/Users/luca/Zotero/storage/UMTTCZGD/Gerber et al. - 2001 - Testing for Publication Bias in Political Science.pdf:application/pdf},
}
@inproceedings{ghoreishi_termination_2017,
	title = {Termination Criteria in Evolutionary Algorithms: A Survey},
	volume = {1},
	doi = {10.5220/0006577903730384},
	shorttitle = {Termination Criteria in Evolutionary Algorithms},
	abstract = {Over the last decades, evolutionary algorithms have been extensively used to solve multi-objective optimization problems. However, the number of required function evaluations is not determined by nature of these algorithms which is often seen as a drawback. Therefore, a robust and reliable termination criterion is needed to stop the algorithm. There is a huge amount of knowledge encapsulated in the studies targeting termination criteria in evolutionary algorithms, but an updated integrated overview of this knowledge is missing. For this reason, we aim to conduct a systematic research through a comprehensive literature study. We extended the basic categorization of termination criteria to a more advanced one that takes the most common used termination criteria into consideration based on their specifications and the way they have been evolved over time. The survey is concluded by suggesting a road-map for future research directions.},
	pages = {373--384},
	booktitle = {Proceedings of 9th International Joint Conference on Computational Intelligence},
	author = {Ghoreishi, Newsha and Clausen, Anders and Jørgensen, Bo Nørregaard},
	date = {2017},
	note = {Publisher: {SCITEPRESS} Digital Library},
	keywords = {Convergence, Evolutionary Algorithm, Evolutionary Computation, Performance Indicator, Progress Indicator, Stopping Criterion, Termination Criterion},
	file = {Eingereichte Version:/Users/luca/Zotero/storage/8TNYRIWE/Ghoreishi et al. - 2017 - Termination Criteria in Evolutionary Algorithms A.pdf:application/pdf},
}
@book{harrer_doing_2021,
	location = {Boca Raton, {FL} and London},
	edition = {1st},
	title = {Doing Meta-Analysis With R: A Hands-On Guide},
	isbn = {978-0-367-61007-4},
	publisher = {Chapman \& Hall/{CRC} Press},
	author = {Harrer, Mathias and Cuijpers, Pim and A, Furukawa Toshi and Ebert, David D},
	date = {2021},
}
@article{hedges_modeling_1992,
	title = {Modeling Publication Selection Effects in Meta-Analysis},
	volume = {7},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-7/issue-2/Modeling-Publication-Selection-Effects-in-Meta-Analysis/10.1214/ss/1177011364.full},
	doi = {10.1214/ss/1177011364},
	abstract = {Publication selection effects arise in meta-analysis when the effect magnitude estimates are observed in (available from) only a subset of the studies that were actually conducted and the probability that an estimate is observed is related to the size of that estimate. Such selection effects can lead to substantial bias in estimates of effect magnitude. Research on the selection process suggests that much of the selection occurs because researchers, reviewers and editors view the results of studies as more conclusive when they are more highly statistically significant. This suggests a model of the selection process that depends on effect magnitude via the p-value or significance level. A model of the selection process involving a step function relating the p-value to the probability of selection is introduced in the context of a random effects model for meta-analysis. The model permits estimation of a weight function representing selection along the mean and variance of effects. Some ideas for graphical procedures and a test for publication selection are also introduced. The method is then applied to a meta-analysis of test validity studies.},
	pages = {246--255},
	number = {2},
	journaltitle = {Statistical Science},
	author = {Hedges, Larry V.},
	urldate = {2023-12-27},
	date = {1992-05},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {publication bias, Meta-analysis, file-drawer problem, random effects models, selection models, weight function models},
	file = {Full Text PDF:/Users/luca/Zotero/storage/7FS3IRZT/Hedges - 1992 - Modeling Publication Selection Effects in Meta-Ana.pdf:application/pdf},
}
@article{ioannidis_why_2005,
	title = {Why Most Published Research Findings Are False},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	pages = {e124},
	number = {8},
	journaltitle = {{PLOS} Medicine},
	shortjournal = {{PLOS} Medicine},
	author = {Ioannidis, John P. A.},
	urldate = {2023-04-24},
	date = {2005-08-30},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Finance, notion, Metaanalysis, Cancer risk factors, Genetic epidemiology, Genetics of disease, Randomized controlled trials, Research design, Schizophrenia},
	file = {Full Text PDF:/Users/luca/Zotero/storage/DVBFMXRN/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf},
}
@inproceedings{ismail_handling_2007,
	title = {Handling Overdispersion with Negative Binomial and Generalized Poisson Regression Models},
	url = {https://www.semanticscholar.org/paper/Handling-Overdispersion-with-Negative-Binomial-and-Ismail-Jemain/2791e7be78958751709b7765d92958c0b295597c},
	abstract = {In actuarial hteramre, researchers suggested various statistical procedures to estimate the parameters in claim count or frequency model. In particular, the Poisson regression model, which is also known as the Generahzed Linear Model ({GLM}) with Poisson error structure, has been x{	extasciitilde}adely used in the recent years. However, it is also recognized that the count or frequency data m insurance practice often display overdispersion, i.e., a situation where the variance of the response variable exceeds the mean. Inappropriate imposition of the Poisson may underestimate the standard errors and overstate the sigruficance of the regression parameters, and consequently, giving misleading inference about the regression parameters. This paper suggests the Negative Binomial and Generalized Poisson regression models as ahemafives for handling overdispersion. If the Negative Binomial and Generahzed Poisson regression models are fitted by the maximum likelihood method, the models are considered to be convenient and practical; they handle overdispersion, they allow the likelihood ratio and other standard maximum likelihood tests to be implemented, they have good properties, and they permit the fitting procedure to be carried out by using the herative Weighted I\_,east Squares {OWLS}) regression similar to those of the Poisson. In this paper, two types of regression model will be discussed and applied; multiplicative and additive. The multiplicative and additive regression models for Poisson, Negative Binomial and Generalized Poisson will be fitted, tested and compared on three different sets of claim frequency data; Malaysian private motor third part T property' damage data, ship damage incident data from {McCuUagh} and Nelder, and data from Bailey and Simon on Canadian private automobile liabili{	extasciitilde},.},
	author = {Ismail, N. and Jemain, A.},
	urldate = {2024-05-07},
	date = {2007},
	file = {Full Text PDF:/Users/luca/Zotero/storage/E8VKJKTP/Ismail und Jemain - 2007 - Handling Overdispersion with Negative Binomial and.pdf:application/pdf},
}
@article{iyengar_selection_1988,
	title = {Selection Models and the File Drawer Problem},
	volume = {3},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/2245925},
	abstract = {Meta-analysis consists of quantitative methods for combining evidence from different studies about a particular issue. A frequent criticism of meta-analysis is that it may be based on a biased sample of all studies that were done. In this paper, we use selection models, or weighted distributions, to deal with one source of bias, namely, the failure to report studies that do not yield statistically significant results. We apply selection models to two approaches that have been suggested for correcting the bias. The fail-safe sample size approach calculates the minimum number of unpublished studies showing nonsignificant results that must have been carried out in order to overturn the conclusion reached from the published studies. The maximum likelihood approach uses a weighted distribution to model the selection bias in the generation of the data and estimates various parameters of interest. We suggest the use of families of weight functions to model plausible biasing mechanisms to study the sensitivity of inferences about effect sizes. By using an example, we show that the maximum likelihood approach has several advantages over the fail-safe sample size approach.},
	pages = {109--117},
	number = {1},
	journaltitle = {Statistical Science},
	author = {Iyengar, Satish and Greenhouse, Joel B.},
	urldate = {2024-05-02},
	date = {1988},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/DG4K39GL/Iyengar und Greenhouse - 1988 - Selection Models and the File Drawer Problem.pdf:application/pdf},
}
@inproceedings{jain_termination_2001,
	title = {On termination criteria of evolutionary algorithms},
	pages = {768--768},
	booktitle = {Proceedings of the 3rd Annual Conference on Genetic and Evolutionary Computation},
	author = {Jain, Brijnesh J. and Pohlheim, Hartmut and Wegener, Joachim},
	date = {2001},
	file = {Jain et al. - 2001 - On termination criteria of evolutionary algorithms.pdf:/Users/luca/Zotero/storage/XJ38H3UP/Jain et al. - 2001 - On termination criteria of evolutionary algorithms.pdf:application/pdf},
}
@article{jennions_publication_2002,
	title = {Publication bias in ecology and evolution: an empirical assessment using the ‘trim and fill’ method},
	volume = {77},
	issn = {1469-185X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1017/S1464793101005875},
	doi = {10.1017/S1464793101005875},
	shorttitle = {Publication bias in ecology and evolution},
	abstract = {Recent reviews of specific topics, such as the relationship between male attractiveness to females and fluctuating asymmetry or attractiveness and the expression of secondary sexual characters, suggest that publication bias might be a problem in ecology and evolution. In these cases, there is a significant negative correlation between the sample size of published studies and the magnitude or strength of the research findings (formally the ‘effect size’). If all studies that are conducted are equally likely to be published, irrespective of their findings, there should not be a directional relationship between effect size and sample size; only a decrease in the variance in effect size as sample size increases due to a reduction in sampling error. One interpretation of these reports of negative correlations is that studies with small sample sizes and weaker findings (smaller effect sizes) are less likely to be published. If the biological literature is systematically biased this could undermine the attempts of reviewers to summarise actual biology relationships by inflating estimates of average effect sizes. But how common is this problem? And does it really effect the general conclusions of literature reviews? Here, we examine data sets of effect sizes extracted from 40 peer-reviewed, published meta-analyses. We estimate how many studies are missing using the newly developed ‘trim and fill’ method. This method uses asymmetry in plots of effect size against sample size (‘funnel plots’) to detect ‘missing’ studies. For random-effect models of meta-analysis 38\% (15/40) of data sets had a significant number of ‘missing’ studies. After correcting for potential publication bias, 21\% (8/38) of weighted mean effects were no longer significantly greater than zero, and 15\% (5/34) were no longer statistically robust when we used random-effects models in a weighted meta-analysis. The mean correlation between sample size and the magnitude of standardised effect size was also significantly negative (rs=-0.20, P {	extless} 0-0001). Individual correlations were significantly negative (P {	extless} 0.10) in 35\% (14/40) of cases. Publication bias may therefore effect the main conclusions of at least 15–21\% of meta-analyses. We suggest that future literature reviews assess the robustness of their main conclusions by correcting for potential publication bias using the ‘trim and fill’ method.},
	pages = {211--222},
	number = {2},
	journaltitle = {Biological Reviews},
	author = {Jennions, Michael D. and Møller, Anders P.},
	urldate = {2023-12-01},
	date = {2002},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1017/S1464793101005875},
	keywords = {meta-analysis, notion, publication bias, effect size, fail-safe number, fluctuating asymmetry, funnel plots, trim and fill},
	file = {Jennions und Møller - 2002 - Publication bias in ecology and evolution an empi.pdf:/Users/luca/Zotero/storage/2B8UV4AU/Jennions und Møller - 2002 - Publication bias in ecology and evolution an empi.pdf:application/pdf},
}
@article{jennions_relationships_2002,
	title = {Relationships fade with time: a meta-analysis of temporal trends in publication in ecology and evolution.},
	volume = {269},
	issn = {0962-8452},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1690867/},
	doi = {10.1098/rspb.2001.1832},
	shorttitle = {Relationships fade with time},
	abstract = {Both significant positive and negative relationships between the magnitude of research findings (their 'effect size') and their year of publication have been reported in a few areas of biology. These trends have been attributed to Kuhnian paradigm shifts, scientific fads and bias in the choice of study systems. Here we test whether or not these isolated cases reflect a more general trend. We examined the relationship using effect sizes extracted from 44 peer-reviewed meta-analyses covering a wide range of topics in ecological and evolutionary biology. On average, there was a small but significant decline in effect size with year of publication. For the original empirical studies there was also a significant decrease in effect size as sample size increased. However, the effect of year of publication remained even after we controlled for sampling effort. Although these results have several possible explanations, it is suggested that a publication bias against non-significant or weaker findings offers the most parsimonious explanation. As in the medical sciences, non-significant results may take longer to publish and studies with both small sample sizes and non-significant results may be less likely to be published.},
	pages = {43--48},
	number = {1486},
	journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
	shortjournal = {Proc Biol Sci},
	author = {Jennions, Michael D and Møller, Anders P},
	urldate = {2024-02-04},
	date = {2002-01-07},
	pmid = {11788035},
	pmcid = {PMC1690867},
	file = {PubMed Central Full Text PDF:/Users/luca/Zotero/storage/ACFV77VB/Jennions und Møller - 2002 - Relationships fade with time a meta-analysis of t.pdf:application/pdf},
}
@article{kicinski_how_2014,
	title = {How does under-reporting of negative and inconclusive results affect the false-positive rate in meta-analysis? A simulation study},
	volume = {4},
	rights = {Published by the {BMJ} Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions.  This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial ({CC} {BY}-{NC} 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/3.0/},
	issn = {2044-6055, 2044-6055},
	url = {https://bmjopen.bmj.com/content/4/8/e004831},
	doi = {10.1136/bmjopen-2014-004831},
	shorttitle = {How does under-reporting of negative and inconclusive results affect the false-positive rate in meta-analysis?},
	abstract = {Objective To investigate the impact of a higher publishing probability for statistically significant positive outcomes on the false-positive rate in meta-analysis.
Design Meta-analyses of different sizes (N=10, N=20, N=50 and N=100), levels of heterogeneity and levels of publication bias were simulated.
Primary and secondary outcome measures The type I error rate for the test of the mean effect size (ie, the rate at which the meta-analyses showed that the mean effect differed from 0 when it in fact equalled 0) was estimated. Additionally, the power and type I error rate of publication bias detection methods based on the funnel plot were estimated.
Results In the presence of a publication bias characterised by a higher probability of including statistically significant positive results, the meta-analyses frequently concluded that the mean effect size differed from zero when it actually equalled zero. The magnitude of the effect of publication bias increased with an increasing number of studies and between-study variability. A higher probability of including statistically significant positive outcomes introduced little asymmetry to the funnel plot. A publication bias of a sufficient magnitude to frequently overturn the meta-analytic conclusions was difficult to detect by publication bias tests based on the funnel plot. When statistically significant positive results were four times more likely to be included than other outcomes and a large between-study variability was present, more than 90\% of the meta-analyses of 50 and 100 studies wrongly showed that the mean effect size differed from zero. In the same scenario, publication bias tests based on the funnel plot detected the bias at rates not exceeding 15\%.
Conclusions This study adds to the evidence that publication bias is a major threat to the validity of medical research and supports the usefulness of efforts to limit publication bias.},
	pages = {e004831},
	number = {8},
	journaltitle = {{BMJ} Open},
	author = {Kicinski, Michal},
	urldate = {2024-02-17},
	date = {2014-08-01},
	langid = {english},
	pmid = {25168036},
	note = {Publisher: British Medical Journal Publishing Group
Section: Medical publishing and peer review},
	keywords = {{EGGER}'S {TEST}, {FUNNEL} {PLOT}, {META}-{ANALYSIS}, {PUBLICATION} {BIAS}, {TYPE} I {ERROR}},
	file = {Full Text PDF:/Users/luca/Zotero/storage/LWNJZMM4/Kicinski - 2014 - How does under-reporting of negative and inconclus.pdf:application/pdf},
}
@book{kitcher_advancement_1993,
	location = {New York},
	title = {The Advancement of Science: Science Without Legend, Objectivity Without Illusions},
	shorttitle = {The Advancement of Science},
	publisher = {Oxford University Press},
	author = {Kitcher, Philip},
	date = {1993},
	file = {Snapshot:/Users/luca/Zotero/storage/Q4V4M2KU/KITTAO-2.html:text/html},
}
@article{klein_investigating_2014,
	title = {Investigating variation in replicability: A "many labs" replication project},
	volume = {45},
	issn = {1864-9335},
	url = {http://www.scopus.com/inward/record.url?scp=84901716350&partnerID=8YFLogxK},
	doi = {10.1027/1864-9335/a000178},
	shorttitle = {Investigating variation in replicability},
	abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect - imagined contact reducing prejudice - showed weak support for replicability. And two effects - flag priming influencing conservatism and currency priming influencing system justification - did not replicate. We compared whether the conditions such as lab versus online or {US} versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
	pages = {142--152},
	number = {3},
	journaltitle = {Social Psychology},
	author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahník, Štěpán and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and Ijzerman, Hans and John, Melissa Sue and Joy-Gaba, Jennifer A. and Kappes, Heather Barry and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and Van 'T Veer, A. E. and Vaughn, Leigh Ann and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
	urldate = {2024-02-17},
	date = {2014},
	file = {Akzeptierte Version:/Users/luca/Zotero/storage/AR6KNAU5/Klein et al. - 2014 - Investigating variation in replicability A many .pdf:application/pdf},
}
@article{klein_many_2018,
	title = {Many Labs 2: Investigating Variation in Replicability Across Samples and Settings},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918810225},
	doi = {10.1177/2515245918810225},
	shorttitle = {Many Labs 2},
	abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {	extless} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {	extless} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen’s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({	extless} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic ({WEIRD}) cultures and less {WEIRD} cultures (i.e., cultures with relatively high and low {WEIRDness} scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
	pages = {443--490},
	number = {4},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahník, Štěpán and Batra, Rishtee and Berkics, Mihály and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and Rédei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and de Bruijn, Maaike and De Schutter, Leander and Devos, Thierry and de Vries, Marieke and Doğulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-Ángel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and Gómez, Ángel and González, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and {IJzerman}, Hans and Inbar, Yoel and Innes-Ker, Åse H. and Jiménez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A. and Kamiloğlu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Knežević, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Daniël and Lazarević, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Međedović, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, Félix and Lee Nichols, Austin and Ocampo, Aaron and O’Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, Gábor and Osowiecka, Malgorzata and Packard, Grant and Pérez-Sánchez, Rolando and Petrović, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Schönbrodt, Felix D. and Sekerdej, Maciej B. and Sirlopú, David and Skorinko, Jeanine L. M. and Smith, Michael A. and Smith-Castro, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and van der Hulst, Marije and van Lange, Paul A. M. and van ’t Veer, Anna Elisabeth and Vásquez- Echeverría, Alejandro and Ann Vaughn, Leigh and Vázquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
	urldate = {2024-02-17},
	date = {2018-12-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/7WUFDPBV/Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf:application/pdf},
}
@article{kuhberger_publication_2014,
	title = {Publication Bias in Psychology: A Diagnosis Based on the Correlation between Effect Size and Sample Size},
	volume = {9},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0105825},
	doi = {10.1371/journal.pone.0105825},
	shorttitle = {Publication Bias in Psychology},
	pages = {e105825},
	number = {9},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Kühberger, Anton and Fritz, Astrid and Scherndl, Thomas},
	editor = {Fanelli, Daniele},
	urldate = {2023-05-10},
	date = {2014-09-05},
	langid = {english},
	note = {Number: 9},
	keywords = {notion},
}
@article{kullback_information_1951,
	title = {On Information and Sufficiency},
	volume = {22},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-1/On-Information-and-Sufficiency/10.1214/aoms/1177729694.full},
	doi = {10.1214/aoms/1177729694},
	abstract = {The Annals of Mathematical Statistics},
	pages = {79--86},
	number = {1},
	journaltitle = {The Annals of Mathematical Statistics},
	author = {Kullback, S. and Leibler, R. A.},
	urldate = {2024-02-05},
	date = {1951-03},
	note = {Publisher: Institute of Mathematical Statistics},
	file = {Kullback und Leibler - 1951 - On Information and Sufficiency.pdf:/Users/luca/Zotero/storage/T8BLE8SW/Kullback und Leibler - 1951 - On Information and Sufficiency.pdf:application/pdf},
}
@article{lakens_calculating_2013,
	title = {Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and {ANOVAs}},
	volume = {4},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2013.00863/full},
	doi = {10.3389/fpsyg.2013.00863},
	shorttitle = {Calculating and reporting effect sizes to facilitate cumulative science},
	abstract = {{	extless}p{	extgreater}Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for {	extless}italic{	extgreater}t{	extless}/italic{	extgreater}-tests and {ANOVA}'s such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.{	extless}/p{	extgreater}},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {Lakens, Daniel},
	urldate = {2024-05-05},
	date = {2013-11-26},
	note = {Publisher: Frontiers},
	keywords = {Cohen's d, effect sizes, eta-squared, power analysis, sample size planning},
	file = {Volltext:/Users/luca/Zotero/storage/WC2RNKZN/Lakens - 2013 - Calculating and reporting effect sizes to facilita.pdf:application/pdf},
}
@article{lakens_equivalence_2017,
	title = {Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses},
	volume = {8},
	issn = {1948-5506},
	url = {https://doi.org/10.1177/1948550617697177},
	doi = {10.1177/1948550617697177},
	shorttitle = {Equivalence Tests},
	abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests ({TOST}) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The {TOST} procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
	pages = {355--362},
	number = {4},
	journaltitle = {Social Psychological and Personality Science},
	author = {Lakens, Daniël},
	urldate = {2024-01-28},
	date = {2017-05-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {notion},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/FFT5TWIM/Lakens - 2017 - Equivalence Tests A Practical Primer for t Tests,.pdf:application/pdf},
}
@article{lakens_equivalence_2018,
	title = {Equivalence Testing for Psychological Research: A Tutorial},
	volume = {1},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245918770963},
	doi = {10.1177/2515245918770963},
	shorttitle = {Equivalence Testing for Psychological Research},
	abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests ({TOST}) procedure to test for equivalence and reject the presence of a smallest effect size of interest ({SESOI}). The {TOST} procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the {SESOI} exists. We explain a range of approaches to determine the {SESOI} in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
	pages = {259--269},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Lakens, Daniël and Scheel, Anne M. and Isager, Peder M.},
	urldate = {2024-01-22},
	date = {2018-06-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/UPDF82T8/Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf:application/pdf},
}
@article{lakens_improving_2020,
	title = {Improving Inferences About Null Effects With Bayes Factors and Equivalence Tests},
	volume = {75},
	issn = {1079-5014},
	url = {https://doi.org/10.1093/geronb/gby065},
	doi = {10.1093/geronb/gby065},
	abstract = {Researchers often conclude an effect is absent when a null-hypothesis significance test yields a nonsignificant p value. However, it is neither logically nor statistically correct to conclude an effect is absent when a hypothesis test is not significant. We present two methods to evaluate the presence or absence of effects: Equivalence testing (based on frequentist statistics) and Bayes factors (based on Bayesian statistics). In four examples from the gerontology literature, we illustrate different ways to specify alternative models that can be used to reject the presence of a meaningful or predicted effect in hypothesis tests. We provide detailed explanations of how to calculate, report, and interpret Bayes factors and equivalence tests. We also discuss how to design informative studies that can provide support for a null model or for the absence of a meaningful effect. The conceptual differences between Bayes factors and equivalence tests are discussed, and we also note when and why they might lead to similar or different inferences in practice. It is important that researchers are able to falsify predictions or can quantify the support for predicted null effects. Bayes factors and equivalence tests provide useful statistical tools to improve inferences about null effects.},
	pages = {45--57},
	number = {1},
	journaltitle = {The Journals of Gerontology: Series B},
	shortjournal = {The Journals of Gerontology: Series B},
	author = {Lakens, Daniël and {McLatchie}, Neil and Isager, Peder M and Scheel, Anne M and Dienes, Zoltan},
	urldate = {2024-04-23},
	date = {2020-01-01},
	file = {Full Text PDF:/Users/luca/Zotero/storage/FGS7RWR6/Lakens et al. - 2020 - Improving Inferences About Null Effects With Bayes.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/RD8RSM5X/5033832.html:text/html},
}
@article{lakens_performing_2014,
	title = {Performing high-powered studies efficiently with sequential analyses},
	volume = {44},
	rights = {Copyright © 2014 John Wiley \& Sons, Ltd.},
	issn = {1099-0992},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2023},
	doi = {10.1002/ejsp.2023},
	abstract = {Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre-registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null-hypothesis significance testing ({NHST}) are discussed. Sequential analyses, which are widely used in large-scale medical trials, provide an efficient way to perform high-powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research. Copyright © 2014 John Wiley \& Sons, Ltd.},
	pages = {701--710},
	number = {7},
	journaltitle = {European Journal of Social Psychology},
	author = {Lakens, Daniël},
	urldate = {2024-02-10},
	date = {2014},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ejsp.2023},
	file = {Snapshot:/Users/luca/Zotero/storage/YBUY3TWT/ejsp.html:text/html;Volltext:/Users/luca/Zotero/storage/KVCKPBYJ/Lakens - 2014 - Performing high-powered studies efficiently with s.pdf:application/pdf},
}
@software{lakens_toster_2023,
	title = {{TOSTER}: Two One-Sided Tests ({TOST}) Equivalence Testing},
	rights = {{GPL}-3},
	url = {https://cran.r-project.org/web/packages/TOSTER/index.html},
	shorttitle = {{TOSTER}},
	abstract = {Two one-sided tests ({TOST}) procedure to test equivalence for t-tests, correlations, differences between proportions, and meta-analyses, including power analysis for t-tests and correlations. Allows you to specify equivalence bounds in raw scale units or in terms of effect sizes. See: Lakens (2017) {	extless}doi:10.1177/1948550617697177{	extgreater}.},
	version = {0.8.0},
	author = {Lakens, Daniel and Caldwell, Aaron},
	urldate = {2024-03-04},
	date = {2023-09-14},
}
@article{levine_sample_2009,
	title = {Sample Sizes and Effect Sizes are Negatively Correlated in Meta-Analyses: Evidence and Implications of a Publication Bias Against {NonSignificant} Findings},
	volume = {76},
	issn = {0363-7751},
	url = {https://doi.org/10.1080/03637750903074685},
	doi = {10.1080/03637750903074685},
	shorttitle = {Sample Sizes and Effect Sizes are Negatively Correlated in Meta-Analyses},
	abstract = {Meta-analysis involves cumulating effects across studies in order to qualitatively summarize existing literatures. A recent finding suggests that the effect sizes reported in meta-analyses may be negatively correlated with study sample sizes. This prediction was tested with a sample of 51 published meta-analyses summarizing the results of 3,602 individual studies. The correlation between effect size and sample size was negative in almost 80 percent of the meta-analyses examined, and the negative correlation was not limited to a particular type of research or substantive area. This result most likely stems from a bias against publishing findings that are not statistically significant. The primary implication is that meta-analyses may systematically overestimate population effect sizes. It is recommended that researchers routinely examine the n–r scatter plot and correlation, or some other indication of publication bias and report this information in meta-analyses.},
	pages = {286--302},
	number = {3},
	journaltitle = {Communication Monographs},
	author = {Levine, Timothy R. and Asada, Kelli J. and Carpenter, Chris},
	urldate = {2023-06-04},
	date = {2009-09-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/03637750903074685},
	keywords = {notion, Effect Size, Meta-Analysis, Publication Bias},
	file = {Levine et al. - 2009 - Sample Sizes and Effect Sizes are Negatively Corre.pdf:/Users/luca/Zotero/storage/28V95VIF/Levine et al. - 2009 - Sample Sizes and Effect Sizes are Negatively Corre.pdf:application/pdf},
}
@book{light_summing_1984,
	title = {Summing up: The science of reviewing research.},
	url = {https://scholars.unh.edu/psych_facpub/194},
	shorttitle = {Summing up},
	publisher = {Harvard University Press},
	author = {Light, Richard and Pillemer, David},
	date = {1984-10-01},
	file = {"Summing up\: The science of reviewing research." by Richard J. Light and David B. Pillemer:/Users/luca/Zotero/storage/MZT78TNA/194.html:text/html;Light und Pillemer - 1984 - Summing up The science of reviewing research..pdf:/Users/luca/Zotero/storage/WMDSNYVP/Light und Pillemer - 1984 - Summing up The science of reviewing research..pdf:application/pdf},
}
@article{linden_heterogeneity_2021,
	title = {Heterogeneity of Research Results: A New Perspective From Which to Assess and Promote Progress in Psychological Science},
	volume = {16},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620964193},
	doi = {10.1177/1745691620964193},
	shorttitle = {Heterogeneity of Research Results},
	abstract = {Heterogeneity emerges when multiple close or conceptual replications on the same subject produce results that vary more than expected from the sampling error. Here we argue that unexplained heterogeneity reflects a lack of coherence between the concepts applied and data observed and therefore a lack of understanding of the subject matter. Typical levels of heterogeneity thus offer a useful but neglected perspective on the levels of understanding achieved in psychological science. Focusing on continuous outcome variables, we surveyed heterogeneity in 150 meta-analyses from cognitive, organizational, and social psychology and 57 multiple close replications. Heterogeneity proved to be very high in meta-analyses, with powerful moderators being conspicuously absent. Population effects in the average meta-analysis vary from small to very large for reasons that are typically not understood. In contrast, heterogeneity was moderate in close replications. A newly identified relationship between heterogeneity and effect size allowed us to make predictions about expected heterogeneity levels. We discuss important implications for the formulation and evaluation of theories in psychology. On the basis of insights from the history and philosophy of science, we argue that the reduction of heterogeneity is important for progress in psychology and its practical applications, and we suggest changes to our collective research practice toward this end.},
	pages = {358--376},
	number = {2},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Linden, Audrey Helen and Hönekopp, Johannes},
	urldate = {2023-11-29},
	date = {2021-03-01},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {notion},
	file = {Full Text PDF:/Users/luca/Zotero/storage/GU9IGIU7/Linden und Hönekopp - 2021 - Heterogeneity of Research Results A New Perspecti.pdf:application/pdf},
}
@article{linden_publication_2024,
	title = {Publication Bias in Psychology: A Closer Look at the Correlation Between Sample Size and Effect Size},
	url = {https://osf.io/s4znd},
	doi = {10.31234/osf.io/s4znd},
	shorttitle = {Publication Bias in Psychology},
	abstract = {Previously observed negative correlations between sample size and effect size (n-{ES} correlation) in psychological research have been interpreted as evidence for publication bias and related undesirable biases. Here, we present two studies aimed at better understanding to what extent negative n-{ES} correlations reflect such biases or might be explained by unproblematic adjustments of sample size to expected effect sizes. In Study 1, we analysed n-{ES} correlations in 150 meta-analyses from cognitive, organizational, and social psychology and in 57 multiple replications, which are free from relevant biases. In Study 2, we used a random sample of 160 psychology papers to compare the n-{ES} correlation for effects that are central to these papers and effects selected at random from these papers. n-{ES} correlations proved inconspicuous in meta-analyses. In line with previous research, they do not suggest that publication bias and related biases have a strong impact on meta-analyses in psychology. A much higher n-{ES} correlation emerged for publications’ focal effects. To what extent this should be attributed to publication bias and related biases remains unclear.},
	author = {Linden, Audrey Helen and Pollet, Thomas V. and Hönekopp, Johannes},
	urldate = {2024-01-21},
	date = {2024-01-21},
	langid = {english},
	note = {Publisher: {OSF}},
	keywords = {notion},
	file = {ATJ publication bias R1 final preprint.docx:/Users/luca/Zotero/storage/2VME7AP9/ATJ publication bias R1 final preprint.docx:application/vnd.openxmlformats-officedocument.wordprocessingml.document;ATJ publication bias R1 final preprint.pdf:/Users/luca/Zotero/storage/AI5QHZDE/linden_etal_2023.pdf:application/pdf;linden_etal_2023.pdf:/Users/luca/Zotero/storage/WDWZX785/linden_etal_2023.pdf:application/pdf;linden_etal_2023.pdf:/Users/luca/Zotero/storage/UXQN38L3/linden_etal_2023.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/HAGRECZX/s4znd.html:text/html},
}
@article{lloyd-smith_maximum_2007,
	title = {Maximum Likelihood Estimation of the Negative Binomial Dispersion Parameter for Highly Overdispersed Data, with Applications to Infectious Diseases},
	volume = {2},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1791715/},
	doi = {10.1371/journal.pone.0000180},
	abstract = {Background
The negative binomial distribution is used commonly throughout biology as a model for overdispersed count data, with attention focused on the negative binomial dispersion parameter, k. A substantial literature exists on the estimation of k, but most attention has focused on datasets that are not highly overdispersed (i.e., those with k≥1), and the accuracy of confidence intervals estimated for k is typically not explored.

Methodology
This article presents a simulation study exploring the bias, precision, and confidence interval coverage of maximum-likelihood estimates of k from highly overdispersed distributions. In addition to exploring small-sample bias on negative binomial estimates, the study addresses estimation from datasets influenced by two types of event under-counting, and from disease transmission data subject to selection bias for successful outbreaks.

Conclusions
Results show that maximum likelihood estimates of k can be biased upward by small sample size or under-reporting of zero-class events, but are not biased downward by any of the factors considered. Confidence intervals estimated from the asymptotic sampling variance tend to exhibit coverage below the nominal level, with overestimates of k comprising the great majority of coverage errors. Estimation from outbreak datasets does not increase the bias of k estimates, but can add significant upward bias to estimates of the mean. Because k varies inversely with the degree of overdispersion, these findings show that overestimation of the degree of overdispersion is very rare for these datasets.},
	pages = {e180},
	number = {2},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} One},
	author = {Lloyd-Smith, James O.},
	urldate = {2024-05-07},
	date = {2007-02-14},
	pmid = {17299582},
	pmcid = {PMC1791715},
	file = {Full Text PDF:/Users/luca/Zotero/storage/IX4UIXR8/Lloyd-Smith - 2007 - Maximum Likelihood Estimation of the Negative Bino.pdf:application/pdf},
}
@article{lovakov_empirically_2021,
	title = {Empirically derived guidelines for effect size interpretation in social psychology},
	volume = {51},
	issn = {1099-0992},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2752},
	doi = {10.1002/ejsp.2752},
	abstract = {This study estimates empirically derived guidelines for effect size interpretation for research in social psychology overall and sub-disciplines within social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis of empirically derived distributions of 12,170 correlation coefficients and 6,447 Cohen's d statistics extracted from studies included in 134 published meta-analyses revealed that the 25th, 50th, and 75th percentiles corresponded to correlation coefficient values of 0.12, 0.24, and 0.41 and to Cohen's d values of 0.15, 0.36, and 0.65 respectively. The analysis suggests that the widely used Cohen's guidelines tend to overestimate medium and large effect sizes. Empirically derived effect size distributions in social psychology overall and its sub-disciplines can be used both for effect size interpretation and for sample size planning when other information about effect size is not available.},
	pages = {485--504},
	number = {3},
	journaltitle = {European Journal of Social Psychology},
	author = {Lovakov, Andrey and Agadullina, Elena R.},
	urldate = {2024-01-10},
	date = {2021},
	langid = {english},
	note = {Number: 3
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ejsp.2752},
	keywords = {effect size, sample size, correlation, Cohen's d},
}
@article{marks-anglin_historical_2020,
	title = {A historical review of publication bias},
	volume = {11},
	rights = {© 2020 John Wiley \& Sons Ltd},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1452},
	doi = {10.1002/jrsm.1452},
	abstract = {Publication bias is a well-known threat to the validity of meta-analyses and, more broadly, the reproducibility of scientific findings. When policies and recommendations are predicated on an incomplete evidence base, it undermines the goals of evidence-based decision-making. Great strides have been made in the last 50 years to understand and address this problem, including calls for mandatory trial registration and the development of statistical methods to detect and correct for publication bias. We offer an historical account of seminal contributions by the evidence synthesis community, with an emphasis on the parallel development of graph-based and selection model approaches. We also draw attention to current innovations and opportunities for future methodological work.},
	pages = {725--742},
	number = {6},
	journaltitle = {Research Synthesis Methods},
	author = {Marks-Anglin, Arielle and Chen, Yong},
	urldate = {2024-01-02},
	date = {2020},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1452},
	keywords = {meta-analysis, reproducibility, publication bias, selection bias, evidence-based medicine},
	file = {Full Text PDF:/Users/luca/Zotero/storage/NUPDB6WI/Marks-Anglin und Chen - 2020 - A historical review of publication bias.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/XRLDJKE5/jrsm.html:text/html},
}
@article{marszalek_sample_2011,
	title = {Sample size in psychological research over the past 30 years},
	volume = {112},
	issn = {0031-5125},
	doi = {10.2466/03.11.PMS.112.2.331-348},
	abstract = {The American Psychological Association ({APA}) Task Force on Statistical Inference was formed in 1996 in response to a growing body of research demonstrating methodological issues that threatened the credibility of psychological research, and made recommendations to address them. One issue was the small, even dramatically inadequate, size of samples used in studies published by leading journals. The present study assessed the progress made since the Task Force's final report in 1999. Sample sizes reported in four leading {APA} journals in 1955, 1977, 1995, and 2006 were compared using nonparametric statistics, while data from the last two waves were fit to a hierarchical generalized linear growth model for more in-depth analysis. Overall, results indicate that the recommendations for increasing sample sizes have not been integrated in core psychological research, although results slightly vary by field. This and other implications are discussed in the context of current methodological critique and practice.},
	pages = {331--348},
	number = {2},
	journaltitle = {Perceptual and Motor Skills},
	shortjournal = {Percept Mot Skills},
	author = {Marszalek, Jacob M. and Barber, Carolyn and Kohlhart, Julie and Holmes, Cooper B.},
	date = {2011-04},
	pmid = {21667745},
	note = {Number: 2},
	keywords = {Humans, Reproducibility of Results, Psychology, Publishing, Research, Forecasting, Periodicals as Topic, Data Interpretation, Statistical, Sample Size, Bias, Editorial Policies, Linear Models, Statistics, Nonparametric},
}
@article{mcshane_abandon_2019,
	title = {Abandon Statistical Significance},
	volume = {73},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1527253},
	doi = {10.1080/00031305.2018.1527253},
	pages = {235--245},
	issue = {sup1},
	journaltitle = {The American Statistician},
	shortjournal = {The American Statistician},
	author = {{McShane}, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
	urldate = {2024-05-02},
	date = {2019-03-29},
	langid = {english},
	file = {Full Text PDF:/Users/luca/Zotero/storage/K9Z93DL4/McShane et al. - 2019 - Abandon Statistical Significance.pdf:application/pdf},
}
@article{merkel_docker_2014,
	title = {Docker: lightweight Linux containers for consistent development and deployment},
	volume = {2014},
	issn = {1075-3583},
	shorttitle = {Docker},
	abstract = {Docker promises the ability to package applications and their dependencies into lightweight containers that move easily between different distros, start up quickly and are isolated from each other.},
	pages = {2:2},
	number = {239},
	journaltitle = {Linux Journal},
	shortjournal = {Linux J.},
	author = {Merkel, Dirk},
	date = {2014-03-01},
}
@misc{molder_sustainable_2021,
	title = {Sustainable data analysis with Snakemake},
	rights = {http://creativecommons.org/licenses/by/4.0/},
	url = {https://f1000research.com/articles/10-33},
	doi = {10.12688/f1000research.29032.1},
	abstract = {Data analysis often entails a multitude of heterogeneous steps, from the application of various command line tools to the usage of scripting languages like R or Python for the generation of plots and tables. It is widely recognized that data analyses should ideally be conducted in a reproducible way.\&nbsp;Reproducibility enables technical validation and regeneration of results on the original or even new data. However, reproducibility alone is by no means sufficient to deliver an analysis that is of lasting impact (i.e., sustainable) for the field, or even just one research group. We postulate that it is equally important to ensure adaptability and transparency. The former describes the ability to modify the analysis to answer extended or slightly different research questions. The latter describes the ability to understand the analysis in order to judge whether it is not only technically, but methodologically valid. Here, we analyze the properties needed for a data analysis to become reproducible, adaptable, and transparent. We show how the popular workflow management system Snakemake can be used to guarantee this, and how it enables an ergonomic, combined, unified representation of all steps involved in data analysis, ranging from raw data processing, to quality control and fine-grained, interactive exploration and plotting of final results.},
	number = {10:33},
	publisher = {F1000Research},
	author = {Mölder, Felix and Jablonski, Kim Philipp and Letcher, Brice and Hall, Michael B. and Tomkins-Tinch, Christopher H. and Sochat, Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O. and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and Rahmann, Sven and Nahnsen, Sven and Köster, Johannes},
	urldate = {2024-04-24},
	date = {2021-01-18},
	langid = {english},
	keywords = {reproducibility, transparency, adaptability, data analysis, scalability, sustainability, workflow management},
	file = {Full Text PDF:/Users/luca/Zotero/storage/E8KEE5ZH/Mölder et al. - 2021 - Sustainable data analysis with Snakemake.pdf:application/pdf},
}
@article{moller_how_2001,
	title = {How important are direct fitness benefits of sexual selection?},
	volume = {88},
	issn = {1432-1904},
	url = {https://doi.org/10.1007/s001140100255},
	doi = {10.1007/s001140100255},
	abstract = {Females may choose mates based on the expression of secondary sexual characters that signal direct, material fitness benefits or indirect, genetic fitness benefits. Genetic benefits are acquired in the generation subsequent to that in which mate choice is performed, and the maintenance of genetic variation in viability has been considered a theoretical problem. Consequently, the magnitude of indirect benefits has traditionally been considered to be small. Direct fitness benefits can be maintained without consideration of mechanisms sustaining genetic variability, and they have thus been equated with the default benefits acquired by choosy females. There is, however, still debate as to whether or not males should honestly advertise direct benefits such as their willingness to invest in parental care. We use meta-analysis to estimate the magnitude of direct fitness benefits in terms of fertility, fecundity and two measures of paternal care (feeding rate in birds, hatching rate in male guarding ectotherms) based on an extensive literature survey. The mean coefficients of determination weighted by sample size were 6.3\%, 2.3\%, 1.3\% and 23.6\%, respectively. This compares to a mean weighted coefficient of determination of 1.5\% for genetic viability benefits in studies of sexual selection. Thus, for several fitness components, direct benefits are only slightly more important than indirect ones arising from female choice. Hatching rate in male guarding ectotherms was by far the most important direct fitness component, explaining almost a quarter of the variance. Our analysis also shows that male sexual advertisements do not always reliably signal direct fitness benefits.},
	pages = {401--415},
	number = {10},
	journaltitle = {Naturwissenschaften},
	shortjournal = {Naturwissenschaften},
	author = {Møller, A. and Jennions, M.},
	urldate = {2024-02-04},
	date = {2001-10-01},
	langid = {english},
	file = {Full Text PDF:/Users/luca/Zotero/storage/J6RQJJKW/Møller und Jennions - 2001 - How important are direct fitness benefits of sexua.pdf:application/pdf},
}
@article{munafo_how_2010,
	title = {How reliable are scientific studies?},
	volume = {197},
	issn = {0007-1250, 1472-1465},
	url = {https://www.cambridge.org/core/journals/the-british-journal-of-psychiatry/article/how-reliable-are-scientific-studies/96B11308710296966BF4B90EBA4F0DF2},
	doi = {10.1192/bjp.bp.109.069849},
	abstract = {{SummaryThere} is growing concern that a substantial proportion of scientific research may in fact be false. A number of factors have been proposed as contributing to the presence of a large number of false-positive results in the literature, one of which is publication bias. We discuss empirical evidence for these factors.},
	pages = {257--258},
	number = {4},
	journaltitle = {The British Journal of Psychiatry},
	author = {Munafò, Marcus R. and Flint, Jonathan},
	urldate = {2024-02-17},
	date = {2010-10},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Full Text PDF:/Users/luca/Zotero/storage/3IS2KBMC/Munafò und Flint - 2010 - How reliable are scientific studies.pdf:application/pdf},
}
@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	rights = {2017 Macmillan Publishers Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
	pages = {1--10},
	number = {1},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	urldate = {2024-01-01},
	date = {2017-01-10},
	langid = {english},
	keywords = {notion, Social sciences},
	file = {Full Text PDF:/Users/luca/Zotero/storage/3H25BTIW/Munafò et al. - 2017 - A manifesto for reproducible science.pdf:application/pdf},
}
@article{nelder_simplex_1965,
	title = {A Simplex Method for Function Minimization},
	volume = {7},
	issn = {0010-4620},
	url = {https://doi.org/10.1093/comjnl/7.4.308},
	doi = {10.1093/comjnl/7.4.308},
	abstract = {A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.},
	pages = {308--313},
	number = {4},
	journaltitle = {The Computer Journal},
	shortjournal = {The Computer Journal},
	author = {Nelder, J. A. and Mead, R.},
	urldate = {2024-04-20},
	date = {1965-01-01},
	file = {Nelder und Mead - 1965 - A Simplex Method for Function Minimization.pdf:/Users/luca/Zotero/storage/LDX5PDQ7/Nelder und Mead - 1965 - A Simplex Method for Function Minimization.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/3H9VYS9J/354237.html:text/html},
}
@article{nosek_registered_2014,
	title = {Registered Reports: A Method to Increase the Credibility of Published Results},
	volume = {45},
	issn = {1864-9335},
	url = {https://econtent.hogrefe.com/doi/full/10.1027/1864-9335/a000192},
	doi = {10.1027/1864-9335/a000192},
	pages = {137--141},
	number = {3},
	journaltitle = {Social Psychology},
	author = {Nosek, Brian A. and Lakens, Daniël},
	urldate = {2024-04-15},
	date = {2014-05},
	note = {Publisher: Hogrefe Publishing},
	file = {Full Text PDF:/Users/luca/Zotero/storage/Y2R6QK3I/Nosek und Lakens - 2014 - Registered Reports.pdf:application/pdf},
}
@article{nosek_scientific_2012,
	title = {Scientific Utopia: {II}. Restructuring Incentives and Practices to Promote Truth Over Publishability},
	volume = {7},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691612459058},
	doi = {10.1177/1745691612459058},
	shorttitle = {Scientific Utopia},
	abstract = {An academic scientist’s professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but abstract accuracy motive—getting it right—competitive with the more tangible and concrete incentive—getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases.},
	pages = {615--631},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Nosek, Brian A. and Spies, Jeffrey R. and Motyl, Matt},
	urldate = {2024-02-11},
	date = {2012-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/CEBH7A6R/Nosek et al. - 2012 - Scientific Utopia II. Restructuring Incentives an.pdf:application/pdf},
}
@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	url = {https://www.science.org/doi/full/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	pages = {aac4716},
	number = {6251},
	journaltitle = {Science},
	author = {{Open Science Collaboration}},
	urldate = {2023-12-01},
	date = {2015-08-28},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {notion},
	file = {Full Text PDF:/Users/luca/Zotero/storage/NEC76IE4/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf:application/pdf},
}
@article{palmer_detecting_1999,
	title = {Detecting Publication Bias in Meta‐analyses: A Case Study of Fluctuating Asymmetry and Sexual Selection.},
	volume = {154},
	issn = {0003-0147},
	url = {https://www.journals.uchicago.edu/doi/10.1086/303223},
	doi = {10.1086/303223},
	shorttitle = {Detecting Publication Bias in Meta‐analyses},
	pages = {220--233},
	number = {2},
	journaltitle = {The American Naturalist},
	author = {Palmer, A. Richard},
	urldate = {2024-02-04},
	date = {1999-08},
	note = {Publisher: The University of Chicago Press},
	keywords = {selective reporting, developmental stability, funnel graph, investigator effects, mating behavior, sexual signaling, statistical methods},
	file = {Palmer - 1999 - Detecting Publication Bias in Meta‐analyses A Cas.pdf:/Users/luca/Zotero/storage/VUNN4FV7/Palmer - 1999 - Detecting Publication Bias in Meta‐analyses A Cas.pdf:application/pdf},
}
@software{r_core_team_r_2023,
	location = {Vienna},
	title = {R: A Language and Environment for Statistical Computing},
	url = {https://www.R-project.org},
	version = {4.4.0},
	publisher = {R Foundation for Statistical Computing},
	author = {{R Core Team}},
	date = {2023},
}
@article{renkewitz_how_2019,
	title = {How to Detect Publication Bias in Psychological Research},
	volume = {227},
	issn = {2190-8370},
	url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000386},
	doi = {10.1027/2151-2604/a000386},
	abstract = {. Publication biases and questionable research practices are assumed to be two of the main causes of low replication rates. Both of these problems lead to severely inflated effect size estimates in meta-analyses. Methodologists have proposed a number of statistical tools to detect such bias in meta-analytic results. We present an evaluation of the performance of six of these tools. To assess the Type I error rate and the statistical power of these methods, we simulated a large variety of literatures that differed with regard to true effect size, heterogeneity, number of available primary studies, and sample sizes of these primary studies; furthermore, simulated studies were subjected to different degrees of publication bias. Our results show that across all simulated conditions, no method consistently outperformed the others. Additionally, all methods performed poorly when true effect sizes were heterogeneous or primary studies had a small chance of being published, irrespective of their results. This suggests that in many actual meta-analyses in psychology, bias will remain undiscovered no matter which detection method is used.},
	pages = {261--279},
	number = {4},
	journaltitle = {Zeitschrift für Psychologie},
	author = {Renkewitz, Frank and Keiner, Melanie},
	urldate = {2023-11-30},
	date = {2019-10},
	note = {Publisher: Hogrefe Publishing},
	keywords = {meta-analysis, notion, publication bias, bias detection, heterogeneity, optional stopping},
	file = {Full Text PDF:/Users/luca/Zotero/storage/TSW6R9JI/Renkewitz und Keiner - 2019 - How to Detect Publication Bias in Psychological Re.pdf:application/pdf},
}
@article{rosenthal_file_1979,
	title = {The file drawer problem and tolerance for null results},
	volume = {86},
	issn = {1939-1455},
	doi = {10.1037/0033-2909.86.3.638},
	abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {638--641},
	number = {3},
	journaltitle = {Psychological Bulletin},
	author = {Rosenthal, Robert},
	date = {1979},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Experimentation, Scientific Communication, Statistical Probability, Statistical Tests, Type I Errors},
	file = {Rosenthal - 1979 - The file drawer problem and tolerance for null res.pdf:/Users/luca/Zotero/storage/ETVSCNZ8/Rosenthal - 1979 - The file drawer problem and tolerance for null res.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/U7C5RPAQ/1979-27602-001.html:text/html},
}
@book{rothstein_publication_2005,
	title = {Publication Bias in Meta‐Analysis: Prevention, Assessment and Adjustments},
	url = {https://doi.org/10.1002/0470870168},
	publisher = {John Wiley \& Sons},
	author = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michael},
	date = {2005},
	file = {Rothstein et al. - 2005 - Publication Bias in Meta‐Analysis Prevention, Ass.pdf:/Users/luca/Zotero/storage/3QBACNXS/Rothstein et al. - 2005 - Publication Bias in Meta‐Analysis Prevention, Ass.pdf:application/pdf},
}
@article{sassenberg_research_2019,
	title = {Research in Social Psychology Changed Between 2011 and 2016: Larger Sample Sizes, More Self-Report Measures, and More Online Studies},
	volume = {2},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/2515245919838781},
	doi = {10.1177/2515245919838781},
	shorttitle = {Research in Social Psychology Changed Between 2011 and 2016},
	abstract = {The debate about false positives in psychological research has led to a demand for higher statistical power. To meet this demand, researchers need to collect data from larger samples—which is important to increase replicability, but can be costly in both time and money (i.e., remuneration of participants). Given that researchers might need to compensate for these higher costs, we hypothesized that larger sample sizes might have been accompanied by more frequent use of less costly research methods (i.e., online data collection and self-report measures). To test this idea, we analyzed social psychology studies published in 2009, 2011, 2016, and 2018. Indeed, research reported in 2016 and 2018 (vs. 2009 and 2011) had larger sample sizes and relied more on online data collection and self-report measures. Thus, over these years, research improved in its statistical power, but also changed with regard to the methods applied. Implications for social psychology as a discipline are discussed.},
	pages = {107--114},
	number = {2},
	journaltitle = {Advances in Methods and Practices in Psychological Science},
	author = {Sassenberg, Kai and Ditrich, Lara},
	urldate = {2024-02-04},
	date = {2019-06-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/8QMC7KGK/Sassenberg und Ditrich - 2019 - Research in Social Psychology Changed Between 2011.pdf:application/pdf},
}
@article{schuirmann_comparison_1987,
	title = {A comparison of the Two One-Sided Tests Procedure and the Power Approach for assessing the equivalence of average bioavailability},
	volume = {15},
	issn = {0090-466X},
	url = {https://doi.org/10.1007/BF01068419},
	doi = {10.1007/BF01068419},
	abstract = {The statistical test of the hypothesis of no difference between the average bioavailabilities of two drug formulations, usually supplemented by an assessment of what the power of the statistical test would have been if the true averages had been inequivalent, continues to be used in the statistical analysis of bioavailability/bioequivalence studies. In the present article, this Power Approach (which in practice usually consists of testing the hypothesis of no difference at level 0.05 and requiring an estimated power of 0.80) is compared to another statistical approach, the Two One-Sided Tests Procedure, which leads to the same conclusion as the approach proposed by Westlake (2) based on the usual (shortest) 1–2α confidence interval for the true average difference. It is found that for the specific choice of α=0.05 as the nominal level of the one-sided tests, the two one-sided tests procedure has uniformly superior properties to the power approach in most cases. The only cases where the power approach has superior properties when the true averages are equivalent correspond to cases where the chance of concluding equivalence with the power approach when the true averages are notequivalent exceeds 0.05. With appropriate choice of the nominal level of significance of the one-sided tests, the two one-sided tests procedure always has uniformly superior properties to the power approach. The two one-sided tests procedure is compared to the procedure proposed by Hauck and Anderson (1).},
	pages = {657--680},
	number = {6},
	journaltitle = {Journal of Pharmacokinetics and Biopharmaceutics},
	shortjournal = {Journal of Pharmacokinetics and Biopharmaceutics},
	author = {Schuirmann, Donald J.},
	urldate = {2024-04-24},
	date = {1987-12-01},
	langid = {english},
	keywords = {bioavailability, bioequivalence, hypothesis testing, interval hypotheses},
	file = {Full Text PDF:/Users/luca/Zotero/storage/GTN6I5NX/Schuirmann - 1987 - A comparison of the Two One-Sided Tests Procedure .pdf:application/pdf},
}
@article{sheather_reliable_1991,
	title = {A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation},
	volume = {53},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2345597},
	abstract = {We present a new method for data-based selection of the bandwidth in kernel density estimation which has excellent properties. It improves on a recent procedure of Park and Marron (which itself is a good method) in various ways. First, the new method has superior theoretical performance; second, it also has a computational advantage; third, the new method has reliably good performance for smooth densities in simulations, performance that is second to none in the existing literature. These methods are based on choosing the bandwidth to (approximately) minimize good quality estimates of the mean integrated squared error. The key to the success of the current procedure is the reintroduction of a non-stochastic term which was previously omitted together with use of the bandwidth to reduce bias in estimation without inflating variance.},
	pages = {683--690},
	number = {3},
	journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Sheather, S. J. and Jones, M. C.},
	urldate = {2024-03-08},
	date = {1991},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/8MBB9P2D/Sheather und Jones - 1991 - A Reliable Data-Based Bandwidth Selection Method f.pdf:application/pdf},
}
@article{shen_samples_2011,
	title = {Samples in applied psychology: Over a decade of research in review},
	volume = {96},
	issn = {1939-1854},
	doi = {10.1037/a0023322},
	shorttitle = {Samples in applied psychology},
	abstract = {This study examines sample characteristics of articles published in Journal of Applied Psychology ({JAP}) from 1995 to 2008. At the individual level, the overall median sample size over the period examined was approximately 173, which is generally adequate for detecting the average magnitude of effects of primary interest to researchers who publish in {JAP}. Samples using higher units of analyses (e.g., teams, departments/work units, and organizations) had lower median sample sizes (Mdn ≈ 65), yet were arguably robust given typical multilevel design choices of {JAP} authors despite the practical constraints of collecting data at higher units of analysis. A substantial proportion of studies used student samples ({	extasciitilde}40\%); surprisingly, median sample sizes for student samples were smaller than working adult samples. Samples were more commonly occupationally homogeneous ({	extasciitilde}70\%) than occupationally heterogeneous. U.S. and English-speaking participants made up the vast majority of samples, whereas Middle Eastern, African, and Latin American samples were largely unrepresented. On the basis of study results, recommendations are provided for authors, editors, and readers, which converge on 3 themes: (a) appropriateness and match between sample characteristics and research questions, (b) careful consideration of statistical power, and (c) the increased popularity of quantitative synthesis. Implications are discussed in terms of theory building, generalizability of research findings, and statistical power to detect effects. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {1055--1064},
	number = {5},
	journaltitle = {Journal of Applied Psychology},
	author = {Shen, Winny and Kiger, Thomas B. and Davies, Stacy E. and Rasch, Rena L. and Simon, Kara M. and Ones, Deniz S.},
	date = {2011},
	note = {Number: 5
Place: {US}
Publisher: American Psychological Association},
	keywords = {Methodology, Statistical Power, Applied Psychology, Experimental Design, Sample Size},
}
@article{simons_introduction_2014,
	title = {An Introduction to Registered Replication Reports at Perspectives on Psychological Science},
	volume = {9},
	issn = {1745-6916},
	url = {https://www.jstor.org/stable/44290039},
	pages = {552--555},
	number = {5},
	journaltitle = {Perspectives on Psychological Science},
	author = {Simons, Daniel J. and Holcombe, Alex O. and Spellman, Barbara A.},
	urldate = {2024-04-27},
	date = {2014},
	note = {Publisher: [Association for Psychological Science, Sage Publications, Inc.]},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/R4T4UZ73/Simons et al. - 2014 - An Introduction to Registered Replication Reports .pdf:application/pdf},
}
@article{slavin_effective_2008,
	title = {Effective reading programs for middle and high schools: A best-evidence synthesis},
	volume = {43},
	issn = {1936-2722},
	doi = {10.1598/RRQ.43.3.4},
	shorttitle = {Effective reading programs for middle and high schools},
	abstract = {This article systematically reviews research on the achievement outcomes of four types of approaches to improving the reading of middle and high school students: (1) reading curricula, (2) mixed-method models (methods that combine large-and small-group instruction with computer activities), (3) computer-assisted instruction, and (4) instructional-process programs (methods that focus on providing teachers with extensive professional development to implement specific instructional methods). Criteria for inclusion in the study were use of randomized or matched control groups, a study duration of at least 12 weeks, and valid achievement measures that were independent of the experimental treatments. A total of 33 studies met these criteria. The review concludes that programs designed to change daily teaching practices have substantially greater research support than those focused on curriculum or technology alone. Positive achievement effects were found for instructional-process programs, especially for those involving cooperative learning, and for mixed-method programs. The effective approaches provided extensive professional development and significantly affected teaching practices. In contrast, no studies of reading curricula met the inclusion criteria, and the effects of supplementary computer-assisted instruction were small. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {290--322},
	number = {3},
	journaltitle = {Reading Research Quarterly},
	author = {Slavin, Robert E. and Cheung, Alan and Groff, Cynthia and Lake, Cynthia},
	date = {2008},
	note = {Place: {US}
Publisher: International Reading Association},
	keywords = {Educational Programs, High Schools, Middle Schools, Reading Education},
	file = {Snapshot:/Users/luca/Zotero/storage/Y4YLJCZG/2008-09872-004.html:text/html},
}
@article{slavin_relationship_2009,
	title = {The Relationship Between Sample Sizes and Effect Sizes in Systematic Reviews in Education},
	volume = {31},
	issn = {0162-3737},
	url = {https://doi.org/10.3102/0162373709352369},
	doi = {10.3102/0162373709352369},
	abstract = {Research in fields other than education has found that studies with small sample sizes tend to have larger effect sizes than those with large samples. This article examines the relationship between sample size and effect size in education. It analyzes data from 185 studies of elementary and secondary mathematics programs that met the standards of the Best Evidence Encyclopedia. As predicted, there was a significant negative correlation between sample size and effect size. The differences in effect sizes between small and large experiments were much greater than those between randomized and matched experiments. Explanations for the effects of sample size on effect size are discussed.},
	pages = {500--506},
	number = {4},
	journaltitle = {Educational Evaluation and Policy Analysis},
	author = {Slavin, Robert and Smith, Dewi},
	urldate = {2023-12-01},
	date = {2009-12-01},
	langid = {english},
	note = {Publisher: American Educational Research Association},
	keywords = {notion},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/DPXQW39N/Slavin und Smith - 2009 - The Relationship Between Sample Sizes and Effect S.pdf:application/pdf},
}
@article{smart_importance_1964,
	title = {The importance of negative results in psychological research},
	volume = {5a},
	issn = {0008-4832},
	doi = {10.1037/h0083036},
	abstract = {The purposes of this study were to determine the proportion of papers which contain negative results (results which fail to reject the null hypothesis), and whether there is some selection in the papers published such that negative results are unlikely to be published. An examination of current psychological journals indicated that studies with negative results constitute about 9 per cent of the total volume of published papers. However, data from several unpublished sources indicate that negative results are less likely to be published. The reasons for their neglect - chiefly author selection and the greater editorial scrutiny they get - were presented. The practical, statistical and heuristic value of negative results was also discussed. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {225--232},
	number = {4},
	journaltitle = {Canadian Psychologist / Psychologie canadienne},
	author = {Smart, Reginald G.},
	date = {1964},
	note = {Place: Canada
Publisher: Canadian Psychological Association},
	keywords = {Psychology, Scientific Communication},
	file = {Smart - 1964 - The importance of negative results in psychologica.pdf:/Users/luca/Zotero/storage/UR9FKN4W/Smart - 1964 - The importance of negative results in psychologica.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/NTEU2N9H/2007-01461-003.html:text/html},
}
@article{smithson_better_2006,
	title = {A better lemon squeezer? Maximum-likelihood regression with beta-distributed dependent variables},
	volume = {11},
	issn = {1082-989X},
	doi = {10.1037/1082-989X.11.1.54},
	shorttitle = {A better lemon squeezer?},
	abstract = {Uncorrectable skew and heteroscedasticity are among the "lemons" of psychological data, yet many important variables naturally exhibit these properties. For scales with a lower and upper bound, a suitable candidate for models is the beta distribution, which is very flexible and models skew quite well. The authors present maximum-likelihood regression models assuming that the dependent variable is conditionally beta distributed rather than Gaussian. The approach models both means (location) and variances (dispersion) with their own distinct sets of predictors (continuous and/or categorical), thereby modeling heteroscedasticity. The location sub-model link function is the logit and thereby analogous to logistic regression, whereas the dispersion sub-model is log linear. Real examples show that these models handle the independent observations case readily. The article discusses comparisons between beta regression and alternative techniques, model selection and interpretation, practical estimation, and software.},
	pages = {54--71},
	number = {1},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychol Methods},
	author = {Smithson, Michael and Verkuilen, Jay},
	date = {2006-03},
	pmid = {16594767},
	keywords = {Humans, Analysis of Variance, Reproducibility of Results, Normal Distribution, Models, Statistical, Data Interpretation, Statistical, Bias, Linear Models, Regression Analysis, Child, Dyslexia, Least-Squares Analysis, Likelihood Functions},
	file = {Smithson und Verkuilen - 2006 - A better lemon squeezer Maximum-likelihood regres.pdf:/Users/luca/Zotero/storage/J6BNSW7K/Smithson und Verkuilen - 2006 - A better lemon squeezer Maximum-likelihood regres.pdf:application/pdf},
}
@article{song_dissemination_2010,
	title = {Dissemination and publication of research findings : an updated review of related biases},
	volume = {14},
	issn = {{ISSN}: 2046-4924, {ISSN}: 1366-5278},
	url = {https://www.journalslibrary.nihr.ac.uk/hta/hta14080/},
	doi = {10.3310/hta14080},
	shorttitle = {Dissemination and publication of research findings},
	abstract = {Objectives To identify and appraise empirical studies on publication and related biases published since 1998; to assess methods to deal with publication and related biases; and to examine, in a random sample of published systematic reviews, measures taken to prevent, reduce and detect dissemination bias. Data sources The main literature search, in August 2008, covered the Cochrane Methodology Register Database, {MEDLINE}, {EMBASE}, {AMED} and {CINAHL}. In May 2009, {PubMed}, {PsycINFO} and {OpenSIGLE} were also searched. Reference lists of retrieved studies were also examined. Review methods In Part I, studies were classified as evidence or method studies and data were extracted according to types of dissemination bias or methods for dealing with it. Evidence from empirical studies was summarised narratively. In Part {II}, 300 systematic reviews were randomly selected from {MEDLINE} and the methods used to deal with publication and related biases were assessed. Results Studies with significant or positive results were more likely to be published than those with non-significant or negative results, thereby confirming findings from a previous {HTA} report. There was convincing evidence that outcome reporting bias exists and has an impact on the pooled summary in systematic reviews. Studies with significant results tended to be published earlier than studies with non-significant results, and empirical evidence suggests that published studies tended to report a greater treatment effect than those from the grey literature. Exclusion of non-English-language studies appeared to result in a high risk of bias in some areas of research such as complementary and alternative medicine. In a few cases, publication and related biases had a potentially detrimental impact on patients or resource use. Publication bias can be prevented before a literature review (e.g. by prospective registration of trials), or detected during a literature review (e.g. by locating unpublished studies, funnel plot and related tests, sensitivity analysis modelling), or its impact can be minimised after a literature review (e.g. by confirmatory large-scale trials, updating the systematic review). The interpretation of funnel plot and related statistical tests, often used to assess publication bias, was often too simplistic and likely misleading. More sophisticated modelling methods have not been widely used. Compared with systematic reviews published in 1996, recent reviews of health-care interventions were more likely to locate and include non-English-language studies and grey literature or unpublished studies, and to test for publication bias. Conclusions Dissemination of research findings is likely to be a biased process, although the actual impact of such bias depends on specific circumstances. The prospective registration of clinical trials and the endorsement of reporting guidelines may reduce research dissemination bias in clinical research. In systematic reviews, measures can be taken to minimise the impact of dissemination bias by systematically searching for and including relevant studies that are difficult to access. Statistical methods can be useful for sensitivity analyses. Further research is needed to develop methods for qualitatively assessing the risk of publication bias in systematic reviews, and to evaluate the effect of prospective registration of studies, open access policy and improved publication guidelines.},
	pages = {1--220},
	number = {8},
	journaltitle = {Health Technology Assessment},
	author = {Song, F. and Parekh, S. and Hooper, L. and Loke, Y. K. and Ryder, J. and Sutton, A. J. and Hing, C. and Kwok, C. S. and Pang, C. and Harvey, I.},
	urldate = {2024-01-02},
	date = {2010-02-22},
	file = {Volltext:/Users/luca/Zotero/storage/J69UJG4P/Song et al. - 2010 - Dissemination and publication of research findings.pdf:application/pdf},
}
@article{stanley_detecting_2021,
	title = {Detecting publication selection bias through excess statistical significance},
	volume = {12},
	rights = {© 2021 John Wiley \& Sons Ltd.},
	issn = {1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1512},
	doi = {10.1002/jrsm.1512},
	abstract = {We introduce and evaluate three tests for publication selection bias based on excess statistical significance ({ESS}). The proposed tests incorporate heterogeneity explicitly in the formulas for expected and {ESS}. We calculate the expected proportion of statistically significant findings in the absence of selective reporting or publication bias based on each study's {SE} and meta-analysis estimates of the mean and variance of the true-effect distribution. A simple proportion of statistical significance test ({PSST}) compares the expected to the observed proportion of statistically significant findings. Alternatively, we propose a direct test of excess statistical significance ({TESS}). We also combine these two tests of excess statistical significance ({TESSPSST}). Simulations show that these {ESS} tests often outperform the conventional Egger test for publication selection bias and the three-parameter selection model (3PSM).},
	pages = {776--795},
	number = {6},
	journaltitle = {Research Synthesis Methods},
	author = {Stanley, T. D. and Doucouliagos, Hristos and Ioannidis, John P. A. and Carter, Evan C.},
	urldate = {2023-12-21},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1512},
	keywords = {meta-analysis, notion, statistical power, excess statistical significance, publication selection bias},
	file = {Snapshot:/Users/luca/Zotero/storage/CT55Z28V/jrsm.html:text/html;Stanley et al. - 2021 - Detecting publication selection bias through exces.pdf:/Users/luca/Zotero/storage/3E887JHZ/Stanley et al. - 2021 - Detecting publication selection bias through exces.pdf:application/pdf},
}
@article{sterling_publication_1959,
	title = {Publication Decisions and their Possible Effects on Inferences Drawn from Tests of Significance—or Vice Versa},
	volume = {54},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.1959.10501497},
	doi = {10.1080/01621459.1959.10501497},
	abstract = {There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs—an “error of the first kind”—and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance. * The author wishes to express his thanks to Sir Ronald Fisher whose discussion on related topics stimulated this research in the first place, and to Leo Katz, Oliver Lacey, Enders Robinson, and Paul Siegel for reading and criticizing earlier drafts of this manuscript.},
	pages = {30--34},
	number = {285},
	journaltitle = {Journal of the American Statistical Association},
	author = {Sterling, Theodore D.},
	urldate = {2024-02-11},
	date = {1959-03-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.1959.10501497},
	file = {Sterling - 1959 - Publication Decisions and their Possible Effects o.pdf:/Users/luca/Zotero/storage/KAYVC7PF/Sterling - 1959 - Publication Decisions and their Possible Effects o.pdf:application/pdf},
}
@article{storn_differential_1997,
	title = {Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces},
	volume = {11},
	issn = {1573-2916},
	url = {https://doi.org/10.1023/A:1008202821328},
	doi = {10.1023/A:1008202821328},
	abstract = {A new heuristic approach for minimizing possiblynonlinear and non-differentiable continuous spacefunctions is presented. By means of an extensivetestbed it is demonstrated that the new methodconverges faster and with more certainty than manyother acclaimed global optimization methods. The newmethod requires few control variables, is robust, easyto use, and lends itself very well to parallelcomputation.},
	pages = {341--359},
	number = {4},
	journaltitle = {Journal of Global Optimization},
	shortjournal = {Journal of Global Optimization},
	author = {Storn, Rainer and Price, Kenneth},
	urldate = {2024-03-25},
	date = {1997-12-01},
	langid = {english},
	keywords = {evolution strategy, genetic algorithm, global optimization, nonlinear optimization, Stochastic optimization},
	file = {Full Text PDF:/Users/luca/Zotero/storage/PZJSZHPN/Storn und Price - 1997 - Differential Evolution – A Simple and Efficient He.pdf:application/pdf},
}
@article{szucs_empirical_2017,
	title = {Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature},
	volume = {15},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.2000797},
	doi = {10.1371/journal.pbio.2000797},
	pages = {e2000797},
	number = {3},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLoS} Biol},
	author = {Szucs, Denes and Ioannidis, John P. A.},
	editor = {Wagenmakers, Eric-Jan},
	urldate = {2023-03-20},
	date = {2017-03-02},
	langid = {english},
	keywords = {notion},
	file = {Volltext:/Users/luca/Zotero/storage/2IHA2K2K/Szucs und Ioannidis - 2017 - Empirical assessment of published effect sizes and.pdf:application/pdf},
}
@software{ushey_renv_2024,
	title = {renv: Project Environments},
	rights = {{MIT} + file {LICENSE}},
	url = {https://cran.r-project.org/web/packages/renv/index.html},
	shorttitle = {renv},
	version = {1.0.7},
	author = {Ushey, Kevin and Wickham, Hadley},
	urldate = {2024-04-24},
	date = {2024-04-11},
	keywords = {{ReproducibleResearch}},
}
@article{van_aert_publication_2019,
	title = {Publication bias examined in meta-analyses from psychology and medicine: A meta-meta-analysis},
	volume = {14},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0215052},
	doi = {10.1371/journal.pone.0215052},
	shorttitle = {Publication bias examined in meta-analyses from psychology and medicine},
	pages = {e0215052},
	number = {4},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Van Aert, Robbie C. M. and Wicherts, Jelte M. and Van Assen, Marcel A. L. M.},
	editor = {Macleod, Malcolm R.},
	urldate = {2023-11-30},
	date = {2019-04-12},
	langid = {english},
	keywords = {notion},
	file = {Full Text PDF:/Users/luca/Zotero/storage/BUMRDEZX/Van Aert et al. - 2019 - Publication bias examined in meta-analyses from ps.pdf:application/pdf},
}
@incollection{vevea_publication_2019,
	edition = {3},
	title = {Publication Bias},
	pages = {383--433},
	booktitle = {The handbook of research synthesis and meta-analysis},
	publisher = {Russell Sage Foundation},
	author = {Vevea, Jack L. and Coburn, Kathleen and Sutton, Alexander J.},
	editor = {Cooper, Harris and Hedges, Larry V. and Valentine, Jeffrey C.},
	date = {2019},
	file = {Vevea et al. - 2019 - Publication Bias.pdf:/Users/luca/Zotero/storage/V67TUXGI/Vevea et al. - 2019 - Publication Bias.pdf:application/pdf},
}
@article{wand_fast_1994,
	title = {Fast Computation of Multivariate Kernel Estimators},
	volume = {3},
	issn = {1061-8600},
	url = {https://www.jstor.org/stable/1390904},
	doi = {10.2307/1390904},
	abstract = {Multivariate extensions of binning techniques for fast computation of kernel estimators are described and examined. Several questions arising from this multivariate extension are addressed. The choice of binning rule is discussed, and it is demonstrated that linear binning leads to substantial accuracy improvements over simple binning. An investigation into the most appropriate means of computing the multivariate discrete convolutions required for binned kernel estimators is also given. The results of an empirical study indicate that, in multivariate settings, the fast Fourier transform offers considerable time savings compared to direct calculation of convolutions.},
	pages = {433--445},
	number = {4},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Wand, M. P.},
	urldate = {2024-05-05},
	date = {1994},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	file = {JSTOR Full Text PDF:/Users/luca/Zotero/storage/A7RZTRIV/Wand - 1994 - Fast Computation of Multivariate Kernel Estimators.pdf:application/pdf},
}
@book{wand_kernel_1994,
	location = {New York},
	title = {Kernel Smoothing},
	isbn = {978-0-429-17059-1},
	abstract = {Kernel smoothing refers to a general methodology for recovery of underlying structure in data sets. The basic principle is that local averaging or smoothing is performed with respect to a kernel function.This book provides uninitiated readers with a feeling for the principles, applications, and analysis of kernel smoothers. This is facilita},
	pagetotal = {224},
	publisher = {Chapman and Hall/{CRC}},
	author = {Wand, M. P. and Jones, M. C.},
	date = {1994-12-01},
	doi = {10.1201/b14876},
}
@software{wand_kernsmooth_2023,
	title = {{KernSmooth}: Functions for Kernel Smoothing Supporting Wand \& Jones (1995)},
	rights = {Unlimited},
	url = {https://cran.r-project.org/web/packages/KernSmooth/index.html},
	shorttitle = {{KernSmooth}},
	abstract = {Functions for kernel smoothing (and density estimation) corresponding to the book: Wand, M.P. and Jones, M.C. (1995) "Kernel Smoothing".},
	version = {2.23-22},
	author = {Wand, M. P. and Moler, Cleve and Ripley, Brian},
	urldate = {2024-05-05},
	date = {2023-07-10},
}
@article{weinerova_published_2022,
	title = {Published correlational effect sizes in social and developmental psychology},
	volume = {9},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.220311},
	doi = {10.1098/rsos.220311},
	abstract = {The distribution of effect sizes may offer insights about the research done and reported in a scientific field. We have evaluated 12 412 manually collected correlation effect sizes (Sample 1) and 31 157 computer-extracted correlation effect sizes (Sample 2) published in journals focused on social or developmental psychology. Sample 1 consisted of 243 studies from six journals published in 2010 and 2019. Sample 2 consisted of 5012 papers published in 10 journals between 2010 and 2019. The 25th, 50th and 75th effect size percentiles were 0.08, 0.17 and 0.33, and 0.17, 0.31 and 0.52 in Samples 1 and 2, respectively. Sample 2 percentiles were probably larger because Sample 2 only included effect sizes from the text but not from tables. In text authors may have emphasized larger correlations. Large sample sizes were associated with smaller reported correlations. In Sample 1 about 70\% of studies specified a directional hypothesis. In 2010 no papers had power calculations, while in 2019 14\% of papers had power calculations. These data offer empirical insights into the distribution of reported correlations and may inform the interpretation of effect sizes. They also demonstrate the importance of computation of statistical power and highlight potential reporting bias.},
	pages = {220311},
	number = {12},
	journaltitle = {Royal Society Open Science},
	author = {Weinerová, Josefína and Szűcs, Dénes and Ioannidis, John P. A.},
	urldate = {2023-12-01},
	date = {2022-12-21},
	note = {Number: 12
Publisher: Royal Society},
	keywords = {notion, effect size, sample size, correlation, statistical power},
}
@software{zeileis_betareg_2021,
	title = {betareg: Beta Regression},
	rights = {{GPL}-2 {	extbar} {GPL}-3},
	url = {https://cran.r-project.org/web/packages/betareg/index.html},
	shorttitle = {betareg},
	abstract = {Beta regression for modeling beta-distributed dependent variables, e.g., rates and proportions. In addition to maximum likelihood regression (for both mean and precision of a beta-distributed response), bias-corrected and bias-reduced estimation as well as finite mixture models and recursive partitioning for beta regressions are provided.},
	version = {3.1-4},
	author = {Zeileis, Achim and Cribari-Neto, Francisco and Gruen, Bettina and Kosmidis, Ioannis and Simas, Alexandre B. Simas and Rocha, Andrea V.},
	urldate = {2024-03-04},
	date = {2021-02-09},
	keywords = {Econometrics, Psychometrics},
}

